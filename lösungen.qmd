---
title: "Lösungen"
editor: visual
format: html
bibliography: references.bib
bibliographystyle: apa
---

# Lösungen und Ansätze zur Verbesserung der Lage der Psychologie

Werfen wir ein Blick darauf, was sich seit 2012 in der Psychologie verändert hat. Eingeteilt sind die Veränderungen dahingehend, welchen Teil des Problems sie vor allem betreffen: Ist der Zweck einer neuen Vorgabe, das System, die Methodik, oder die Theorien der Forschung zu verbessern? Einige Lösungen sind mit vielen Problemen gleichzeitig verknüpft und manche sind auf bestimmte Angelegenheiten maßgeschneidert. Abbildung 3 enthält eine grobe Unterteilung.

**Abbildung 5**

Lösungsansätze und welches Problem in der Wissenschaft sie aufgreifen

[\[LR1\]](#_msocom_1) [\[LR2\]](#_msocom_2) 

## Das System

Fangen wir mit dem System an. Ich selbst verspreche mir von diesen Ansätzen am meisten, denn solange Publikationen die Währung sind und Paper mit knackigen Titeln und eindeutigen Ergebnissen als qualitativ hochwertiger befunden werden, sind Forschende darin motiviert, statt der Wahrheit eben nach knackigen Titeln und eindeutigen Ergebnissen zu suchen.

Allgemein ist eine positive Entwicklung sichtbar (REF <https://www.nature.com/articles/s44271-023-00003-2> ) und eine Veränderung der Anreizstruktur wird anvisiert. Sie lässt sich als Angleichung des wissenschaftlichen Systems an die *Mertonschen Normen* (nach Robert Merton) auffassen[\[LR3\]](#_msocom_3) : (1) Kommunismus: Das wissenschaftliche Wissen sollte allen Wissenschaftler\*innen gleichermaßen gehören, um die Zusammenarbeit zu fördern. (2) Universalismus: Wissenschaftliche Güte ist unabhängig vom soziopolitischen Status und persönlichen Attributen der Teilhabenden. (3) Desinteresse: Wissenschaftliche Institutionen handeln im Interesse der Wissenschaft und nicht für persönlichen Gewinn. (4) Organisierter Skeptizismus: Wissenschaftliche Behauptungen sollten einer kritischen Prüfung unterzogen werden bevor sie akzeptiert werden.

Nosek (REF, 2019 make it possible, siehe Abbildung X) empfiehlt eine Maßnahmenstruktur, nach welcher die gewünschten Veränderung nacheinander ...

1.      möglich\
(z.B. durch Infrastruktur wie online Repositorien, in denen Forschungsmaterialien öffentlich und gratis hochgeladen werden können),

2.      einfach\
(z.B. durch barrierearme Angebote, mehrsprachige Anleitungen),

3.      normativ\
(z.B. durch Wissenschaftliche Communities, die gemeinsam hinter Forderungen der Verbesserung stehen),

4.      belohnend\
(z.B. durch designierte Preise), und

5.      notwendig\
(z.B. durch Mindeststandards, die von Zeitschriften oder Drittmittelgebern gefordert werden)

gemacht werden sollen. Wie die verschiedenen Ansätze bei den verschiedenen Akteuren, also Politik, Universitäten, oder Zeitschriften konkret aussehen, wird im Folgenden diskutiert.

Abbildung X

*Kulturwandel in der Wissenschaft nach Nosek (2019, Abbildung N, REF)*

*PYRAMIDE HIER EINFÜGEN AUF DEUTSCH XXX*

### Infrastruktur (Open Infrastructure)

-          ermöglichend

-          NFDI

-          Prinzipien: <https://openscholarlyinfrastructure.org> (übersetzen, einfügen, zitieren)

-          Beispiele

o   Wege, um Materialien, Daten, oder Ergebnisse

§  Literaturdatenbanken (OpenAlex)

§  (Data) Repositories, re3data.org; OSF.io

§  Pre-Print Server (arXiv), Zeitschriften, Zeitschriftensysteme (OJS)

§  Begutachtungssysteme (PCI)

§  Post-Publication Review

o   Meta-Daten, um Verweise zu ermöglichen (ORCID-ID, ROR <https://ror.org>, DOI)

§  ORCID-ID Kritik: <https://www.sciencedirect.com/science/article/pii/S0099133324000144?dgcid=rss_sd_all>

-          Übersicht: <https://kumu.io/access2perspectives/open-science#disciplines/by-os-principle/open-infrastructure?focus=1> von <https://access2perspectives.org/mapping-open-science-resources/>

### Politik

International stehen politische Parteien und Vereinigungen deutlich hinter Open Science und Open Access. Beispielsweise empfiehlt die UNESCO einen universellen Zugang zu wissenschaftlichen Wissen ungeachtet von Herkunftsland, Geschlechterrolle, politischen Grenzen, Ethnizität, oder ökonomischen oder technologischen Hürden (REF <https://unesdoc.unesco.org/ark:/48223/pf0000374837>, p. 3). Arbeitsgruppen für politische Instrumente, Förderung, und Infrastruktur wurden entsprechend gegründet (REF[\[LR4\]](#_msocom_4) ). Die G7 setzen sich für wissenschaftliche Integrität, akademische Freiheit, und Open Science ein (REF[\[LR5\]](#_msocom_5) ). Offener Zugang (REF[\[LR6\]](#_msocom_6) ) aber auch Transparenz des wissenschaftlichen Vorgehens (REF[\[LR7\]](#_msocom_7) ) wird auch seitens der Europäischen Union gefordert. Infrastruktur (z.B. European Open Science Cloud, https://eosc-portal.eu) und diverse Open Science Forschungsprojekte werden gefördert.

In Deutschland hat sich die Regierung des Zyklus 2021-2025 im Rahmen des Koalitionsvertrages vorgenommen, „Open Access ... als gemeinsamen Standard \[zu\] etablieren." Einzelne Bundesländer wie Nordrhein-Westfalen haben in Zusammenschlüssen aus den jeweiligen Universitäten darüber hinaus Open Access Strategien entwickelt (REF[\[LR8\]](#_msocom_8) ) und arbeiten aktuell an Open Science Strategien. Andere Länder, wie zum Beispiel Schweden, haben bereits nationale Richtlinien zu Open Science entwickelt (<https://www.kb.se/samverkan-och-utveckling/nytt-fran-kb/nyheter-samverkan-och-utveckling/2024-01-15-national-guidelines-for-promoting-open-science-in-sweden.html>). Hinsichtlich der Problematik von Machtmissbrauch wird das Problem beispielsweise in einem Eckpunkte-Papier des Landes NRW anerkannt, doch als Einzelfall- statt System-Problem verstanden (REF[\[LR9\]](#_msocom_9) , für einen Kommentar siehe z.B. REF[\[LR10\]](#_msocom_10) ).

### Universitäten[\[LR11\]](#_msocom_11) 

Das Thema Open Science hat bei vielen Universitäten bereits Anklang gefunden. Während die meisten deutschen Universitäten Mittel für Open Access Publikationen haben, existieren an einigen darüber hinaus Open Science Policys (z.B. FAU Erlangen, <https://oa-info.sh/2022/01/open-science-policy-der-uni-erlangen-nuernberg/>), Open Science Centers (z.B. [LMU Open Science Center](https://www.osc.uni-muenchen.de/index.html), <https://osf.io/smqpn>; Köln Open Science Center; Münster Center for Open Science; [Mannheim Open Science Office](https://www.uni-mannheim.de/open-science/open-science-office/)). Darüber hinaus unterstützen das Leibniz-Informationszentrum Wirtschaft in Kiel und das Leibniz-Institut für Psychologie Replikationsforschung beispielsweise mit einer Replikationszeitschrift (<https://www.jcr-econ.org>) oder im Rahmen einer Juniorprofessur für Psychologische Metawissenschaft. Eines der Metawissenschaftlichen Zentren innerhalb Europas hat sich in den Niederlanden in Tilburg gebildet. Die Berliner Universitäten haben eine gemeinsame Open Access Erklärung entwickelt (<https://openaccess.mpg.de/Berliner-Erklaerung>[\[LR12\]](#_msocom_12) ) und in Frankreich ist die Universität Sorbonne ein Pionier: Gemeinsam mit der Universität Amsterdam und dem Universitätscollege London wurde eine Erklärung über die die Veröffentlichung von Forschungsdaten unterzeichnet. XXX[\[LR13\]](#_msocom_13) . Seit 2024 hat die Universität Sorbonne darüber hinaus den Vertrag mit Clarivate für die Nutzung der Forschungsdatenbank „Web of Science" gekündigt und arbeitet seitdem mit der Open Source Software „OpenAlex" (REF[\[LR14\]](#_msocom_14) ).

Für die langfristige Entwicklung der Wissenschaften haben Universitäten dadurch eine besondere Verantwortung, dass sie Wissenschaftler\*innen beschäftigen und an ihnen die Auswahl für die wenigen unbefristeten Arbeitsplätze in der Wissenschaft fallen. Wenn jahrzehntelang Professuren auf Basis subjektiver, nicht-reproduzierbarer, und für gute Wissenschaft nachrangigen Kriterien gewählt werden (z.B. Anzahl an Publikationen in Fachzeitschriften), kann sich das negativ auf die Entwicklung von Wissenschaften auswirken. Ein Forschungspreis des Berlin Institute of Health (BIH), der jährlich für Projekte zur Förderung von wissenschaftlicher Integrität verliehen wird, ging 2023 an ein Projekt, das objektive und sinnvolle Auswahlkriterien für Professor\*innen entwickelt (REF, REF[\[LR15\]](#_msocom_15) ). Innerhalb von Universitäten spielen außerdem die Bibliotheken eine aufklärerische Rolle hinsichtlich Forschungsdatenmanagement und Publikationskultur (REF, <https://liberquarterly.eu/article/view/14947>). Über sie kann der Forschungsprozess mit entsprechender Infrastruktur (z.B. zum Lagern und Veröffentlichen von Forschungsmaterialien und -ergebnissen) unterstützt werden (z.B. REF[\[LR16\]](#_msocom_16) ) und verhindert werden, dass sich eine „Abhängigkeit von wenigen kommerziellen Anbietern" ergibt, die „begrenzen, was \[be\] der Forschung an Arbeitsmöglichkeiten und Fragestellungen erreichbar ist" (REF[\[LR17\]](#_msocom_17) ). Eine Pflicht, Mitglieder eine Universität zur Einhaltung von Open Science Strategien zu bewegen, haben Universitäten jedoch nur eingeschränkt durch den hohen Stellenwert der „Freiheit der Forschung" sowie der Gefahr, für Forschende weniger attraktiv zu werden, wenn beispielsweise auf namhafte Zeitschriften nicht mehr über die Universität zugegriffen werden kann, weil Verträge mit closed-access Zeitschriften gekündigt wurden. Beispielsweise klagte die juristische Fakultät der Universität Konstanz gegen eine Zweitveröffentlichungspflicht (REF[\[LR18\]](#_msocom_18) ), nach welcher Forschende von ihrem Zweitveröffentlichungsrecht Gebrauch machen müssen oder es werden Zeitschriften, deren wissenschaftliche Qualität angezweifelt wird, dennoch mit Mitteln zur Veröffentlichung bezuschusst (<https://www.suub.uni-bremen.de/ueber-uns/neues-aus-der-suub/unter-kritischer-beobachtung-open-access-publikationen-im-mdpi-verlag>).

Eine oft vernachlässigte Rolle kommt außerdem der universitären Lehre hinzu. Durch die Freiheit von Forschung und Lehre und bereits durchgeplanten Studiengängen gestaltet sich die Integration von Open Science in die Lehre schwierig. Forschende, in deren Lehre die Thematik eine Rolle spielt, teilen proaktiv ihre Materialien, erstellen gemeinsam Curricula, und sind beispielsweise in großen internationalen Initiativen wie dem Framework for Open and Reproducible Research Training (FORRT.org) vernetzt (konkrete Vorschläge zur Integration von Open Science in die Lehre siehe z.B. REF[\[LR19\]](#_msocom_19) ).

### Institute und Vereinigungen

Wissenschaftliche Gebiete leben vor allem durch Communities, also alle in dem Bereich forschenden Personen. Sie organisieren sich üblicherweise in Vereinen (z.B. Deutsche Gesellschaft für Psychologie), Interessensverbunden, oder ähnlichen Gemeinschaften. Eine besondere Stellung hat in Deutschland die Deutsche Forschungsgemeinschaft, welche staatlich und über die Bundesländer mit mehreren Milliarden Euro ausgestattet Forschungsgelder vergibt. Als eine der wichtigsten nationalen Institution hat ihre Open Science Positionierung einen hohen Stellenwert (REF[\[LR20\]](#_msocom_20) ), erfahrungsgemäß gehen Veränderungen jedoch nicht von der DFG aus, sondern die DFG wartet auf Anstöße aus den Fächern. DGPs[\[LR21\]](#_msocom_21) . In der Psychologie fördert darüber hinaus das ZPID die Infrastruktur durch Zeitschriften, Pre-Print Server, und weitere Methoden (REF[\[LR22\]](#_msocom_22) ). Auch interdisziplinäre Vereinigungen wie das CERN (REF[\[LR23\]](#_msocom_23) ) oder internationale Akteure wie die American Psychological Association (REF[\[LR24\]](#_msocom_24) ) verpflichten sich Offenheit und Transparenz. Für Bürger\*innen der Europäischen Union existieren die European Open Science Cloud und die Open Access Publishing Plattform *Open Research Europe*.

Im Rahmen der Open Science Reform entstanden außerdem viele neue Vereinigungen. Das interdisziplinäre und besonders von Wissenschaftler\*innen in der frühen Karrierephase geleitete FORRT (REF[\[LR25\]](#_msocom_25) ) setzt sich für eine Verankerung von Open Science in der Lehre ein. Der Verbesserung psychologischer Forschung hat sich die Society for the Improvement of Psychological Science (SIPS) verschrieben. Sogenannte „grassroot"-Initiativen (also von jungen Wissenschaftler\*innen ausgehende Bewegungen) haben sich an zahlreichen Universitäten herausgebildet und zu Netzwerken wie dem [Netzwerk der Open Science Initiativen (NOSI)](https://osf.io/tbkzh/) und „Reproducibility Networks" wie dem GRN (<https://reproducibilitynetwork.de>), dem UKRN (<https://www.ukrn.org>) und weiteren zusammengeschlossen. Aber auch Zusammenschlüsse von Professor\*innen zur Änderung von Kurzzeitverträgen existieren (z.B. Netzwerk Nachhaltige Wissenschaft, <https://netzwerk-nachhaltige-wissenschaft.de>).

### Zeitschriften

Wissenschaftliche Zeitschriften gelten als Bühne des wissenschaftlichen Diskurses und bestimmen maßgeblich, welche Elemente des Forschungsprozesses zum „scientific record" gehören und damit relevant sind. Sie sind darüber hinaus als Organisator des Begutachtungsprozesses für die Qualitätssicherung in der Wissenschaft verantwortlich. Dem Mangel an Qualität entgegnend entstehen im Rahmen von Open Science Empfehlungen zur Gestaltung von Zeitschriften, es bilden sich neue Zeitschriften, und vollständige neue Begutachtungs- und Publikationsmodelle werden vorgeschlagen und vielseitig implementiert. Das Journal of Open Source Software (REF[\[LR26\]](#_msocom_26) ) basiert beispielsweise auf Github (XXX[\[LR27\]](#_msocom_27) ) und seine Infrastruktur lässt sich für weitere Zeitschriften kopieren und anpassen.

#### Empfehlungen

Herausgeber\*innen, die sich in Bezug auf die von ihnen verwaltete Zeitschrift mit Open Science Praktiken auseinandersetzen möchten, können inzwischen auf einen umfangreichen Leitfaden zurückgreifen (REF[\[LR28\]](#_msocom_28) ). Über eine Diskussionsplattform (Journal Editors Discussion Interface, JEDI) wurden Vorschläge gesammelt und es wir erklärt, worum es sich bei Dingen wie Registered Reports, Open Peer Review, Diversifizierung, und Open Access handelt und wie diese in eine Zeitschrift implementiert werden können. Eventuelle Sorgen und Ängste werden angesprochen und beantwortet. Das Committee on Publication Ethics (COPE) setzt sich ebenfalls für Aufklärung und Lehre ein, die Herausgeber, Universitäten, und Forschungsinstitute im Umgang mit Problemen im Publikationssystem helfen soll. Es bietet beispielsweise Richtlinien unter welchen Umständen Publikationen zurückgezogen oder korrigiert werden sollten (<https://publicationethics.org/retraction-guidelines>), oder welche ethischen Standards ein Begutachtungsprozess erfüllen sollte (<https://publicationethics.org/resources/guidelines/cope-ethical-guidelines-peer-reviewers>). Herausgeber\*innen, die Zeitschriften für kommerzielle Verlage verwalten und auf Systeme umsteigen möchten, die vollständig in der Hand der Forschenden liegen, können über Universitätsbibliotheken Hilfe bei der Migration von den kommerziellen zu offenen und kostenfreien Systemen erhalten und Zeitschriften beispielsweise mit dem Open Journal System verwalten (siehe z.B. OJS Netzwerk, <https://ojs-de.net/start>). Eine Datenbank mit bereits über 20,000 offen zugänglichen Zeitschriften verwaltet das Directory of Open Access Journals (DOAJ, <https://doaj.org>). Gutachter\*innen von Forschungsartikeln können über die Reviewer Zero Initiative (<https://www.reviewerzero.net>) auf Lehrmaterialien und Leitfäden zugreifen (<https://osf.io/e7z5k/wiki/Resources/>).  

#### Open Science Praktiken hervorheben

-          Badges

-          TOPfactor.org

-          Badges können ge-"game"t werden: <https://journals.sagepub.com/doi/10.1177/10731911241253430>

#### Review Systeme

-          „criteria related to consensus-building do not yet espouse sufficient reliability" <https://www.researchgate.net/publication/380433173_Inter-Rater_Reliability_in_Assessing_the_Methodological_Quality_of_Research_Papers_in_Psychology> aber es gibt Kriterienkataloge, die gut funktionieren

-          PCI

-          F100research.com: Mischung aus Zeitschrift und Pre-Print Server; Artikel ist sofort öffentlich verfügbar ab Einreichung und es steht dann dort, dass er under review ist

-          Review-Ampel

-          Open Peer Review

-          Crowd peer review

o   Wenig Beteiligung an offenem Review, Zeitschrift Synlett und ASAPbio: Koordinierung von Gruppe im Sinne eines Journal Clubs (=preprint review club), <https://asapbio.org/crowd-preprint-review>

-          Review von negativen Zeitschriften: Infos via SIPS 2024 (Zoltan Kekecs SIPS Unconference, sehr spannende Ideen auch von Madhwa Galgali, Ekaterina Pronizius, Willemijn Plomp, Belay Weldemicheal)

o   Aktuell „Open peer review" betrifft nur akzeptierte Papers; Meta-Wissenschaft über Peer Review wäre spannend

o   Doppelarbeit und enorme Kosten <https://link.springer.com/article/10.1186/s41073-021-00118-2>

o   Daten im Review mit anschauen und Zombie Papers: <https://associationofanaesthetists-publications.onlinelibrary.wiley.com/doi/full/10.1111/anae.15263>

§  Falsifizierte Daten wurden in Artikeln identifiziert, abgelehnt, aber dann woanders veröffentlict [\[LR29\]](#_msocom_29) hat;

§  *„losing epistemic arms race", „we are arming the opposite site" à* feedback für abgelehnte Paper hilft Datenfälscher\*innen

§  institutionelles Gedächtnis nötig; editorial notes / decisions sollten „für immer" an dem Artikel dranheften; dafür Versionierungssystem nötig, Namen von Beteiligten, Zeitschriften, usw.; Antwort auf Rejection sollte möglich sein

o   Implementierung: automatisiertes System über editorial management Systeme; Möglichkeit für Embargo

o   Mögliche Nachteile

§  Paper mit 5 Rejections sieht nicht so hübsch aus; evtl. akzeptieren manche Journals nichts, was bei „schlechten" Journals rejected wurde „death spiral", Ankereffekte zwischen Reviews; evtl. nur Editors erlauben, alte Reviews zu sehen?

§  Reviewers orientieren sich evtl. aneinander

§  Nachteile für Reviewer (ich wurde negativ bewertet und nun stellt mich die Person nicht ein)

o   Implementierung

§  Soziales Netzwerk für Reviews mit Ratings dafür wie hilfreich usw. Reviews sind

§  Pubpeer mit Anforderung, Papers als Pre-Print zu posten (keine Versionierung und nur mit Pre-Prints möglich)

§  Personality Science: Einreichung mit bisherigen Reviews

§  Über PCI (dort werden Reviews aktuell nicht veröffentlicht, wenn es negativ ausfällt)

-          Post Publication Peer Review

o   Pubpeer.com

o   Hypothes.is (?)

o   Disqus,

o   aphaxiv.org

o   https://scirev.org

#### Aufmerksamkeit zum Thema in bestehenden Zeitschriften

Anforderungen an wissenschaftliche Artikel seitens der Zeitschriften sind 2010 maßgeblichen Änderungen untergangen. In den Sozialwissenschaften orientieren sich zahlreiche Zeitschriften an den Richtlinien zur „Transparency and Openness Promotion" (TOP) und erhalten entsprechende TOP-Faktoren (<https://topfactor.org/summary>). Dabei wird festgehalten, welcher Grad an Offenheit und Transparenz von Forschungsdaten und -materialien gefordert wird und ob Replikationen bei der jeweiligen Zeitschrift veröffentlicht werden. Neue Herausgeber\*innen bei existierenden Zeitschriften haben große Änderungen vorgenommen (z.B. Editorial psych bulletin, REF[\[LR30\]](#_msocom_30) ). Beispielsweise haben Hardwicke und Vazire (REF[\[LR31\]](#_msocom_31) ) für die Zeitschrift *Psychological Science* ein standardmäßiges Nachrechnen aller berichteten Ergebnisse ab 2024 angekündigt (REF), indem sie mit dem Institute for Replication zusammenarbeiten (<https://i4replication.org>). Vereinzelt haben Zeitschriften Spezialausgaben herausgegeben, bei denen der Fokus auf Replikationsstudien oder der Reproduzierbarkeit von Ergebnissen lag (<https://imstat.org/publications/sts/sts_38_4/sts_38_4.pdf>).

#### Zeitschriften für "nicht Innovatives"

Durch die Selektion spannender Ergebnisse gibt es für Forschung, die nicht bahnbrechend und dennoch höchst relevant ist, keine Plattform. Schätzungen zufolge werden bis zu 40% aller durchgeführten Studien innerhalb von 4 Jahren nach Durchführung nicht veröffentlicht (REF[\[LR32\]](#_msocom_32) ). Andere Forschende können nicht davon lernen und Ressourcen, die in die Sammlung und Auswertung der Daten geflossen sind, Zeit von Versuchspersonen, und lange Vorbereitungen der Forschung werden schließlich verschwendet. Zur Lösung dieses Problems haben sich neue Zeitschriften und Formate gebildet. In der Ökonomie hat sich auf Forderungen (REF[\[LR33\]](#_msocom_33) ) hin beispielsweise eine Zeitschrift für Replikationen und Kommentare gebildet (JCRE[\[LR34\]](#_msocom_34) ), die Zeitschrift Meta-Psychology bietet das Format des „Schubladenbericht" an. Dieses Format ist für Studien vorgesehen, die wegen wenig überraschenden Ergebnissen oder Fehlern in der Durchführung anderweitig in der Schublade landen würden. Ebenfalls zur Abbildung des für den Forschungsprozess typischen Fehlschlagens wurde das Journal of Trial and Error (LINK) gegründet, und bei ReScience C werden Berichte über Reproduzierbarkeit veröffentlicht (LINK).

#### Publikationsmodelle

Noch radikalere Vorschläge als die Anpassung bisheriger Zeitschriften ist der Vorschlag, das bisherige System durch ein neues zu ersetzen. Dabei handelt es sich um ein soziales Dilemma, bei dem Millionen von Forschenden sich plötzlich anders verhalten müssen und dabei entgegen der Spielregeln des wissenschaftlichen Systems handeln müssen (REF[\[LR35\]](#_msocom_35) ). Das Dilemma wurde von kommerziellen Verlagen gestaltet, welche daraus Geld verdienen. Brembs et al. (REF[\[LR36\]](#_msocom_36) ) haben einen präzisen Vorschlag erarbeitet, bei welchem ein dezentrales System aufgebaut wird, das soziale Netzwerke wie Mastodon als Vorbild hat, von Wissenschaftler\*innen organisiert wird, und Forschungsprodukte wie Daten oder Programme ebenso wie die traditionellen Forschungsberichte wertschätzt. Plattformen, die ein solches Mikro-Publishing-System bereits implementieren sind Research-Equals (https://www.researchequals.com/) oder Octopus.ac (Hsing et al.). Dem System, bei dem alle Produkte begutachtet werden, der Begutachtungsprozess aber nicht die Qualität sicherstellt, stehen hier Ampel-Systeme und öffentliche Kommentierung entgegen, die signalisieren, was und ob begutachtet wurde, welche Kritikpunkte vorlagen, und wie damit umgegangen wurde.

### Forschende

Unabhängig von Nationalität, Wissenschaftsgebiet, und Universität haben viele Forschende ihre Arbeitsweisen im Zuge von Open Science überdacht und angepasst. Hunderte haben öffentliche Erklärungen zur Forschungstransparenz (<http://www.researchtransparency.org>) und der Forderung von Open Science Praktiken in der Rolle von Gutachter\*innen (<https://www.opennessinitiative.org>) unterzeichnet. Einzelne Forschende führen im Rahmen von Lehre Replikationsstudien durch (REF[\[LR37\]](#_msocom_37) , REF[\[LR38\]](#_msocom_38) ), schließen sich weltweit zusammen um gemeinsam Projekte durchzuführen, die einzelne nicht stämmen könnten (<https://psysciacc.org>), und entwickeln Sammlungen (<https://docs.google.com/spreadsheets/d/1KUMSeq_Pzp4KveZ7pb5rddcssk1XBTiLHniD0d3nDqo/edit#gid=0>, https://oercommons.org/hubs/OSKB), Leitfäden (REF <https://osf.io/jfh3t/>) und Glossare (REF[\[LR39\]](#_msocom_39) ) um den Zugang zu Open Science zu erleichtern. Eine noch unerfüllte Forderung ist die, Forschende das System über Zusammenschlüsse im Rahmen von Gewerkschaften zu reformieren (REF[\[LR40\]](#_msocom_40) ).

### Bewertungskriterien

<https://www.researchgate.net/publication/351638932_The_Natural_Selection_of_Good_Science>

-          Systematische Erarbeitung objektiver Kriterien

o   <https://psycharchives.org/en/item/dca8878a-1f6a-4599-abe0-16134b4b7f64>

o   <https://osf.io/preprints/psyarxiv/5yexm>

-          Peer review guidelines <https://www.researchgate.net/publication/375607348_Quantitative_manuscript_peer_review_template>

<https://psyarxiv.com/rgh5b/>\
<https://psyarxiv.com/5yexm/>

-   

-   Alternative Auswahlkriterien für Professor\*innen: <https://osf.io/preprints/metaarxiv/juwck>

-   

-   Multilevel-Selektion: <https://journals.sagepub.com/doi/pdf/10.1177/17456916231182568>

-   

-   <https://tu-dresden.de/mn/psychologie/ifap/differentielle-psychologie/die-professur/Mitarbeiter/dipl-psych-anne-gaertner> Einstein Award Thema (Auswahlkriterien)

-   

-   Problem von Kriterien ist, dass sie oft Dinge jenseits des Mainstreams benachteiligen (REF[\[LR41\]](#_msocom_41) )

-   

#### Alternativen zum Impact Factor

Altmetric für Einfluss jenseits von Publikationen (z.B. in sozialen Medien, Wikipedia, news outlets),

DORA: <https://sfdora.org/read/read-the-declaration-deutsch/>

COARA <https://coara.eu> Alternative Bewertungskriterien (qualitativ, nicht quantitativ; kann man signieren)

TOP guidelines

-          <https://www.researchgate.net/publication/369670371_Evaluating_Implementation_of_the_Transparency_and_Openness_Promotion_Guidelines_Reliability_of_Instruments_to_Assess_Journal_Policies_Procedures_and_Practices>

CiTO <https://sparontologies.github.io/cito/current/cito.html>

Free Lunch Index <https://link.springer.com/article/10.1007/s11192-023-04862-8>

Im Rahmen von XXX wurden Richtlinien zur Förderung von Transparenz und Offenheit in der Wissenschaft erstellt (Transparency and Opennness Promotion guidelines; kurz: TOP guidelines). Eine Zeit lang gaben manche Zeitschriften freiwillig an, welche Richtlinien wie eingehalten werden. Darin befindet sich zum Beispiel die Frage, ob mindestens eine Studie eines publizierten Artikels präregistriert sein muss, oder ob Datensätze veröffentlicht werden müssen. Je Aspekt (z.B. Öffentlichkeit der Daten) gibt es dann XX Levels: Das niedrigste Level 0 gibt an, dass es zu dem jeweiligen Aspekt keine Empfehlungen oder Vorschriften gibt. Das höchste Level 3 hingegen fordert, dass alle verwendeten Datensätze online verfügbar sind. \[XXX genau anpassen an Vorgaben\].

Seit 2022 lassen sich Zeitschriften, die TOP guidelines auf verschiedenen Levels umsetzen, in der topfactor.org Datenbank durchsuchen. Somit haben Forschende eine Alternative zum Impact Factor, die tatsächlich mit der Qualität der Zeitschrift zusammenhängt.

### Open Access Publikationen

Typen von Open Access (siehe zenodo NRW AG Open Science Auflistung)

An manchen Orten gibt es ungeschriebene Gesetze wie "wenn du während deiner Promotion in einem hochrangigen Journal publizierst, wirst du die Bestnote kriegen". Damit wird den sogenannten prestigereichen Zeitschriften immer mehr Macht zugeschoben. Leuten wird außerordentlich dafür gratuliert, dass sie etwas in Psychological Bulletin oder sogar Nature Human Behavior veröffentlicht haben. Dass niemand, der nicht mit einer Universität affiliiert ist (also an einer arbeitet oder studiert), den Artikel im Psychological Bulletin lesen kann, oder dass für die Veröffentlichung in Nature XXX Euro bezahlt wurden, spielt dabei keine Rolle. Leider sind genaue Regeln, wann welche Artikel von diesen Zeitschriften veröffentlicht werden intransparent. Auf den Seiten der Zeitschriften sind zwar üblicherweise Hinweise für Autor\*innen und kurze Texte über die Zeitschrift, ob eine spezifische Studie genommen wird oder nicht, entscheiden zuerst alleine die Herausgeber\*innen oder auf Englisch Editors. Zwischen zahlreichen Einreichungen müssen sie diejenigen auswählen, die an die Gutachtenden weitergeleitet werden.

Diesem intransparenten System stehen Open Access Zeitschriften entgegen. Obgleich ihr Kern die Öffentlichkeit aller Artikel ist, geht das Prinzip oft damit einher, dass die Ergebnisse des Gutachtenprozesses ebenfalls veröffentlicht werden.

-          Guide

o   <https://psycharchives.org/en/item/4d0f12d8-a542-4cfd-b718-9ee7c9d11f5b>

-          Paywall umgehen

o   Sci-hub

o   12ft.io

o   #canihazpaper

o   Autor\*innen persönlich anschreiben (z.B. Mail, Researchgate.org, Academia.edu)

-          Offene zeitschriften

o   Directory of Open Access Journals: <https://doaj.org>

o   Free Journal Network: <https://freejournals.org/current-member-journals/>

o   Journals finden: <https://service.tib.eu/bison/> 

o   TOP Factor

-          Predatory Publishers

o   Bealls Liste, https://beallslist.net

o   <https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5493177/>

o   <https://thinkchecksubmit.org> (oder bei Konferenzen: <https://thinkcheckattend.org>)

o   White list: <https://doaj.org>

o   Bei Bibliotheken nachfragen!

-          Monitoring

o   OpenAPC

o   <https://open-access-monitor.de>

o   <https://hal.science/hal-04121339v2>

o   Offener Datensatz mit von AI kodierten Indikatoren von Open Science des Verlages PLOS: <https://theplosblog.plos.org/2023/10/open-science-indicators-q2-2023/?utm_id=354&utm_source=internal&utm_medium=email&utm_campaign=354lib1023&utm_content=link>

-          Open Acces Typen

o   (moving wall

o   Green

o   Gold

o   diamond), siehe NRW OS Strategie  <https://zenodo.org/record/8322048>  s. 11-12

o   "guerilla open access" (sci-hub)

o  

-          Open Journals:

o   <http://www.theoj.org>

o   OJS

o   Directory of open access journals

-          Offene Bücherei Badge: <https://badge.openbiblio.eu>

-          DEAL Verträge mit Publishern: <https://deal-konsortium.de/publizierende>

-          Bücher (Open Textbooks): <https://content.iospress.com/articles/education-for-information/efi190260>

-          Editorial Mass Resignations

o   Sprachwissenschaften: Linguista à Glossa (Johan Rooryck; Vortrag am 10.01.2024)

§  Herausgeber (Wissenschaftler) hat sich um Zeitschrift gekümmert, Elsevier Vertrag um Typesetting

§  Wissenschaftler\*innen wollten Open Access Zeitschrift mit APCs, die transparent gemacht werden

§  2015: 4 weitere Zeitschriften, die durch Community geleitet wurden, haben ebenfalls Diamond OA durchgesetzt

§  Unterstützung durch Bücherei und Open Library of Humanities

§  Eine vollständige Community wurde erfolgreich zu Diamond OA verschoben

§  Zeitschrift als Vehikel für eine wissenschaftliche Community -- wie ein Auto, das von einer Familie gekauft wird: man lässt das alte zurück und nimmt das neue, Editors, Reviewers, Readers, sind alle dieselben Personen

§  Übergang zu OA ist schwierig

·         Finanzielle Ungewissheit

·         Impact Factors benachteiligen neue Journals, sind unabhängig von Content, Community, Mission Statement

·         Alle Herausgeber\*innen müssen zustimmen

·         Montgomery & Neylon 2019: „The value of a journal is the community it cfreates, not the papers it publishes" REF[\[LR42\]](#_msocom_42) 

·         Was eine Zeitschrift nicht sein sollte X[\[LR43\]](#_msocom_43) 

·         Shared Ownership à Zeitschrift kann nicht an Verlag verkauft werden

·         Content vs. Service: content controlled by academic community, technical services (publishing, typesetting) can be paid for

o   Neuroimage (Chris Chambers)

§  Übergang zu OA 2020 mit APC (mehrere tausend Dollar)

§  Pandemie war zeitgleich, Community fand es nicht gut, hat es aber mitgemacht

§  Wegen großem Erfolg wurden APCs erhöht (3150\$?)

§  Community hat viele Jahre hart gearbeitet und wird dann mit höheren Kosten bestraft, hat aber nichts von den Kosten

§  Chief Editor (Steve Smith) wollten Reduktion auf 2000\$ verhandeln, Elsevier wollte das nicht: „Market Forces support the current APC" und fand die Kosten absolut berechtigt. Solche Kosten sind eine große Hürde für globalen Süden.

§  Editorial Board hatte genug und alle haben resigniert. 3 Journals: Neuroimage, Neuroimage reports, Neuroimage clinical à \~40 Leute, haben das koordiniert, um öffentliche Wirkung zu maximieren (mit Medien, sozialen Medien, haben mit non-profit Publisher gesprochen)

§  Kommerzielle Verlage sind Parasiten, fügen der Wissenschaft keinen Wert hinzu, existieren nur, um Profit aus Wissenschaft zu ziehen und geben den Wissenschaftler\*innen fast nichts zurück (höchstens Typesetting)

§  Wichtige Message an alle: „from this point onwards, it is socially inacceptable to support these journals" "it's important that we do this all together"

§  Die Zeitschriften nun so gut wie Tod à nicht einfach ein anderes Journal erstellen, sondern klar machen, dass der Name nichts bedeutet, sondern die Community

§  Und das reicht noch nicht: Mass Resignations sollten eher wöchentlich stattfinden, sind aber ziemlich selten; strong editor in chief ist nötig; wir brauchen auch gar keine Publisher (Peer Community In Initiative zeigt das)

§  Verlag (Elsevier) hat wohl versucht, öffentliche Kommunikation über EMR zu beeinflussen und versucht, Forschende einzuschüchtern

o   Critical Public Health (Judith XXX)

§  Editorin hatte Moratorium für 1 Jahr, laut Vertrag durfte sie also 1 Jahr nach Ende des Vertrages Verbot, für eine Zeitschrift zu arbeiten

§  Verlag wollte Verträge ändern: statt „page budget" (5 issues pro Jahr mit vorgegebener Seitenzahl) „minimum number of papers" Modell; Editorial Board hatte dabei kein Mitspracherecht

§  Graduelle Erosion von akademischer Kontrolle

§  2023: Board unilateral resigned (erstes Mal, dass sich alle einig waren)

§  <https://cphn.net/breaking-news/>

§  Journal of Critical Public Health neu gegründet, über OJS gehostet

§  Schwierigkeit:

·         Netzwerk, dem das Journal gehört, musste formalisiert werden (bisher vor allem jiscmail Liste)

·         Nachhaltigkeit ist schwierig langfristig mit diamond Open Access

·         Keine Indizierung bisher (ISSN, IF, Abstracting)

·         Legale Fragen zum Intellectual Property unklar und nicht geprüft: Die Community hat Submission Guidelines usw. geschrieben, Verlag (Taylor and Francis) behauptet nun, dass diese Texte dem Verlag gehören. Wissenschaftler\*innen haben keine Kapazitäten für Gerichtsprozess.

o   Neuropathologie: XXX à Free Neuropathology

o   Verlage sind dagegen ausgestattet

§  Editorial Board wird rotiert, damit sich Editors nicht so stark vereinen können

§  Unwahrscheinliche Treffen in Person

§  Non-compete Klauseln / Moratorien

o   Liste von EMR: <https://retractionwatch.com/the-retraction-watch-mass-resignations-list/>

o   <https://sparceurope.org/the-seed-of-a-global-federation-for-diamond-open-access-has-been-planted/>

Eine besondere Form des öffentlichen Zugangs verbreitet sich aktuell in der biotechnologischen Forschung aus. Damit Wissenschaftler\*innen, Ingenieur\*innen, aber auch die allgemeine Bevölkerung Zugang zu Konstruktionen hat, wird dort zu Klemmbausteinen wie beispielsweise LEGO® Steinen [\[LR44\]](#_msocom_44) gegriffen (REF[\[LR45\]](#_msocom_45) ). So hat David Aguilar (Tabelle 1) beispielsweise einen prosthetischen Arm mit Klemmbausteinen entworfen.

### Pre-Prints

-          Stark angelehnt an Jonny Coates Vortrag (REF); recording sollte bald verfügbar sein über FZJülich

-          Definition: noch nicht von einem journal ge peer reviewet

-          In Corona-Pandemie viel Aufmerksamkeit erfahren von Forschenden, Politik, und Medien, weil Journals zu langsam sind (Veröffentlichungsprozess kann bis zu 10 Jahre dauern) <https://www.biorxiv.org/content/10.1101/2020.05.22.111294v3>

-          Qualität genau so gut: <https://pubmed.ncbi.nlm.nih.gov/36240832/>

o   Journals machen

-          Pre-Prints von fast allen Journals gedulded (sherpa romeo v2)

-          Können kommentiert werden, Probleme werden manchmal innerhalb von 2 Tagen aufgedeckt werden, statt mehreren Jahren <https://www.statnews.com/2020/02/03/retraction-faulty-coronavirus-paper-good-moment-for-science/>

o   Retractions in Journals sehr sehr schwierig, bei Preprints einfach

o   Retraction guidelines vom Committee on Publication Ethics (Barbour et al., 2019)

-          Schneller: <https://www.pnas.org/doi/10.1073/pnas.1511912112>

-          Alternatives Publishing Modell <https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.3000116>

o   Unabhängig von Publishern

o   Open Access

o   Schneller (<https://europepmc.org/preprints>)

o   Open Peer Review, funktioniert gut <https://asapbio.org/review-commons-9-months>

o   Kein Gatekeeping <https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0281659>

o   mit dem Modell wird experimentiert und geschaut, wie es funktioniert (Service zum Reviewen von Preprints: <https://www.reviewcommons.org>)

o   Zitat J. Coates: „I don't think that we need peer review" (REF, später nochmal prüfen oder ihn anschreiben, er müsste bald dazu was veröffentlichen)"

-          Tracker: <https://europepmc.org/preprints>

-          Preprints als Promotionsleistung statt veröffentlichte Paper

-          Günstiger: APCs revenue bei Springer Nature fast 600 Gazillionen!!! APCs werden immer teurer <https://direct.mit.edu/qss/article/doi/10.1162/qss_a_00272/118070/The-Oligopoly-s-Shift-to-Open-Access-How-the-Big>

-          in line mit EU Council calls und US government

-          Sorge: dass jemand vor einem kommt und etwas irgendwo veröffentlicht

o   Das geht auch bei Konferenzen schon immer, sogar bei veröffentlichten Papers

o   Bei Pre-Print gibt es eine DOI + Zeitstempel und alle können sehen, wer zuerst da war

o   Einige Zeitschriften prüfen auch schon, ob in einem Pre-Print was schon war

-          "Put scientists back in charge"

-          Institutionen, Funders, Reviewers: manche verpflichten schon, Preprints zu posten

-          Je nach Feld sind Pre-Prints unterschiedlich beliebt

o   Felder mit großer Sorge von Wissensdiebstahl möchten Forschung evtl. möglichst lange zurückhalten -- nicht ganz sinnvoll, weil Reviewer auch Ideen klauen können und dann sogar schwieriger zu fassen sind; mit Pre-Print ist öffentlich klar, wer zuerst wozu geforscht hat

o   In manchen Bereichen ist es nicht sinnvoll, wenn mehrere Forschende an derselben Sache arbeiten (z.B. zwei Forschungsgruppen können nicht gleichzeitig und unabhängig voneinander an einem bestimmten Ort archäologische Ausgrabungen machen)

### Ansätze gegen die Selektion spannender Ergebnisse

Die Ergebnisse einer Untersuchung sind das, was am wenigsten in der Hand der forschenden Person liegt (bzw. liegen sollte - immerhin interessiert uns ja die Wahrheit und nicht die Kompetenz Forschender, Daten möglichst stark zu schönen). Umso frustrierender ist es, dass Zeitschriften das Ergebnis als Kriterium zur Publikation verwenden. Unter der Vielzahl von Einreichungen werden vor allem diejenigen Artikel gewählt, die spannende Ergebnisse erzielt haben oder die ihre anfängliche Vermutung bestätigen konnten (*Confirmation Bias*). Die folgenden Ansätze lösen dieses Problem zum Beispiel dadurch, dass die Ergebnisse aus dem Begutachtungsprozess ausgeschlossen werden.

#### Results-blind peer review

Der einfachste Weg ist dabei, den Ergebnisteil einfach zu schwärzen oder wegzulassen. Verschiedene Zeitschriften bieten das als Option an. Da es sich hierbei zurzeit (2024) eher um eine Ausnahme handelt, ist den meisten Gutachtenden jedoch klar, dass vor allem diejenigen die Option zum results-blind peer review wählen, deren Ergebnisse nicht "hübsch genug" für den klassischen Weg sind.

#### Registered Report

Ein radikalerer Ansatz als die Begutachtung ohne Ergebnisteil ist die Begutachtung des Artikels, ohne dass Ergebnisse überhaupt existieren. Dieses Format heißt *Registered Report*. Dabei wird das Manuskript mit der zu prüfenden Theorie, Methodik, und dem Analyseplan bei der Zeitschrift eingereicht, ohne dass überhaupt Daten erhoben wurden. Kommt es zur Akzeptanz dieses „halbfertigen" Artikels (*in principle acceptance*), werden die Daten gesammelt, wie geplant ausgewertet, und es folgt eine weitere Begutachtungsrunde. Hierbei ist vorgeschrieben, dass die Autor\*innen nichts an den bereits verfassten Teilen verändern dürfen und die Gutachter\*innen im Nachhinein keine Kritik am bereits geprüften Vorgehen üben dürfen. Es geht nur noch darum, ob der Plan eingehalten wurde und ob die Schlussfolgerungen auf den geplanten Analysen fußen. Damit soll verhindert werden, dass Artikel abgelehnt werden, weil die Ergebnisse nicht spannend genug, innovativ genug, oder den Erwartungen entsprechend sind. Erste Untersuchungen können bereits nachweisen, dass sich damit die Qualität der Forschung gegenüber dem traditionellen Vorgehen verbessert (REF[\[LR46\]](#_msocom_46) ). Eine Übersicht über Zeitschriften, die dieses Format anbieten ist online verfügbar (<https://www.cos.io/initiatives/registered-reports> à Participating Journals).[\[LR47\]](#_msocom_47)  Ebenfalls wird dadurch deutlich, dass psychologische Forschung einem massiven Publikationsbias unterliegt (d.h. es werden vor allem Studien veröffentlicht, die ihre Vermutungen bestätigen konnten und kaum Studien, in denen das nicht geschah): REF [\[LR48\]](#_msocom_48) zeigten, dass der Anteil erwartungskonformer Ergebnisse bei Registered Reports mit 44% deutlich unter den in der Psychologie üblichen 96% liegt.

**Abbildung 6**

*Klassisches Vorgehen beim Forschen und Veröffentlichen im Vergleich mit Registered Reports: Bei letzterem wird bereits der Studienplan (für gewöhnlich in Form eines unvollständigen Berichtes) begutachtet und revidiert. Eine Ablehnung im zweiten Peer Review aufgrund der Ergebnisse ist nicht möglich.*

1.  

2.  Anleitung (ten rules paper): <https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1010571>

3.  

4.  metaROR so wie PCI RR: <https://easst.net/easst-review/easst-review-volume-421-july-2023/metaror-a-new-form-of-scholarly-publishing-and-peer-review-for-sts/>

5.  

#### Pre-Print basierte Modelle

Durch die immer häufigere Veröffentlichung von Pre-Prints, also noch nicht begutachteten Manuskripten, eröffnen sich für die Begutachtung neue Wege. Sogenannte Overlay Journals (elife) wählen unter Pre-Prints solche aus, die sie an Gutachtende schicken um deren Meinungen einzuholen. Sofern die Autor\*innen des Preprints einverstanden sind, erhalten sie dann Gutachten und ihr Artikel wird schließlich in der Zeitschrift veröffentlicht.

Ein Modell, das unabhängig von Zeitschriften arbeitet wird von Peer Community In (PCI) verwendet: Dabei werden Pre-Prints nicht zu Zeitschriften, sondern ohne Kosten zu einer PCI Initiative geschickt. Diese existieren für zahlreiche Fächer (z.B. XXX, XXX, XXX). Dort wird das Begutachtungsverfahren ähnlich wie bei klassischen Zeitschriften organisiert, nur statt am Ende im Idealfall eine *Akzeptanz* und eine Veröffentlichung gibt es von der PCI Initiative eine Empfehlung (*Recommendation*) und eine Veröffentlichung der Gutachten[\[LR49\]](#_msocom_49) . Gutachtende können selbst entscheiden, ob sie dabei anonym bleiben möchten. PCIs arbeiten mit fachspezifischen Zeitschriften zusammen. Sogenannte „PCI-friendly Journals" veröffentlichen dann die empfohlenen Pre-Prints ohne weitere Gutachten einzuholen. Dadurch sind sowohl der Mechanismus zur wissenschaftlichen Qualitätssicherung als auch der Selektionsmechanismus (*was* wird veröffentlicht) in den Händen der Forschenden. Dadurch, dass die Veröffentlichung in klassischen Zeitschriften als letzter Schritt optional ist, können sich wissenschaftliche Communities langfristig von Zeitschriften unabhängig machen. Ein Erklärvideo zu PCIs ist online verfügbar (<https://www.youtube.com/watch?v=4PZhpnc8wwo>). Die übergreifende Organisation ist nicht-kommerziell und wird seit ihrer Gründung 2017 von Forschenden geleitet (<https://peercommunityin.org/pci-structure-history/>). Eine spezifische PCI Initiative bezieht sich ausschließlich auf Registered Reports (PCI-RR).

Abbildung erstellen, die den Ablauf grob erklärt

Je nach Fach haben unterschiedlich viele Zeitschriften mit PCI Initiativen Vereinbarungen, dass sie die akzeptierten Artikel ohne eigenes Peer Review veröffentlichen. Mehr und vor allem bekannte teilnehmende Zeitschriften machen PCIs für Forschende attraktiver. Zeitschriften, die Interesse an einem PCI haben, aber die Begutachtung nicht aus der Hand geben möchten, können sich als „PCI interested Journals" listen lassen (vs. „PCI friendly journals"). Teilnehmende Zeitschriften sparen dadurch Arbeit und bleiben relevant, indem sich Ihre Funktion dahin verschiebt, dass sie thematisch relevante Forschung sammeln und disseminieren. In einem Fall hat bereits eine Zeitschrift, die als „PCI friendly" eine Vereinbarung mit PCI-RR hatte, ein Manuskript mit einer *Recommendation* zum erneuten Peer Review versendet und wurde sofort von den PCI Partnern entfernt. Forschende, die Gutachtenprozesse für PCIs organisieren möchten -- analog zu Herausgeber\*innen von klassischen Zeitschriften -- können *Recommender* werden und müssen dazu ein Mindestmaß an Wissen haben sowie eine Schulung absolvieren (<https://rr.peercommunityin.org/about/recommenders>).

**Tabelle 2**

*Zusammenfassung der verschiedenen Begutachtungsmodelle und der jeweiligen Art des Umgangs mit den Forschungsergebnissen*

+-------+--------------------------------------+-------+----------------------------------------------------------------------+-------+
|       |                                      |       |                                                                      |       |
+-------+--------------------------------------+-------+----------------------------------------------------------------------+-------+
|       | **Begutachtungsprozedur**            |       | **Ausblendung der Ergebnisse**                                       |       |
+-------+--------------------------------------+-------+----------------------------------------------------------------------+-------+
|       |                                      |       |                                                                      |       |
+-------+--------------------------------------+-------+----------------------------------------------------------------------+-------+
|       | Traditionell                         |       | Ergebnisse sind sichtbar und fließen in die Beurteilung ein          |       |
+-------+--------------------------------------+-------+----------------------------------------------------------------------+-------+
|       |                                      |       |                                                                      |       |
+-------+--------------------------------------+-------+----------------------------------------------------------------------+-------+
|       | Results-Blind Peer Review            |       | Ergebnisse liegen vor, werden den Begutachtenden jedoch vorenthalten |       |
+-------+--------------------------------------+-------+----------------------------------------------------------------------+-------+
|       |                                      |       |                                                                      |       |
+-------+--------------------------------------+-------+----------------------------------------------------------------------+-------+
|       | Registered Report                    |       | Ergebnisse liegen noch nicht vor                                     |       |
+-------+--------------------------------------+-------+----------------------------------------------------------------------+-------+
|       |                                      |       |                                                                      |       |
+-------+--------------------------------------+-------+----------------------------------------------------------------------+-------+
|       | Peer-Community-In Registered Report\ |       | Ergebnisse liegen noch nicht vor                                     |       |
|       | (PCI-RR)                             |       |                                                                      |       |
+-------+--------------------------------------+-------+----------------------------------------------------------------------+-------+
|       |                                      |       |                                                                      |       |
+-------+--------------------------------------+-------+----------------------------------------------------------------------+-------+

#### Open peer-review

So können Leser\*innen von Meta-Psychology oder dem Journal of Open Psychology Data zu jedem Artikel einen Review Report (XXX richiges Wort? Link einfügen) lesen und nachvollziehen, was die Reviewer genau getan haben und wie sich der Artikel dadurch entwickelt hat. Vereinzelten Studien zufolge (REF, REF) führt das offene Review zu faireren und positiveren Einschätzungen.

XXX: Arten von Open Access einfügen - Gold -- Diamond

Weitere Peer-Review Modelle

-          <https://reuse-dept.org> Replikation in Computer Science

Review Ampel

#### Meine Erfahrungen mit Peer Review

Peer Review ist brutal. Das ist meine persönliche Erfahrung mit dem Prozess. Schnell habe ich in meiner wissenschaftlichen Arbeit gelernt, dass es in vielen Fällen ein Glücksspiel ist: Dabei geht es in erster Linie nicht einmal darum, ob der Artikel akzeptiert wird oder nicht, sondern ob die Gründe für die Ablehnung konstruktiv sind oder nicht. Hierzu ein paar Beispiele:

Einreichung bei Collabra: Es handelte sich um einen Artikel, der zwischen den Disziplinen steht. Es geht nicht nur um Erwartungen, nicht nur um Produktbewertungen, nicht nur um die Methode, Daten direkt aus dem Internet herunterzuladen. Der "bunte-Vogel-Artikel" war bereits bei drei Zeitschriften abgelehnt worden. Bei keiner wurde er an die Reviewer weitergegeben, weil er nie zur Zeitschrift passte. Die zeischrift Collabra, bei der wir ihn schließlich einreichten, war zu dem Zeitpunkt noch wenige Jahre alt und war breit aufgestellt. Über ein halbes Jahr haben wir auf das Gutachten gewartet. Länger zu warten ist erstmal ein gutes Zeichen: Der Artikel wurde wohl an Reviewer rausgeschickt. In dem Fall wurden wir jedoch bitter enttäuscht: Es wurde nur XX Gutachter gefunden und die Kommentare waren nicht hilfreich. XXX NOCH GENAUER AUSFÜHREN

Einreichung bei Journal of Experimental Social Psychology: Wir hatten eine Replikation einer dort erschienen Studie durchgeführt - der Befund ließ sich nicht replizieren. Meine Überlegung war, der Zeitschrift, die den nicht robusten Befund ursprünglich publizierte, selbst die Chance zur Selbstkorrektur zu geben. Die Reviews waren fair und positiv, es gab ein paar Punkte zu diskutieren, aber uns war klar, dass es sich hier um Verständnisprobleme und keine inhaltlichen Aspekte handelte. Nicht so der Editor: Er erklärte, dass der Befund nicht neu genug wäre und klar wäre, dass es nicht replizierbar ist. Ich erklärte ihm, dass noch niemand den Befund zu replizieren versucht hatte und wir selbst sogar vor der Analyse der Ergebnisse eine Abstimmung darüber gemacht hatten, welches Ergebnis wir erwarteten: Es war sehr ausgewogen 50-50. Wir stellten klar, dass wir alle aufgezeigten Probleme einfach lösen können und gerne die Chance zur Revidierung hätten. Der Editor hätte dabei kaum Arbeit außer den Artikel anschließend nochmal an Gutachtende zu schicken. Auf unsere Mail erhielten wir nur eine kurze Antwort: XXX (nachschauen und einfügen). Hier befand ich mich an einem Scheideweg: Warum reagiert jemand patzig auf eine ehrliche Nachfrage zu seinen Gründen? Wir entschieden uns, einen anderen Editor direkt zu kontaktieren. Nach kurzer Zeit erhielten wir die Einladung zu einer Revision. Der Artikel wurde in der revidierten Fassung veröffentlicht.

Einreichung bei XXX: Die zentrale Aussage dieses Artikel war, dass verschiedene Messwerte einer angeblichen Eigenschaft nicht miteinander zusammenhängen. Der Befund stellte die Annahme infrage, dass es sich dabei überhaupt um eine Eigenschaft handelte. Die Ablehnungsgründe zweier Gutachtenden und der Herausgeberin machten deutlich: Niemand hatte den Artikel überhaupt gelesen. Ein Gutachter merkte an, dass etwas mit den Werten nicht stimme, weil sie laut einer der Tabellen nicht miteinander zusammenhängen. Genau das war ja unsere Aussage. Wir zeigten, dass es nich an unseren Daten lag, sondern sich in anderen Datensätzen so verhielt. Hätte er die Überschrift der Tabelle gelesen, den Absatz davor, oder den danach, wäre das klar geworden. Hat er aber nicht...

XXX noch Einreichung bei Meta-Psych als Kontrast?

Das sind nur kurze Auszüge aus dutzenden Einreichungen und Ablehnungen. Darüber hinaus kann fast jede\*r Forschende\*r von substanzlosen persönliche Beleidigungen berichten. Meiner Erfahrung nach ist anonymes versus öffentliches Peer Review wie ein Vergleich von anonymen Kommentarspalten mit nicht anonymen: Bei Anonymität beherrschen persönliche Beleidigungen und Unwahrheiten den Dialog.

#### Die Abgründe des anonymen peer-reviews: Eine Sammlung von Zitaten

·         "I tried to like it but I couldn't"

·         XXX hier auf Twitter nachfragen

### Open Science Incentives FORRT NOSI Badges Open Science in Berufungskommission

-          N-best evaluation (REF[\[LR50\]](#_msocom_50) ): nur die z.B. N = 5 für die ausgeschriebene Stelle wichtigsten Publikationen werden berücksichtigt, statt auszuzählen werden diese dann ganz genau angeschaut

-          StudySwap "However, a potential barrier to independent, prepublication replication attempts is that many researchers have a difficult time finding other labs to conduct such attempts. StudySwap can be used to find an independent research team to conduct a replication attempt of a not-yet-published study" (Chartier et al., 2018, p. 575). Zitat von: <https://osf.io/preprints/psyarxiv/dtvs7/> p. 11

-          Reviewer Initiativen und Boycotts\
[Richard D. Morey et al. (2016) The Peer Reviewers' Openness Initiative: incentivizing open research practices through peer review, Royal Society Open Science; DOI: 10.1098/rsos.150547. Published 13 January 2016](http://rsos.royalsocietypublishing.org/content/3/1/150547#sec-6)

### Replikationsforschung[\[LR51\]](#_msocom_51) : Ansehen und Kompetenz (Normalität) im Umgang mit Replikationsstudien steigern

-   

-   Unterscheidung Vertrauenskrise und Replikationskrise: <https://philarchive.org/archive/FEEWRI>

    -   

    -   Replikationsstudien können keine Forschung ersetzen, die auf ein tieferes Verständnis von Theorien abzielt

    -   

    -   Replizierbarkeit betrifft Robustheit, Mindestanforderungen an Verallgemeinerbarkeit, und Erhöhung des Vertrauens dadurch, dass QRPs und Datenfälschung weniger wahrscheinlich werden \[was, wenn original- und replikations-studie gefälscht sind?\]

    -   

    -   Kein Schwarz-weiß-denken: Unsicherheiten sind mit Replikations- und Original-Studien verbunden <https://journals.sagepub.com/doi/abs/10.1177/10755470241239947?journalCode=scxb>

    -   

-   

-   Es wird klar, was replizierbar sein sollte und was nicht. REF[\[LR52\]](#_msocom_52)  listen notwendige [\[LR53\]](#_msocom_53) Bedingungen dafür auf, dass „wahre" Befunde replizierbar sein können

    -   

    -   Wahre, statistische Phänomene können nicht replizierbar sein

        -   

        -   Methode könnte sehr ungenau sein und trotz „wahrem" Befund nur eine sehr geringe Replikationsrate haben (mathematischer Beweis dafür bei <https://royalsocietypublishing.org/doi/10.1098/rsos.200805#d1e454>)

        -   

        -   Mere-Measurement Effect

        -   

        -   Ethnografie, qualitative Methoden, Archäologie („Befund wird bei Ausgrabung aus Kontext gerissen und dadurch zerstört")

        -   

    -   

    -   Sachen, die keine allgemeinen Gesetzmäßigkeiten darstellen können trotzdem replizierbar sein

        -   

        -   Artefakte

        -   

        -   Unpassende Modelle, deren Annahmen inkorrekt sind, lassen sich manchmal reproduzieren und mit neuen Daten auch replizieren

        -   

        -   Heisenbug (?)

        -   

    -   

    -   Replizierbarkeit ist nicht hinreichend für Wahrheit

    -   

    -   Belief updating nach Replikationen <https://www.nature.com/articles/s41562-021-01220-7>

    -   

-   

-   Ziele von Replikationen[\[LR54\]](#_msocom_54) 

    -   

    -    

    -   

-   

-   Niemand repliziert

    -   

    -   <https://www.econstor.eu/bitstream/10419/267931/1/I4R-DP013.pdf>

    -   

    -   <https://osf.io/preprints/psyarxiv/fzngs>

    -   

    -   <https://journals.sagepub.com/doi/10.1177/1745691620979806>

    -   

    -   <https://peerj.com/articles/7654/>

    -   

    -   <https://doi.org/10.1016/j.respol.2018.07.019>

    -   

    -   <https://doi.org/10.1111/lang.12286>

    -   

    -   <https://journals.sagepub.com/doi/10.1177/0741932516646083>

    -   

    -   <https://journals.sagepub.com/doi/10.1177/1477370815578197>

    -   

    -   <https://journals.sagepub.com/doi/10.3102/0013189X14545513>

    -   

    -   <https://journals.sagepub.com/doi/10.1177/1745691612460688>

    -   

    -   <https://www.journals.uchicago.edu/doi/10.1086/506236>

    -   

    -   <https://osf.io/preprints/psyarxiv/sa6rc>

    -   

    -   <https://www.pnas.org/doi/abs/10.1073/pnas.2208863120>

    -   

    -   tweet von gilad: https://twitter.com/giladfeldman/status/1735950123291779119

    -   

-   

-   Geringes Ansehen verändert sich

    -   

    -   Vor 10 Jahren: <https://osf.io/preprints/psyarxiv/dtvs7/> p. 9 replications of Bem's pre-cognition study were desk-rejected by the editor of JPSP, Eliot Smith, who stated "This journal does not publish replication studies, whether successful or unsuccessful" and "We don't want to be the Journal of Bem Replication" (Aldhous, 2011).

    -   

    -   Jetzt immer mehr Replikation als innovativ: "Defining replication as a confrontation of current theoretical expectations clarifies its important, exciting, and generative role in scientific progress." <https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.3000691>

    -   

    -   Mehr Replikationen, aber zB in Soc Psych noch nicht Mainstream <https://psycharchives.org/en/item/74463c10-7347-4bf2-8156-5618d42c4e93>

    -   

    -   Replikationen Initiative: <https://i4replication.org/reports.html>

    -   

    -   <https://openresearch.amsterdam/image/2018/1/15/20180115_replication_studies_web.pdf>

    -   

    -   Ruf nach Zeitschrift (<https://s3.amazonaws.com/real.stlouisfed.org/wp/2015/2015-016.pdf>) erhört (JCRe)

    -   

    -   Diskussionen sind zum Teil noch hart und Replikationsstudien werden falsch dargestellt <https://osf.io/96pnj>

    -   

-   

-   Arten von Replikationsprojekten

    -   

    -   RRR: <https://www.researchgate.net/publication/366862422_A_Roadmap_to_Large-Scale_Multi-Country_Replications_in_Psychology>

    -   

    -   Registered Repots /PCI-RR (steht dazu s chon woanders etwas? Ist ja nicht explizit für replikationen)

    -   

    -   Abschlussarbeiten für Replikationen nutzen:

        -   

        -   <https://www.nature.com/articles/s41562-021-01192-8>

        -   

        -   Studierende als Replizierende: <https://link.springer.com/article/10.1007/s12286-023-00578-4>

        -   

        -   Hagen Cumulative Science Project:\
            <https://journals.sagepub.com/doi/10.1177/1475725719868149>

        -   

        -   Gilad Feldmans Projekte CORE <https://osf.io/5z4a8/>

        -   

    -   

-   

§  Leipziger Replikationsstudien: <https://home.uni-leipzig.de/lerep/>

§  CREP <https://www.frontiersin.org/articles/10.3389/fpsyg.2019.00247/full>

§  Laufende Großprojekte

§  I4R und Replication Games: <https://www.nature.com/articles/s41562-023-01807-2>

§  ZWB Lab²

§  Beteiligung von Original-Autor\*innen

§  <https://osf.io/preprints/psyarxiv/dwg9v>

-   

-   Veröffentlichung von Replikationen

    -   

    -   Vorschlag: jede Zeitschrift ist zuständig für Replizierbarkeit eigener Studien und MUSS Replikationsversuche veröffentlichen; pottery barn rule <https://thehardestscience.com/2012/09/27/a-pottery-barn-rule-for-scientific-journals/>

    -   

    -   Neue Journals

        -   

        -   Replikationen Journal: <https://www.jcr-econ.org>

        -   

        -   Reproducibility in Neuroscience: <https://jrn.epistemehealth.com/about/editorialteam/>

        -   

        -   Journal of Trial and Error <https://trialanderror.org>

        -   

        -   <http://rescience.org/x> Replication Journals

        -   

    -   

    -   Datenbanken für Replikationen

        -   

        -   Göttingen Replication Wiki <https://replication.uni-goettingen.de/wiki/index.php/Special:FormEdit/New_Replication> or <https://blog.repec.org/2020/08/04/a-replication-database-for-economics-and-social-sciences-the-replicationwiki/> (Jan H. Höffler) <https://doi.org/10.1045/march2017-hoeffler>

        -   

        -   CurateScience.org

        -   

        -   FORRT Replications and Reversals

        -   

        -   Replication Database (ReD)

        -   

    -   

    -   Auffinden von Replikationen: <https://arxiv.org/pdf/2311.15055.pdf>

    -   

-   

-   Ressourcen zum Lernen über Replikationen

    -   

    -   Review über Replikationen in quantitativer Soziologie: <https://www.annualreviews.org/doi/10.1146/annurev-soc-060116-053450>

    -   

    -   Wiki für Studierende / Webinar in Göttingen: <https://replication.uni-goettingen.de/wiki/index.php/Webinar_series:_Replicating_empirical_studies_in_economics_-_an_opportunity_for_students> Replication Wiki

    -   

-   

-   Replikationsmethodologie weiterentwickelt

    -   

    -   Terminologie wie am Anfang vorgestellt (Hüffmeier, Paper zu Generalizability von dem akademischen Rat aus Bamberg \[evolutionsbiologie\])

    -   

    -   Small telescopes approach

    -   

    -   Bayesian sample size planning <https://www.researchgate.net/publication/373047996_Bayesian_approaches_to_designing_replication_studies>\
        <https://link.springer.com/article/10.1007/s11749-023-00916-4>

    -   

    -   Selection of replication studies <https://www.sciencedirect.com/science/article/pii/S0010945223002691>

    -   

    -   <https://www.researchgate.net/publication/365181293_Bayesian_approaches_to_designing_replication_studies>

    -   

    -   Empfehlungen, wie Autor\*innen von Replikationsstudien mit den Originalautor\*innen kommunizieren sollten: <https://doi.org/10.1017/S1049096520000943>

    -   

    -   Kriterien für Bewertung: <https://docs.google.com/document/d/1p7GeOpwzQyTuzAWsD1w3zql2dS6mFgEhdf38Lc3uXC4/edit#heading=h.oti7bgwflneo>

        -   

        -   Problem beim Updaten von Materialien: wenn original, dann veraltet und unpassend -- wenn neu, dann anders und unpassend; siehe auch Kommentar von John Protzko <https://rr.peercommunityin.org/PCIRegisteredReports/articles/rec?id=750>

        -   

    -   

    -   Identifikation probleamtischer Forschungsliteratur:  Original Replication of Meta-Analyses or ORMA [https://www.researchgate.net/publication/365057816_ORMA_A_strategy_to_reduce_Psychology's_replication_problems](https://www.researchgate.net/publication/365057816_ORMA_A_strategy_to_reduce_Psychology's_replication_problems)

    -   

-   

§  Qualitative Forschung profitiert evtl. nicht von Replikationen <https://open.lnu.se/index.php/metapsychology/article/view/3764>

KASTEN / Leitfaden: Eine Replikationsstudie durchführen

1.      Wahl der Studie

·         Wichtigkeit der Originalstudie (Grundlegend, evtl. erkennbar durch viele Zitationen)

·         Ungewissheit der Originalstudie (Replikationswahrscheinlichkeit möglichst nicht 0 oder 1)

·         Bisher noch keine Replikationen

·         Diskussionen dazu beachten (z.B. via hypothes.is oder pubpeer.com à auf pubpeer.com Paper suchen und schauen, ob es dazu Kommentare gibt)

·         Machbarkeit (online möglich? Querschnitt? Instrumente Verfügbar bzw. bezahlbar?)

·         Etablierte Replikationsstandards für den Bereich? (bei FMRI Daten oder Längsschnittanalysen schwieriger als bei einfachen F- oder t-Tests)

·         Zusatz: Meta-analytische Indikatoren: auffällige P-Curve/Z-Curve, klare Hinweise auf Publikationsbias oder QRP

2.      Durchführung

·         Bestandsaufnahme der veröffentlichten Materialien und Daten

·         Daten berechnen

·   Bei verfügbaren Daten: komputationale Reproduzierbarkeit prüfen

·   Bei fehlenden Teststatistiken diese ausrechnen

·   Bei fehlenden Werten zu allem möglichen, evtl. andere Studien hinzuziehen

·         Kontaktaufnahme zu Originalautor\*innen, wenn möglich

·         Stichprobenplanung: nicht den Originalstichprobenumfang nehmen, sondern Poweranalyse-Ansätze nutzen

·   Small-Telescopes-Approach

·   Untere KI-Grenze

·   Äquivalenztests

·         Anpassung der Methode

·   Ggf. Update veralteter Materialien

·   Übersetzung in andere Sprache

·   Notwendigkeit einer besonderen Stichprobe?

·   Gibt es etablierte Standards bezüglich des statistischen Tests (v.a. bei standardisierten Effektstärken \[r, *d*, *f*\], nicht bei komplexen Verfahren wie RSA, Clustering, Decision Trees, FMRI Daten)

·   Gibt es ähnliche Replikationsstudien in dem Bereich, die für die Methode herangezogen werden können (z.B. via Replication Database suchen)

·         Anpassungen

·   Qualitätssicherung: Welche Veränderungen sind nötig oder sinnvoll? Was wissen wir neues, was wir berücksichtigen sollten?

·   Extensions: Lassen sich zusätzliche Items o.ä. „hinten" an die Studie anhängen, womit sich die externe Validität erhöhen könnte? Gibt es zusätzliche einfache Ergänzungen, bei denen eine Replikation nicht zu erwarten ist?

·         Präregistrierung mittels Template (Empfehlung: Replication Recipe, Brandt et al., 2014 <http://dx.doi.org/10.1016/j.jesp.2013.10.005>

·         Ggf. als PCI-RR submitten

3.      Analyse

·         Berechnung der Ergebnisse

·         Vergleich Original vs. Replikation

·         Vergleich naher vs. „ferner" Ergebnisse (z.B. bei denselben Items vs. Bei Extension \[falls vorhanden\]

4.      Diskussion

·         Bewertung des Ergebnis

·   LeBel Kriterien (signal/no signal + size)

·   Umgangssprache (success, failure, practical failure, inconclusive)

·         Abweichungen von Präregistrierung (jeweils Beschreibung, Begründung, und Auswirkung auf Ergebnisse)

·         Abweichung von Originalstudie (jeweils Beschreibung, Begründung, und ob es die Ähnlichkeit der Ergebnisse hätte beeinflussen können)

·         Bei anderen Ergebnissen Originalautor\*innen um Kommentar bitten

5.      Bericht

·         Manuskript: Orientierung an state-of-the Art von Gilad Feldman (LINK/REF) und an I4R Replication Report Template (<https://osf.io/j2qrx>)

·         Eintragung in Replication Database (via Fragebogen oder StaRT)

·         Registrierung der Ergebnisse über Replication Recipe („post completion")

·         Ggf. Verweis auf Replikationsstudie unter der Originalstudie bei pubpeer.com

·         Preprint: auf passendem Server hochladen

·         Zeitschrift: Originalzeitschrift (meistens kein Interesse), Collabra: Psychology, PCI (wenn es eins gibt), Meta-Psychology (designierte Section für Replikationen); Hinweise zu Zeitschriften hier: <https://i4replication.org/publishing.html>

### Modellierung von Replizierbarkeit

Alle Studien in einem Wissenschaftsbereich zu replizieren ist aktuell unrealistisch und würde enorme Ressourcen benötigen. Daher befinden sich verschiedene Methoden in Entwicklung, um auf Basis von Eigenschaften einer Studie vorherzusagen, welche Replikationsergebnis zu erwarten ist. Im repliCATS Projekt (REF[\[LR55\]](#_msocom_55) ), das Teil des SCORE Projektes zur Messung wissenschaftlicher Qualität ist, werden Einschätzungen von Forschenden, Reproduktionsversuche, und Replikationsversuche kombiniert. Ähnliche Projekte verwenden komplexe statistische Modelle (z.B. Large-Language-Models) oder meta-analytische Methoden wie p-curve oder Z-curve zur Vorhersage von Replikationsraten. Solche Modelle benötigen üblicherweise sehr viele Beobachtungen und Vorhersagen sind nur auf der Ebene von hunderten Studien sinnvoll. Beispielsweise rechneten Boyce et al. 2023 und Hagen Cumulative Science project Moderationsanalysen, prüften also welche Eigenschaften von Studien sich auf das Replikationsergebnis auswirken (z.B. Wechsel von Studie im Labor zu Online-Studie). Mittels umfassender Datenbanken wie der FORRT Replication Database können in Zukunft präzisere Modelle erstellt werden. Aktuelle Ergebnisse sind als vorläufig und mit Vorsicht zu interpretieren, da sie auf nicht-zufälligen Stichproben basieren, die Studieneigenschaften nicht systematisch und zufällig variiert wurden, und Replikationen solcher meta-analytischen Befunde schwierig sind.

### Veröffentlichung aller Ergebnisse

-          <https://opentrials.net/about/>

-          Unveröffentlichte Medizinische Studien: Quest Dashboard: <https://quest-cttd.bihealth.org/#tabStart>

-          File drawer reports

o   Meta psychology

o   psychfiledrawer.org meta-psychology

-          Datenbanken, die auch unveröffentlichte Ergebnisse aufnehmen (auch Lösung für cumulative science)

o   Bodypositions, OpAQ Community Augmented: metalab.stanford, metabus, <https://camarades.shinyapps.io/AD-SOLES/> , ...

o   Replication database

o   Metapsy <https://www.metapsy.org>

o   Metalab stanford <https://langcog.github.io/metalab/>

+------+---------------------------------+------+---------------------------------------------------------------------------------------------+------+---------------------------------------+------+
|      |                                 |      |                                                                                             |      |                                       |      |
+------+---------------------------------+------+---------------------------------------------------------------------------------------------+------+---------------------------------------+------+
|      | **Name**[\[LR56\]](#_msocom_56) |      | **Link**                                                                                    |      | **Topics**                            |      |
+------+---------------------------------+------+---------------------------------------------------------------------------------------------+------+---------------------------------------+------+
|      |                                 |      |                                                                                             |      |                                       |      |
+------+---------------------------------+------+---------------------------------------------------------------------------------------------+------+---------------------------------------+------+
|      | MetaLab                         |      | [langcog.github.io/metalab](https://langcog.github.io/metalab)                              |      | Early Language, Cognitive Development |      |
+------+---------------------------------+------+---------------------------------------------------------------------------------------------+------+---------------------------------------+------+
|      |                                 |      |                                                                                             |      |                                       |      |
+------+---------------------------------+------+---------------------------------------------------------------------------------------------+------+---------------------------------------+------+
|      | metaBUS                         |      | [metabus.org](http://metabus.org/)                                                          |      | All of social sciences                |      |
+------+---------------------------------+------+---------------------------------------------------------------------------------------------+------+---------------------------------------+------+
|      |                                 |      |                                                                                             |      |                                       |      |
+------+---------------------------------+------+---------------------------------------------------------------------------------------------+------+---------------------------------------+------+
|      | SOLES                           |      | [camarades.shinyapps.io/AD-SOLES/](https://camarades.shinyapps.io/AD-SOLES/)                |      | Animal models of Alzheimer's disease  |      |
+------+---------------------------------+------+---------------------------------------------------------------------------------------------+------+---------------------------------------+------+
|      |                                 |      |                                                                                             |      |                                       |      |
+------+---------------------------------+------+---------------------------------------------------------------------------------------------+------+---------------------------------------+------+
|      | OpAQ                            |      | [t1p.de/openanchoring](https://t1p.de/openanchoring)                                        |      | Anchoring Effects                     |      |
+------+---------------------------------+------+---------------------------------------------------------------------------------------------+------+---------------------------------------+------+
|      |                                 |      |                                                                                             |      |                                       |      |
+------+---------------------------------+------+---------------------------------------------------------------------------------------------+------+---------------------------------------+------+
|      | FReD                            |      | [t1p.de/ReD](https://t1p.de/ReD)                                                            |      | Replications (all fields)             |      |
+------+---------------------------------+------+---------------------------------------------------------------------------------------------+------+---------------------------------------+------+
|      |                                 |      |                                                                                             |      |                                       |      |
+------+---------------------------------+------+---------------------------------------------------------------------------------------------+------+---------------------------------------+------+
|      | Power Posing                    |      | [metaanalyses.shinyapps.io/bodypositions](https://metaanalyses.shinyapps.io/bodypositions/) |      | Body Position Effects                 |      |
+------+---------------------------------+------+---------------------------------------------------------------------------------------------+------+---------------------------------------+------+
|      |                                 |      |                                                                                             |      |                                       |      |
+------+---------------------------------+------+---------------------------------------------------------------------------------------------+------+---------------------------------------+------+
|      | Metadataset                     |      | [metadataset.com](https://www.metadataset.com/)                                             |      | Agriculture                           |      |
+------+---------------------------------+------+---------------------------------------------------------------------------------------------+------+---------------------------------------+------+
|      |                                 |      |                                                                                             |      |                                       |      |
+------+---------------------------------+------+---------------------------------------------------------------------------------------------+------+---------------------------------------+------+
|      | METAPSY                         |      | <https://www.metapsy.org>                                                                   |      | Psychotherapy                         |      |
+------+---------------------------------+------+---------------------------------------------------------------------------------------------+------+---------------------------------------+------+
|      |                                 |      |                                                                                             |      |                                       |      |
+------+---------------------------------+------+---------------------------------------------------------------------------------------------+------+---------------------------------------+------+

Anleitungen:

metaUI: <https://github.com/lukaswallrich/metaui>

Dynameta: <https://github.com/gls21/Dynameta>

Materialien: <https://cama.psychopen.eu>

-          Journal of Articles in Support of the Null Hypothesis

-          Journal of Negative Results <https://www.jnr-eeb.org/index.php/jnr/about>

### Umgang mit Fehlern

-          Fehlerkultur

-          Retraction

o   Standardisierte Retraction Notes: <https://www.researchgate.net/publication/374082265_A_tale_of_three_retractions_a_call_for_standardized_categorization_and_criteria_in_retraction_statements>

o   Wissenschaftliche Fehler können extreme Folgen haben; Lancet MMR Autism Fraud <https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2831678/>

o   Einige Wissenschaftler\*innen prüfen große Mengen an Artikeln, Elisabeth Bik z.B. hat zu Retractions von fast 600 und Korrekturen von fast 500 Artikeln beigetragen (<https://www.buzzfeednews.com/article/stephaniemlee/elisabeth-bik-didier-raoult-hydroxychloroquine-study>)

o   Aktuell Studie zur Sichtbarkeit von Retractions mit Möglichkeit, sich über zurückgezogene Artikel, die man zitiert, informieren zu lassen (RetractoBot, <https://www.retracted.net>) vom Bennett Institute for Applied Data Science at the University of Oxford.

-          Pubpeer.com

o   Beispiel einer Diskussion eines Papers vom Nobelpreisträger Thomas Südhof <https://pubpeer.com/publications/DAF32F6DB6C166337E5381F769AE52#1>

o   Hohe Aufmerksamkeit führt zu Korrektur vieler Fehler, zum Beispiel von 12 Fehlern (Stand: Dezember 2023) im Südhof Lab (<https://med.stanford.edu/sudhoflab/science-resources/integrity---pubpeer.html>)

-          Error project (malte elson)

### Offene Lehre / Open Teaching / Open Educational Resources

-          [https://zpid.cloud.panopto.eu/Panopto/Pages/Sessions/List.aspx?utm_campaign=ptos_videos#folderID="2936633e-384a-4cf1-a8ea-aee500adc653"&notificationBannerShown=true](https://zpid.cloud.panopto.eu/Panopto/Pages/Sessions/List.aspx?utm_campaign=ptos_videos#folderID=%222936633e-384a-4cf1-a8ea-aee500adc653%22&notificationBannerShown=true)

-          Repositorien für Bildungs- und Lehrmaterialien (Beispiele u.a. von <https://leibniz-psychology.org/practices-and-tools-of-open-science/open-science-in-der-lehre>)

o   <https://www.oerbw.de>

o   <https://www.twillo.de/oer/web/>

o   <https://portal.hoou.de>

o   <https://zenodo.org>

o   Osf.io

o   <https://forrt.org/syllabus/> (spezifisch open science)

-          Offene Webinare, zB zu Replikationsforschung <https://acm-rep.github.io/2024/posters/ACM_REP24_paper_30.pdf>

-          Anleitungen und Glossare (Parsons et al., 2022) zum Beispiel wundervoll gesammelt bei FORRT.org

-          Open Scholarship Knowledge Base (OSKB)

-          ROSiE[\[LR57\]](#_msocom_57)  Knowledge Hub

-          Open Economics Guide <https://openeconomics.zbw.eu/en/>

-          ARIADNE

## Methoden

Große Menge an Lösungsvorschlägen, für einzelne Forschende oft überwältigend

Wegweise:

-          Ariadne

-          Kumu Access2perspectives <https://kumu.io/access2perspectives/open-science#disciplines/>

#### Meta-Analyses und Reviews

-          use meta-analysis for replication studies <https://www.researchgate.net/publication/360848749_Replication_Is_for_Meta-Analysis>

-          Ricthlinien im Hinblick auf neue Standards: <https://osf.io/preprints/psyarxiv/59vsw>

##### Assessing publication bias

-          [https://www.researchgate.net/publication/363583595_Assessing_Publication_Bias_a_7-Step_User's_Guide_with_Best-Practice_Recommendations](https://www.researchgate.net/publication/363583595_Assessing_Publication_Bias_a_7-Step_User's_Guide_with_Best-Practice_Recommendations)

-          Toolbox: <https://www.sciencedirect.com/science/article/abs/pii/S0148296323005489>

-          z-curve und z-curve validation preprint

-          Test for excess significance (Ioannidis & Trikalinos, 2007)

-          Incredibility Index (Schimmack, 2012)

-          Funnel plot (Light & Pillemer, 1984)

-          P-curve (Simonsohn, Nelson, & Simmons, 2014)

-          Z-Curve (Bartos & Schimmack, 2020)

-          Sensitivitätsbatterie (bushman kepes röseler körner)

-          RoBMA bayesian <https://onlinelibrary.wiley.com/doi/full/10.1002/jrsm.1594>

-          Cumulative science

o   Machine readable manuscripts

o   Reporting templates (e.g., Brandt et al., 2014, replication recipe post completion registration on OSF, REF successes and failures)

o   Dynamic meta analyses

§  Bodypositions,

§  OpAQ Community Augmented:

§  metalab.stanford,

§  metabus,

§  soles, ...

§  Replication database

§  ReproSci <https://reprosci.epfl.ch>

§  <https://portlandpress.com/clinsci/article/137/10/773/233083/Systematic-online-living-evidence-summaries>

§  metaUI r Paket

o   Multiverse analyses

o    

### Reproducibility Checks / Reproduzierbarkeit

-          Meta-Psychology

o   Societies: Besteht auf Reproducibility: <https://doi.org/10.15626/MP.2023.4020>

-          Statcheck

o   <https://www.mdpi.com/2306-5729/1/3/14>

o   M nuijten paper

-          <https://osf.io/mydzv/>

-          Guidelines: https://docs.google.com/document/d/1p7GeOpwzQyTuzAWsD1w3zql2dS6mFgEhdf38Lc3uXC4/edit#heading=h.gjdgxs

-          Container / Capsules

-          Journal für Computational Reproducibility: <http://rescience.github.io>

-          Übergreifendes Gebiet: **Research Software Engineering** (RSE)

o   <https://arxiv.org/pdf/1609.00037.pdf>

-          Großangelegte Reproduktionsprojekte

o   Vor allem zwei Projekte, beteiligte Personen überschneiden sich, sind aber unabhängig

o   Verwenden ordinalen Reproducibility Score

§  Erklärung <https://bitss.github.io/ACRE/assessment.html#levels-of-computational-reproducibility-for-a-specific-display-item>

o   Social Science Reproduction Platform, <https://www.socialsciencereproduction.org/metrics>

§  Leitfaden für Durchführung von Reproduktionen: <https://www.socialsciencereproduction.org/metrics>

o   Institute for Replication (I4R)

§  Eigene Datenbank

§  Internationale Events „Replication Games"

§  Vor allem Ökonomie und Politikwissenschaften (REF[\[LR58\]](#_msocom_58) )

§  Ergebnisse von 110 Reproduktionen „Mass reproducibility and replicability: a new hope" <https://econpapers.repec.org/paper/zbwi4rdps/>

o   Open Code wird seitens Zeitschriften und zukünftig eventuell auch für den Abschluss von Datennutzungsverträgen mit Panels nötig (SOEP hat zB nur 20% open code, <https://www.wifa.uni-leipzig.de/fileadmin/Fakultät_Wifa/Institut_für_Theoretische_Volkswirtschaftslehre/Professur_Makroökonomik/Economics_Research_Seminar/ERS-Paper_Marcus.pdf>)

### Robustness checks

-          Multiverse analysis <https://www.researchgate.net/publication/367204012_EEGManyPipelines_A_large-scale_grass-root_multi-analyst_study_of_EEG_analysis_practices_in_the_wild>

#### Markdown Documents

-   

-   Computational Reproducibility Pilot Project: <https://psyarxiv.com/k8d4u/> \| HIER AUCH DISKUTIERT ALS SERVICE (könnte ULB machen)

-   

interactive multiverse / multiverse analysis

Statistik

-          Bayesianische Statistik

-          Verbannung von NHST bei Basic and Applied Psychology, bisher mit negativen Auswirkungen, die die Probleme eventuell noch verschärfen (<https://www.tandfonline.com/doi/full/10.1080/00031305.2018.1537892#:~:text=Part%20of%20the%20BASP%20editors,their%20expository%20text%20with%20a>)

\#### Transparency Checklist

### Open Data

-          Abbruch einer Promotion, Wechsel der Institution, usw. können dazu führen, dass wertvolle Zeit & Arbeit verloren gehen; im Extremfall wurden Tiere aufgezogen, operiert, und Forschung kann nicht weitergeführt werden wegen schlechter Dokumentierung

-          Vor Durchführung der Studie sollte klar sein, wie die Daten aussehen werden (Variablen, Bedeutung, Umfang, usw.)

-          <https://www.datalad.org>  

-          Myths: <https://www.sortee.org/blog/2024/04/12/2024_open_data_myths/>

-          <https://leibniz-psychology.org/das-institut/drittmittelprojekte/datawiz-ii>

-          originalstudie hierzu,

-          Vanpaemel et al., 205,

-          und das: <https://www.researchgate.net/publication/370001752_Data_sharing_upon_request_and_statistical_consistency_errors_in_psychology_A_replication_of_Wicherts_Bakker_and_Molenaar_2011>

-          Repositorien für daten

o   Osf.io

o   Psych archives zpid

o   Re3

o   Universitätsspezifische (z.B. datastore der Uni Münster)

o   Fachspezifische (info auch hier: <https://forschungsdaten.info/themen/veroeffentlichen-und-archivieren/repositorien/>)

§  Economics & social sciences: data.gesis.org

§  Earth science and life science: Pangaea.de

§  Humanities, cultural studies: de.dariah.eu

§  Linguistics: [www.clarin.eu](http://www.clarin.eu)

§  Biology: gfbio.org

§  Materialwissenschaften: nomad-lab-eu

§  Psychologie: Researchbox.org

o   Coscine <https://about.coscine.de>

o   Zenodo (interdisziplinär)

o   Git

o   Open Source Lösungen statt proprietärer Software, <https://www.mkw.nrw/hochschule-und-forschung/digitalisierung-hochschule-und-wissenschaft/forschungsdatenmanagement-fdm> <https://www.nfdi.de>

o   <https://www.researchobject.org/ro-crate/>

o   <https://www.protocols.io>

o   <https://www.icpsr.umich.edu/web/pages/ICPSR/index.html>

o   Für Qualitative Daten: <https://qdr.syr.edu/deposit/data>

-          Kriterien für Daten teilen: FAIR

o   Pyramid of requirements <https://content.iospress.com/articles/information-services-and-use/isu805>

o   Halbautomatische Extraktion von Meta-Daten <https://www.nfdi4chem.de/de/lister-halbautomatische-metadatenextraktion-aus-kommentierter-experimentdokumentation-in-elabftw/>

o   Datenstruktur: <https://psych-ds.github.io>

o   Fair Aware Tool zum Testen des Wissens zu FAIR <https://fairaware.dans.knaw.nl>

o   Ohne Fair kostet es Europäische Wirtschaft jedes Jahr 10.2 Milliarden Euro <https://publications.europa.eu/resource/cellar/d375368c-1a0a-11e9-8d04-01aa75ed71a1.0001.01/DOC_1>

o   Feld-spezifische Templates für FAIRe daten: <https://www.cos.io/blog/cedar-embeddable-editor?utm_source=linkedin&utm_medium=social>

-          CARE

o   Collective benefit: Ergebnisse sind auch von Gesellschaft verwendbar, Bürgerbeteiligung

o   Authority to control:

o   Responsibility: verschiedene Sprachenu und Weltanschauungen werden berücksichtigt

o   Ethics: Wertschätzung

-          Zeitschriften

o   JOPD (psychologie)

o   Ing.grid (ingenieurswissenschaften) <https://www.inggrid.org>

o   Weitere wichtige?

-          Beispiele für Datenbanken

o   <https://db.mocoda2.de/c/home> Chatverläufe

o   OpAQ Ankereffekte

o   Cooperation database

o   Stanford meta analysis aggregator

o   [http://metabus.org#](http://metabus.org)

o   Listen

§  Aaron Charlton Mktgo.org

§  <https://replications.clearerthinking.org/replications/>

-          Teilen von qualitativen Daten, dort ist Anonymisierung relative schwierig: <https://doi.org/10.1177/25152459231205832>

-          Data parasites / data police / data terrorists

-          Nachfragen: <https://www.whatdotheyknow.com/request/trial_protocols_behavioural_insi_2>

-          Mehr Zitate? <https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0230416>

-          Workshop-Unterlagen: <https://j-5chneider.github.io/PTOS-open-data/>

-          Infosammlung zu Forschungsdaten: <https://github.com/UB-Mannheim/awesome-RDM>

-          Daten auch wertvoll als Gedächtnis der Gesellschaft, soetwas sollte archiviert werden, z.B. <https://liebesbriefarchiv.de>

o   Daten von Personen aus allen Feldern der Gesellschaft (crowdsourced) und öffentlich verfügbar

o   Andere Beispiele für Citizen Science: <https://www.buergerschaffenwissen.de> und <https://wissenschaft-im-dialog.de/mitmachen/>

-          Langzeitarchivierung vs. Archivierung

o   1000 Jahre <https://archiveprogram.github.com>

### Replicability

\#### Assuring Replicability in Primary Research

1.  

2.  Andere Felder, Soziologie, Präregistrierung: <https://www.researchgate.net/publication/368454140_Preregistration_and_Registered_Reports_in_Sociology_Strengths_Weaknesses_and_Other_Considerations>

3.  

4.  Irise project (EU-gefördertes Projekt): <https://irise-project.eu/research-outputs>

5.  

6.  Messungenauigkeit: <https://osf.io/2me7t>

7.  

#### Preregistration / Präregistrierung

-          Erklärung

-          Gütekriterien: Struktur + Vollständigkeit (simmons nelson vs. Pham consumer psych)

-          Analyseplan wichtig, um wirklich P-Hacking vorzubeugen: <https://www.journals.uchicago.edu/doi/abs/10.1086/730455>

-          <https://www.researchgate.net/publication/375575020_Preregistration_in_practice_A_comparison_of_preregistered_and_non-preregistered_studies_in_psychology>

-          High replicability neues project <https://www.nature.com/articles/s41562-023-01749-9>

-          Prävalenz: <https://datacolada.org/115>

-          Es sollte nicht nur über Bereitschaft von Forschenden gehen, sondern auch im Peer Review verankert werden, sauber zu präregistrieren: <https://journal.trialanderror.org/pub/reflections-on-preregistration/release/2>

-          Deviations:

o   Alle weichen ab: <https://osf.io/preprints/psyarxiv/nj4es> , <https://osf.io/f2z7y> slide 12/21

o   niemand prüft auf Abweichungen: <https://osf.io/preprints/psyarxiv/nh7qw>

o   Transparent changes: <https://osf.io/6fk87>

o   How to deviate: <https://osf.io/preprints/psyarxiv/ha29k>

-          Repositorien

o   [https://meta-meta-resources.org/running-studies/preparation/pre-reg-repos/#](https://meta-meta-resources.org/running-studies/preparation/pre-reg-repos/)!

o   <https://www.alltrials.net>

o   <https://clinicaltrials.gov>

o   <http://www.crd.york.ac.uk/PROSPERO/>

-          Auswirkung auf Ergebnisse

o   Wahrscheinlichkeit positiven Befunden statt \>90% dadurch nur 40-50%, Scheel, Schijen, & Lakens(2021) \[siehe auch <https://osf.io/f2z7y>, slide 17/21\]

o   <https://drive.google.com/file/d/1gcyBE78tb9zerl4M35uS3npVGvMp-MPZ/view> The effectiveness of preregistration in psychology: Assessing preregistration strictness and preregistration-study consistency olmo van den akker[\[LR59\]](#_msocom_59) 

\##### Pre-Registration templates what is a good prereg? (see Simmons et al) What to preregister? Analysis script

OSF arbeitet neue ein <https://www.cos.io/blog/call-for-submission-for-preregistration-templates>

-   

-   Diskrepanz zwischen Präregistrierung/Prereg und Publikation: <https://bmjopen.bmj.com/content/13/10/e076264.long>

-   

-   Kritik: <https://journals.sagepub.com/doi/10.1037/gpr0000135>

-   

-   Prereg als Mittel zur Transparenz in der Forschungsplanung auch für Exploration sinnvoll, bei Hypothesentesten v.a. Eliminierung von Freiheitsgraden und Erhöhung von Vertrauen

-   

#### Internal Replications

<https://perception.yale.edu/papers/17-Scholl-APSObserver.pdf>

#### Power Analysis

-          Power

-          Small-telescopes approach

-          Bayesianischer Ansatz

o   So lange erheben, bis Bayes-Faktor konvergiert; kann Stichprobeunumfang reduzieren, wurde so zB für Verhaltensforschung bei Tieren empfohlen (<https://www.nature.com/articles/s41684-023-01308-9>)

#### New statistics

-          Frequentistische Statistik wird benutzt, ist oft missverstanden und Inhalte sind mit anderen Perpsektiven vermischt (<https://journals.sagepub.com/doi/10.1177/0959354314546157?icid=int.sj-full-text.similar-articles.3>)

o   Gigerenzer P-Value unwissen

-          Bayesian, Kritik: <https://journals.sagepub.com/doi/10.1177/25152459231213371>

-          Effektstärken statt p-Werte

o   Equivalence testing

o   Rölle von kleinen Effekten: <https://journals.sagepub.com/doi/full/10.1177/17456916221100420>

o   practical relevance of small effect sizes <https://www.researchgate.net/publication/352412241_Evaluating_the_practical_relevance_of_observed_effect_sizes_in_psychological_research>

o   effect sizes guide

o    

-          Alternative lakens paper "alternative to p-value is correctly used p-value"

o   Philosophische Diskussion über Nutzen von P-Werten: <https://journals.sagepub.com/doi/full/10.1177/09593543231160112> und hier <https://journals.sagepub.com/doi/10.1177/0959354312465483?icid=int.sj-full-text.similar-articles.1>

#### 21 Word Solution

#### Open Materials

#### Transparency

-          Bewertungskriterien: <https://osf.io/preprints/psyarxiv/djmcq>

-          Transparency Checklist

### Qualitative Forschung und Open Science

-          Pseudonymisierung von Daten ist hier sehr besonders, denn...: [https://doi.org/10.1177/160940692110346](https://doi.org/10.1177/16094069211034641)

-          Präregistrierungs-Template: <https://docs.google.com/document/d/16vLoRAs6RmXKy0v1RCx6osSaYk4r64Hgl6S4_KtWbeM/edit#heading=h.fwbi14d4b65g>

-          Sharing sensitive qualitative data: <https://www.youtube.com/watch?v=-Pha3EINF3s>

o   Berichte von Opfern von Vergewaltigungen à müssen anonymisiert werden, extrem aufwändig, kann stellvertretene Traumata auslösen

### Open Source Software

-          Research Software Engineering (RSE) \[siehe auch oben bei reproducible code, aber hier sollte das meiste dazu stehen\]

-          Non proprietär à Gefahr vor Lock-in

-          GNU Lizenz

-          Open Source Bedeutung

-          Bibliotheken, die Wissen verwalten, positionieren sich neu, um nun Infrastruktur zu stellen (z.B. [https://www.cambridge.org/core/books/abs/data-science-in-the-library/toward-reproducibility-academic-libraries-and-open-science/6FB18D7BCD8B82E597ED0D4B390C72EE#](https://www.cambridge.org/core/books/abs/data-science-in-the-library/toward-reproducibility-academic-libraries-and-open-science/6FB18D7BCD8B82E597ED0D4B390C72EE))

-          Von Chan-Zuckerberg-Initiative Unternehmen: <https://chanzuckerberg.com/rfa/essential-open-source-software-for-science/>

-          Alternative zu Clarivate Analytics Web of Science Literaturdatenbank: <https://openalex.org> REF[\[LR60\]](#_msocom_60) 

+--------+-----------------------------------+--------+-------------------------------+--------+---------------------------------+--------+
|        |                                   |        |                               |        |                                 |        |
+--------+-----------------------------------+--------+-------------------------------+--------+---------------------------------+--------+
|        | **Infrastruktur**                 |        | **Kommerzielle Software**     |        | **Open Source Software**        |        |
+--------+-----------------------------------+--------+-------------------------------+--------+---------------------------------+--------+
|        |                                   |        |                               |        |                                 |        |
+--------+-----------------------------------+--------+-------------------------------+--------+---------------------------------+--------+
|        | Literaturdatenbank                |        |                               |        | <https://openalex.org>          |        |
+--------+-----------------------------------+--------+-------------------------------+--------+---------------------------------+--------+
|        |                                   |        |                               |        |                                 |        |
+--------+-----------------------------------+--------+-------------------------------+--------+---------------------------------+--------+
|        | Literaturverwaltung               |        | Citavi, Mendeley              |        | Zotero                          |        |
+--------+-----------------------------------+--------+-------------------------------+--------+---------------------------------+--------+
|        |                                   |        |                               |        |                                 |        |
+--------+-----------------------------------+--------+-------------------------------+--------+---------------------------------+--------+
|        | Datenerhebung                     |        | Unipark, Millisecond Inquisit |        | PsychoPy                        |        |
+--------+-----------------------------------+--------+-------------------------------+--------+---------------------------------+--------+
|        |                                   |        |                               |        |                                 |        |
+--------+-----------------------------------+--------+-------------------------------+--------+---------------------------------+--------+
|        | Datenanalyse                      |        | IBM SPSS, Stata               |        | GNU R, Python, Jupyter Notebook |        |
+--------+-----------------------------------+--------+-------------------------------+--------+---------------------------------+--------+
|        |                                   |        |                               |        |                                 |        |
+--------+-----------------------------------+--------+-------------------------------+--------+---------------------------------+--------+
|        | Begutachtung und Veröffentlichung |        | Editorial Manager             |        | Open Journal System             |        |
+--------+-----------------------------------+--------+-------------------------------+--------+---------------------------------+--------+
|        |                                   |        |                               |        |                                 |        |
+--------+-----------------------------------+--------+-------------------------------+--------+---------------------------------+--------+

### Slow Science

Viele der Entwicklungen lassen sich als „langsame Wissenschaft" (oder engl. Slow Science) zusammenfassen. Der traditionellen Produktion von Forschungsartikeln steht die achtsame Auseinandersetzung entgegen. Dass viele der vorgeschlagenen Lösungen im Wettbewerb um veröffentlichte Forschungsartikel nachteilig sind, kritisiert beispielsweise Hyman (2024[\[LR61\]](#_msocom_61) ). Er schlägt stattdessen vor, die Probleme mittels künstlicher Intelligenz zu lösen. Verlage wie Elsevier benutzen diese beispielsweise bereits für Peer Review (<https://www.elsevier.com/about/policies-and-standards/the-use-of-generative-ai-and-ai-assisted-technologies-in-the-review-process>), wenngleich Modelle wie ChatGPT 4.0 nicht dafür geeignet sind (REF[\[LR62\]](#_msocom_62) ).

### Big Team Science

-          CRediT und Übersetzungen <https://github.com/contributorshipcollaboration/credit-translation>

## Theorien

Theorien übernehmen in der Wissenschaft oft die Funktion eines Wegweisers. Durch sie lässt sich einschätzen, wie die Ergebnisse einer Untersuchung aussehen, bevor sie durchgeführt werden. Falls die Ergebnisse also nicht wie erwartet sind, zum Beispiel wenn eine Replikation fehlschlägt, besteht der Verdacht, die Theorie könnte falsch oder unvollständig sein. In der Psychologie, in welcher Theorien häufig in Alltagssprache und nicht formalisiert (also als mathematische Formeln) notiert werden, ist der Sinn von formalen Varianten schon lange diskutiert (REF[\[LR63\]](#_msocom_63) , REF[\[LR64\]](#_msocom_64) , REF[\[LR65\]](#_msocom_65) ) und Replikationsfehlschläge werden darauf zurückgeführt, dass die Theorien zu unpräzise, missverständlich, und nicht gut ausgearbeitet sind. Es herrscht das Motto, Theorien seien wie Zahnbürsten und man solle niemals die von jemand anderem nehmen (REF[\[LR66\]](#_msocom_66) ). Daran, dass das Problem seit Jahrzehnten beschrieben wird, hat sich wenig geändert (REF[\[LR67\]](#_msocom_67) , REF[\[LR68\]](#_msocom_68) ). Inzwischen gibt es jedoch Bemühungen, das Problem auch zu lösen (REF[\[LR69\]](#_msocom_69) , REF[\[LR70\]](#_msocom_70) ), beispielsweise durch das herausstellen der Vorteile von bekannten formalen Theorien (REF[\[LR71\]](#_msocom_71) [\[LR72\]](#_msocom_72) ), Entwicklung von Werkzeugen zur Formalisierung von Theorien (<https://www.theorymaps.org>), oder durch einzelne Forschende. Der Wert von Replikationsstudien wird im Rahmen der „Theorie-Krise" darin gesehen, dass nur durch wiederholte Untersuchung festgestellt werden kann, unter welchen Bedingungen ein bestimmtes Phänomen nachweisbar ist (z.B. REF[\[LR73\]](#_msocom_73) ).

### Spezifizierung und Formalisierung

Das Problem einer Theorie in Alltagssprache lässt sich wie folgt veranschaulichen: REF[\[LR74\]](#_msocom_74)  versuchten, eine Studie zur Facial Feedback Hypothese zu replizieren. Die Hypothese besagt, dass Personen, wenn sie einen Stift so im Mund halten, dass dabei diejenigen Muskeln beansprucht werden, die auch beim Grinsen genutzt werden, Dinge lustiger finden, als wenn sie den Stift anders im Mund halten. Damit geprüft werden konnte, ob Versuchspersonen ihre Stifte richtig in den Mund nahmen, wurden sie in der Replikationsstudie gefilmt. Das war in der Originalstudie nicht der Fall. Die Replikationsstudien schlugen fehl und der Unterschied der Kamera wurde von den Autoren der Originalstudie als Grund genannt, weshalb die Replikation fehlschlug (REF[\[LR75\]](#_msocom_75) ). Ob und wie die Präsenz einer Kamera in dem Versuchsaufbau relevant ist, war nicht Teil der Theorie beziehungsweise Hypothese. Durch die Formulierung der Theorie in gewöhnlicher Sprache können unendlich viele solcher „Post Hoc" Erklärungen in die Diskussion eingebracht werden, die jegliche Replikationsfehlschläge relativieren. Bei einer formalen Theorie sind solche Missverständnisse seltener.

Aus dem Problem, dass Theorien schwer verständlich und wenig informativ sind, folgt die Tatsache, dass die meisten Theorien nie falsifiziert werden, aber auch nicht weiter untersucht werden („Zombie-Theorien", REF[\[LR76\]](#_msocom_76) ). Ähnlich verhält es sich mit psychologischen Skalen, bei denen die meisten nur einmal verwendet werden (REF[\[LR77\]](#_msocom_77) ). Flexible Theorien vereinfachen außerdem das p-Hacking: In der Theorie ist keine Vorgabe zum Aufbereiten der Daten, also kann diejenige Variante gewählt werden, die die Theorie bestätigt (REF[\[LR78\]](#_msocom_78) ).

### Gegnerische Kollaborationen

Ein Ansatz, bei dem verhindert werden soll, dass uninformative Studien durchgeführt werden und sich Diskussionen auf Verstehensweisen von Theorien fokussieren, sind gegnerische Kollaborationen (adversarial collaborations, REF[\[LR79\]](#_msocom_79) ). Dabei arbeiten Vertreter\*innen von zwei inkompatiblen Standpunkten miteinander und erstellen beispielsweise eine Studie, die den Konflikt beilegen soll (z.B. REF[\[LR80\]](#_msocom_80) , REF[\[LR81\]](#_msocom_81) ).

## Die Welt

Nachdem nun systemische, methodische, und theoretische Gründe für die Replikationskrise diskutiert wurden, endet die Diskussion mit einem allgemeinen und ontologischen Punkt: Die Erwartung, dass sich Befunde wiederholt feststellen lassen müssen beruht auf der Annahme oder Theorie, dass die Welt -- oder zumindest die jeweils in der Wissenschaft untersuchte Gesetzmäßigkeit -- stabil ist. Bei fluktuierenden Mustern in der Welt oder im menschlichen Verhalten sind Replikationsfehlschläge wenig überraschend. Für die Psychologie haben REF [\[LR82\]](#_msocom_82) und Gergen [\[LR83\]](#_msocom_83) (REF, siehe auch REF[\[LR84\]](#_msocom_84) )

-          Historizität (why psychology cannot be a science jan smedslund)

-          Indeed, the argument put forward by Gergen (1973) that most (if not all) effects in psychology are not stable across time and context was one of the starting points for the first crisis in social psychology. Gergen writes: "It is the purpose of this paper to argue that social psychology is primarily an historical inquiry. Unlike the natural sciences, it deals with facts that are largely nonrepeatable and which fluctuate markedly over time" (1973, p. 310). Nach Lakens two crises <https://osf.io/preprints/psyarxiv/dtvs7/> p. 12

o   Das ist aber unklar und auch nicht ausreichend erforscht

-          more research

-          evtl Ontologie[\[LR85\]](#_msocom_85) 

 [\[LR1\]](#_msoanchor_1)Überarbeiten, sodass alles drin ist, was diskutiert wird und Sprache einfach ist (ist die Abb nicht oben auch oder ist das die andere Version?

 [\[LR2\]](#_msoanchor_2)Auch so machen, dass open access (zugänglichkeit von wissen) dabei ist

 [\[LR3\]](#_msoanchor_3)REF

 [\[LR4\]](#_msoanchor_4)https://opusproject.eu/openscience-news/open-science-horizon-unescos-recommendation/

-           [\[LR5\]](#_msoanchor_5)<https://www8.cao.go.jp/cstp/kokusaiteki/g7_2023/2023_bestpracticepaper.pdf>

 [\[LR6\]](#_msoanchor_6)<https://www.consilium.europa.eu/en/press/press-releases/2023/05/23/council-calls-for-transparent-equitable-and-open-access-to-scholarly-publications/>

 [\[LR7\]](#_msoanchor_7)<https://www.ema.europa.eu/en/documents/regulatory-procedural-guideline/triggers-audits-good-laboratory-practice-glp-studies_en.pdf>

 [\[LR8\]](#_msoanchor_8)<https://zenodo.org/record/8322048> 

 [\[LR9\]](#_msoanchor_9)https://opal.landtag.nrw.de/portal/WWW/dokumentenarchiv/Dokument/MMV18-2422.pdf

 [\[LR10\]](#_msoanchor_10)https://www.jmwiarda.de/https-www.jmwiarda.de-2024-05-28-bitte-nachschaerfen/

 [\[LR11\]](#_msoanchor_11)Erhöhung der Offenheit an finnischen Universitäten <https://www.zbw-mediatalk.eu/de/2024/02/open-science-monitoring-offenheit-als-wichtiger-teil-guter-forschungspraxis-in-finnland/>

Netzwerk Nachhaltige Wissenschaft

<https://zenodo.org/records/11204833>

 [\[LR12\]](#_msoanchor_12)Das zu Universitäten?

 [\[LR13\]](#_msoanchor_13)Worum geht es dabei genau?

 [\[LR14\]](#_msoanchor_14)https://arxiv.org/abs/2205.01833

 [\[LR15\]](#_msoanchor_15)schönbrodt research practices I und II

 [\[LR16\]](#_msoanchor_16)<https://www.cambridge.org/core/books/abs/data-science-in-the-library/toward-reproducibility-academic-libraries-and-open-science/6FB18D7BCD8B82E597ED0D4B390C72EE>

 [\[LR17\]](#_msoanchor_17)<https://www.degruyter.com/document/doi/10.1515/bfp-2024-0008/html>

 [\[LR18\]](#_msoanchor_18)https://de.wikipedia.org/wiki/Zweitveröffentlichung

 [\[LR19\]](#_msoanchor_19)<https://doi.org/10.31234/osf.io/prx8w>

 [\[LR20\]](#_msoanchor_20)https://zenodo.org/records/7193838

 [\[LR21\]](#_msoanchor_21)ergänzen, Gesellschaft für Wirtschaftswissenschaften?

o    [\[LR22\]](#_msoanchor_22)[Public Open Science Institut für die Psychologie (ZPID)](https://leibniz-psychology.org/das-institut)

 [\[LR23\]](#_msoanchor_23)<https://openscience.cern>

o    [\[LR24\]](#_msoanchor_24)<https://www.apa.org/pubs/journals/resources/publishing-tips/transparency-openness-promotion-guidelines?utm_campaign=apa_publishing&utm_medium=direct_email&utm_source=businessdevelopment&utm_content=openscience_promo_11302023&utm_term=text_middle_learnmore>

 [\[LR25\]](#_msoanchor_25)<https://osf.io/bnh7p>

 [\[LR26\]](#_msoanchor_26)<https://joss.theoj.org>

 [\[LR27\]](#_msoanchor_27)erklären

 [\[LR28\]](#_msoanchor_28)<https://www.researchgate.net/publication/372509504_A_Guide_for_Social_Science_Journal_Editors_on_Easing_into_Open_Science>

 [\[LR29\]](#_msoanchor_29)Idee: Review als Kommentar bei Pubpeer posten bei Ablehnung

 [\[LR30\]](#_msoanchor_30)https://doi.org/10.1177/09567976231221573

 [\[LR31\]](#_msoanchor_31)<https://doi.org/10.1177/09567976231221573>

 [\[LR32\]](#_msoanchor_32)<https://osf.io/preprints/psyarxiv/5hkjz>

 [\[LR33\]](#_msoanchor_33)<https://s3.amazonaws.com/real.stlouisfed.org/wp/2015/2015-016.pdf>

 [\[LR34\]](#_msoanchor_34)link

 [\[LR35\]](#_msoanchor_35)<https://royalsocietypublishing.org/doi/10.1098/rsos.230206>

 [\[LR36\]](#_msoanchor_36)<https://royalsocietypublishing.org/doi/10.1098/rsos.230206>

 [\[LR37\]](#_msoanchor_37)boyce 2023 eleven years

 [\[LR38\]](#_msoanchor_38)hagen cumulative science project

 [\[LR39\]](#_msoanchor_39)forrt glossary

 [\[LR40\]](#_msoanchor_40)<https://osf.io/preprints/psyarxiv/bkxfq>

 [\[LR41\]](#_msoanchor_41)<https://open.lnu.se/index.php/metapsychology/article/view/3764>

 [\[LR42\]](#_msoanchor_42)https://blogs.lse.ac.uk/impactofsocialsciences/2019/03/29/the-value-of-a-journal-is-the-community-it-creates-not-the-papers-it-publishes/

 [\[LR43\]](#_msoanchor_43)

 [\[LR44\]](#_msoanchor_44)Hier Bild von Lego-Steinen einfügen (selbst schießen)

 [\[LR45\]](#_msoanchor_45)<https://doi.org/10.1016/j.tibtech.2022.02.003>

 [\[LR46\]](#_msoanchor_46)<https://osf.io/preprints/metaarxiv/7x9vy>

 [\[LR47\]](#_msoanchor_47)ergänzen:

·         [10.1038/s41562-021-01193-7](https://doi.org/10.1038/s41562-021-01193-7)

 [\[LR48\]](#_msoanchor_48)<https://journals.sagepub.com/doi/10.1177/25152459211007467>

 [\[LR49\]](#_msoanchor_49)Irgendwo diskutieren, warum negative gutachten nicht veröffentlicht werden

 [\[LR50\]](#_msoanchor_50)https://osf.io/preprints/psyarxiv/tzaqh

 [\[LR51\]](#_msoanchor_51)hier nochmal durchschauen: <https://lakens.github.io/statistical_inferences/17-replication.html#analyzing-replication-studies> 

und

<https://pubmed.ncbi.nlm.nih.gov/26214497/>

 [\[LR52\]](#_msoanchor_52)<https://royalsocietypublishing.org/doi/10.1098/rsos.200805#d1e454>

 [\[LR53\]](#_msoanchor_53)Kasten: Begriffserklärung für *Notwendige und hinreichende Bedingungen*

 [\[LR54\]](#_msoanchor_54)Fletcher (2020) describes the function of replication is to confirm one of the following claims regarding the findings of the original research: (1) they are not due to mistakes in data analysis, (2) they are not due to sampling error, (3) they do not depend on contextual factors, (4) they do not arise from fraud or questionable research practices, (5) they generalize to a larger population than that sampled in the original, (6) their aspects pertaining to the theoretical hypothesis of interest hold even when that hypothesis is operationalized or tested in completely different ways

<https://cornerstone.lib.mnsu.edu/cgi/viewcontent.cgi?article=2424&context=etds>

 [\[LR55\]](#_msoanchor_55)<https://www.youtube.com/watch?v=qa2_9NrEmeA>

 [\[LR56\]](#_msoanchor_56)aktualisieren (siehe ppt)

 [\[LR57\]](#_msoanchor_57)<https://rosie-project.eu/knowledge-hub/#!training/health-and-life-sciences>

 [\[LR58\]](#_msoanchor_58)<https://journals.sagepub.com/doi/10.1177/20531680241233439>

 [\[LR59\]](#_msoanchor_59)viele paper von denen dazu

 [\[LR60\]](#_msoanchor_60)https://arxiv.org/abs/2205.01833

 [\[LR61\]](#_msoanchor_61)<https://www.researchgate.net/publication/377965519_Freeing_social_and_medical_scientists_from_the_replication_crisis>

 [\[LR62\]](#_msoanchor_62)https://arxiv.org/abs/2402.05519#:\~:text=Practical%20implications%3A%20Overall%2C%20ChatGPT%20does,steps%20to%20control%20its%20use.

 [\[LR63\]](#_msoanchor_63)meehl

 [\[LR64\]](#_msoanchor_64)Severe testing(?) platt 1984

 [\[LR65\]](#_msoanchor_65)<https://doi.org/10.31219/osf.io/9amhe>

 [\[LR66\]](#_msoanchor_66)Herkunft des zitates rausfinden

 [\[LR67\]](#_msoanchor_67)Two crises lakens

 [\[LR68\]](#_msoanchor_68)muthukrishna

 [\[LR69\]](#_msoanchor_69)https://www.researchgate.net/publication/378966795_Replication_and_Theory_Development_in_the_Social_and_Psychological_Sciences

 [\[LR70\]](#_msoanchor_70)<https://www.researchgate.net/publication/375305901_Tension_Between_Theory_and_Practice_of_Replication>

 [\[LR71\]](#_msoanchor_71)Formal theories paper mit angst, partnerschaft

 [\[LR72\]](#_msoanchor_72)Robinaugh, D. J., Haslbeck, J. M., Ryan, O., Fried, E. I., & Waldorp, L. J. (2021). Invisible hands and fine calipers: A call to use formal theory as a toolkit for theory construction. *Perspectives on Psychological Science*, *16*(4), 725-743.

 [\[LR73\]](#_msoanchor_73)<https://journal.trialanderror.org/pub/tension-between-theory/release/1>

 [\[LR74\]](#_msoanchor_74)replication facial feedback

 [\[LR75\]](#_msoanchor_75)antwort auf RRR facial feedback

 [\[LR76\]](#_msoanchor_76)Ferguson, C. J., & Heene, M. (2012). A vast graveyard of undead theories: Publication bias and psychological science's aversion to the null. *Perspectives on Psychological Science*, *7*(6), 555-561.

 [\[LR77\]](#_msoanchor_77)Elson, M., Hussey, I., Alsalti, T., & Arslan, R. C. (2023). Psychological measures aren't toothbrushes. *Communications Psychology*, *1*(1), 25.

 [\[LR78\]](#_msoanchor_78)https://www.researchgate.net/publication/364419271_Selective_Hypothesis_Reporting_in_Psychology_Comparing_Preregistrations_and_Corresponding_Publications

 [\[LR79\]](#_msoanchor_79)Cleeremans, A. (2022). Theory as adversarial collaboration. *Nature Human Behaviour*, *6*(4), 485-486.

 [\[LR80\]](#_msoanchor_80)Mellers, B., Hertwig, R., & Kahneman, D. (2001). Do frequency representations eliminate conjunction effects? An exercise in adversarial collaboration. *Psychological Science*, *12*(4), 269-275.

 [\[LR81\]](#_msoanchor_81)Cowan, N., Belletier, C., Doherty, J. M., Jaroslawska, A. J., Rhodes, S., Forsberg, A., \... & Logie, R. H. (2020). How do scientific views change? Notes from an extended adversarial collaboration. *Perspectives on Psychological Science*, *15*(4), 1011-1025.

 [\[LR82\]](#_msoanchor_82)Dänemark, why psychology cannot be a science

 [\[LR83\]](#_msoanchor_83)REF siehe bei lakens

 [\[LR84\]](#_msoanchor_84)Lakens two crises

 [\[LR85\]](#_msoanchor_85)<https://psycnet.apa.org/record/2024-93999-001>
