---
title: "Methoden"
editor: visual
format: html
bibliography: references.bib
bibliographystyle: apa
---

In der Wissenschaft gibt es nicht *die eine Methode* [@Feyerabend.19752002], sondern Methoden werden für Probleme entwickelt, Probleme gelöst und Methoden weiterentwickelt oder fallen gelassen. Methoden sind also wie Werkzeuge, und nicht alles lässt sich mit einem Schraubendreher zusammenbauen. Mit Open Science Reformen kommen unzählige Methodische Neuerungen, Verbesserungen, und Vorschläge, die für Forschende häufig überwältigend sind: Forschungsprojekte voranbringen, Seminare und Vorlesungen halten, Drittmittel einwerben, und jetzt auch noch Open Science? Während viele Probleme auf eine unzureichende Methodenausbildung zurückgeführt werden [@lakens2021practical], die Forschende zu ihrer eigenen Last nachholen müssen, erleichtern einige Methoden die Arbeit. Für Promovierende wurde in der biologischen Psychologie beispielsweise der Wegweise ARIADNE entwickelt (https://igor-biodgps.github.io/ARIADNE/graph/graph.html). Dieses Kapitel bietet einen Überblick über methodische Entwicklungen und Diskussionen in den Sozialwissenschaften und Disziplinen, die vorrangig mit statistischen Methoden arbeiten.

#### Meta-Analysen

<<<<<<< HEAD
Unter Meta-Analysen werden Studien über Studien verstanden. Dabei extrahieren Forschende üblicherweise Ergebnisse aus bereits veröffentlichten Studien, schreiben andere Forschende aus einem Feld an und fragen nach unveröffentlichten Studien, und analysieren mit statistischen Methoden die Gemeinsamkeiten und Unterschiede zwischen den Ergebnissen. @fletcher2022replication argumentiert, dass nur mithilfe von Meta-Analysen die Allgemeingültigkeit von (statistischen) Phänomenen nachgewiesen werden kann. Im Idealfall könnten alle so weitermachen wie bisher und meta-analytische Modelle würden die Probleme korrigieren. Angesichts verheerender Publikationsbiases ist das allerdings aktuell nicht möglich. Wie Meta-Analysen dennoch informativ sein können, empfehlen @carlsson2024beginner allgemein und liste ich im Folgenden für spezifische Probleme auf.
=======
Unter Meta-Analysen werden Studien über Studien verstanden. Dabei extrahieren Forschende üblicherweise Ergebnisse aus bereits veröffentlichten Studien, schreiben andere Forschende aus einem Feld an und fragen nach unveröffentlichten Studien, und analysieren mit statistischen Methoden die Gemeinsamkeiten und Unterschiede zwischen den Ergebnissen. @fletcher2022replication argumentiert, dass nur mithilfe von Meta-Analysen die Allgemeingültigkeit von (statistischen) Phänomenen nachgewiesen werden kann. Angesichts verheerender Publikationsbiases ist das allerdings aktuell nicht möglich. Wie Meta-Analysen dennoch informativ sein können, empfehlen @carlsson2024beginner allgemein und liste ich im Folgenden für spezifische Probleme auf.
>>>>>>> fafde0efa311638f4b2cb750dea40208e22560f3

##### Publikationsbias und P-Hacking einschätzen

Bei Meta-Analysen gilt: "Garbage in, garbage out". Wer viele schlecht durchgeführte Studien in einer Meta-Analyse zusammenfasst, erhält eine schlechte Zusammenfassung. Das hatte beispielsweise zur Folge, dass @Hagger.2010 ihrer Meta-Analyse einen deutlichen Effekt für ein Modell über die Willensstärke finden konnten, nachfolgende, groß angelegte Replikationsversuche und Analysen jedoch alle scheiterten, einen ebenso großen Effekt zu finden [@Hagger.2016b; @Friese.2018; @Dang.2020; @vohs2021multisite]. Was trotzdem möglich ist und auch in jeder Meta-Analyse getan werden sollte, ist eine Einschätzung der Datenqualität, beispielsweise der Stärke des Publikationsbiases. Dabei gibt es Methoden, die prüfen, ob es nicht veröffentlichte Studien gibt, und Methoden, die für die potenziell fehlenden Studien korrigieren. Teilweise funktionieren diese erst bei über 200 Studien, manche lassen sich jedoch auch schon bei einem Dutzend Studien anwenden.

<<<<<<< HEAD
###### Funnel Plot

Eine der ältesten Methoden ist der Funnel Plot (Trichter-Diagramm) [@light1984summing]. Dabei werden Präzision und Effektstärke der einzelnen Studien in einem Diagramm dargestellt. Im Idealfall sollten die Punkte das Bild eines Trichters bilden: Je präziser eine Studie ist (zum Beispiel durch eine große Stichprobe), desto eher sollte der dort gemessene Zusammenhang im Mittel liegen. Unpräzisere Studien verschätzen sich unsystematisch, also sie liegen mal drüber und mal drunter. Dadurch, dass nicht signifikante Ergebnisse selten veröffentlicht werden, bildet sich in dem Trichter-Diagramm fast nie ein Trichter, sondern nicht-signifikante Ergebnisse fehlen einfach.

In der folgenden Abbildung ist ein Trichter-Diagramm für eine Studie zum Zusammenhang zwischen Angst vor Mathematik und Leistung in Mathematik. Das Muster ist fast symmetrisch, es liegt also nur ein schwacher Publikationsbias vor. Die kleinen Zusammenhänge am unteren Rand sind etwas nach rechts verzerrt, und die präzisen Effekte ganz oben sind nicht alle gleich groß, sondern variieren stark.

```{r}
#| code-fold: true
# Funnel plot example
library(psymetadata)
ds <- psymetadata::barroso2021

model <- metafor::rma.mv(yi = ds$yi, V = ds$vi, random = ~1 | study_id, data = ds)
metafor::funnel(model, xlab = "Beobachtete Effektstärke", ylab = "Präzision (Standardfehler)")

```

###### P-Curve

Beim P-Hacking werden Daten auf verschiedene Weisen ausgewertet und es wird diejenige berichtet, die einen niedrigen und damit signifikanten P-Wert zur Folge hat. Ergebnisse werden also nicht signifikant, weil die Hypothesen korrekt sind, sondern weil die Daten so lange ausgewertet wurden, bis sie signifikant wurden. Im Kapitel P-Hacking konnten wir sehen, wie P-Werte verteilt sind, je nachdem ob die Hypothese korrekt ist oder nicht. Diese Tatsache macht sich die P-Curve (@Simonsohn.2014; @Simonsohn.2014b; @Simonsohn.2015b) zu Nutzen. P-Werte eines Sets and Studien werden in einem Diagramm abgebildet und ihre Verteilung wird geprüft. Liegt kein P-Hacking vor sind die Werte entweder gleich verteilt (alle P-Werte kommen gleich häufig vor) oder sammeln sich bei 0 (kleinere P-Werte kommen häufiger vor). P-Hacking hat jedoch zur Folge, dass sich die Werte an der 5% Grenze tummeln, denn weiter als bis dort müssen die Daten nicht "gehackt" werden. Die Methode gewann vor allem deshalb an Bekanntheit, weil @Simmons.2017 die Methode auf eines der berühmtesten psychologischen Phänomene, dem Power Posing, angewandt haben und herausfanden, dass dort wahrscheinlich P-Hacking vorlag. Kritiker stellten später weitere Möglichkeiten vor, wie auch ohne P-Hacking eine suspekte P-Curve entstehen kann und das Verfahren wird inzwischen kaum mehr verwendet [@Erdfelder.2019].

###### Z-Curve

Statt P-Werte zu nehmen, können meta-analytische Befunde auch in sogenannte Z-Werte umgerechnet werden. Sie sind normalverteilt und mittels zusätzlicher Algorithmen lässt sich auf Basis von beobachteten Effekten schätzen, wie viele weitere Effekte es geben müsste. Dieses Verfahren namen Z-Curve [@Bartos.2022b] kann also für den File-Drawer Effekt und für P-Hacking korrigieren. Das Ergebnis daraus ist auch eine Schätzung, wie hoch die Replikationsrate wäre, wenn alle analysierten Studien erneut durchgeführt würden. Aktuellen Studien zufolge funktionieren diese Schätzungen ziemlich gut, obgleich sie eine große Menge an Daten benötigen [@Sotola.2022; @Sotola.2023; @roseler2023predicting].

###### Sensitivitätsanalyse

Ein Ansatz, welcher nicht nur bei Meta-Analysen sondern bei fast allen statistischen Auswertungen funktioniert, sind sogenannte Sensitivitäts- oder Robustheits-Analysen. Es werden dabei verschiedene Auswertungswege durchgegangen und dabei geprüft, wie stark sie sich auf die Ergebnisse auswerten. Bei Meta-Analysen können zum Beispiel viele mögliche Verfahren gleichzeitig gerechnet werden. Einen solchen Schrotschuss-Ansatz hat @Kepes.2017 geprägt, woraufhin er in anderen Studien übernommen wurde [@Korner.2022]. Einen Überblick über verschiedene Verfahren und unter welchen Bedingungen sie für Daten geeignet sind, bieten @Carter.2019.

##### Daumenregeln für Beurteilung einzelner Artikel

Eine Meta-Analyse ist aufwändig und kann mehrere Jahre dauern. Selbst Forschende, die keine Mittel für Studentische Hilfskräfte haben, die ihnen beim Codieren und Prüfen von hunderten bis tausenden Studien helfen, haben hier kaum eine Chance, eine ordentliche Analyse durchzuführen. Die folgenden Daumenregeln - und mehr als das sollen sie auch nicht sein - bieten Abkürzungen zur Beurteilung wissenschaftlicher Qualität.

###### Viele Signifikante Studien

Seit einer Vertrauenskrise in der Sozialpsychologie in den 1960er Jahren [@Lakens.2023] werden in Forschungsartikeln seitens vieler Zeitschriften mehrere Studien gefordert. Das hatte zur Folge, dass die Ressourcen statt in eine ordentliche Studie in mehrere kleinere Studien investiert wurden. Die meisten dieser "Multi-Study-Paper" haben dann ausschließlich signifikante Ergebnisse über bis zu 10 Studien hinweg. Während viele Studien mit durchweg signifikanten Ergebnissen auf den ersten Blick beeindruckend aussehen, lösen Sie beim näheren Hinsehen jedoch Skepsis aus: Einzelne Studien haben üblicherweise eine Wahrscheinlichkeit von 80-95%, dass dabei signifikante Ergebnisse bei der zentralen Analyse herauskommen. Diese Wahrscheinlichkeit (Statistische Teststärke oder Power) nimmt ab, wenn man mehrere Studien nacheinander durchführt. Es ist vergleichbar mit einem Schützen, der in 99% der Fälle mit einem Gewehr eine Glasflasche trifft. Die Wahrscheinlichkeit, dass er bei einem Schuss eine Glasflasche trifft ist also 99%. Die Wahrscheinlichkeit, dass er mit 50 Schüssen 50 Glasflaschen trifft ist weniger, nämlich 99%\^50 (hoch fünfzig) = 60,5%. Bei wissenschaftlichen Studien kommt es auf ähnliche Weise zu einer "Power-Deflation". Die Wahrscheinlichkeit, 4 signifikante Studien mit jeweils 80% Power durchzuführen, ist 40,96%. Dann eine genau solche Studie zu veröffentlichen ist extrem unwahrscheinlich [@Lakens.2017b].

###### Effektstärken "gerade so signifikant"

Angelehnt an die Logik der P-Curve ist es unwahrscheinlich, dass P-Werte zwischen 1 und 5% liegen. Aufgrund von p-hacking kommt es allerdings häufig vor. Ein P-Wert nahe 5% geht außerdem mit einem Konfidenzintervall der Effektstärke nahe 0 einher (z.B. @jané2024). Angenommen jemand führt zwei Studien zu einem Thema durch und beide haben P-Werte nahe 5% und ungefähr gleich große Versuchspersonen-Anzahlen, dann kommt die Frage auf, weshalb die Stichprobengröße für die spätere Studie nicht erhöht wurde: Auf Basis eines gerade so signifikanten Ergebnisses ist klar, dass man "Glück" hatte, da die statistische Power nicht sonderlich hoch war. Plant man also die Stichprobe für die nächste Studie, sollte man die erste Studie dabei zugrund legen und den Plan anpassen (z.B. @Lakens.2021).

```{r}
#| code-fold: true
rs <- c(seq(.01, .9, by = .01)) # Korrelationen, für die die Funktion ausgeführt wird
rsci <- sapply(rs, FUN = function(rs) {psychometric::CIr(r = rs, n = 250)}) # füre Funktion mehrfach aus

rci <- data.frame(t(rsci))
names(rci) <- c("lcl", "ucl")
rci$r <- rs

plot(rci$r, rci$r, type = "l", xlab = "Korrelation"
     , ylab = "Korrelation mit Konfidenzintervall"
     , main = "Breite von Konfidenzintervallen je nach Korrelationsgröße\nfür N = 250")
lines(rci$r, rci$ucl, lty = 2)
lines(rci$r, rci$lcl, lty = 2)
```

### Prüfung auf Reproduzierbarkeit

Wird ein Befund mit denselben Daten und idealerweise demselben Programm bzw. Analysecode erneut getestet und geprüft, ob dieselben Zahlen dabei rauskommen - also nicht nur, ob die Hypothese erneut bestätigt wird - dann handelt es sich um eine Reproduktion der Ergebnisse. Im Gegensatz zu einer Replikation werden also keine neuen Daten erhoben. Dass Ergebnisse reproduzierbar sind, sollte das absolute Minimum für wissenschaftliche Berichte sein, ist es jedoch noch längst nicht.

::: callout-important
## Begriffs-Wirrwarr

Während der Begriff *Replikation* in den Wirtschaftswissenschaften sowohl die Prüfung einer vorliegenden Studie mit neuen Daten, als auch die erneute Prüfung mit denselben Daten meint, wird für letzteres in der Psychologie *Reproduktion* verwendet. In der biologischen Forschung über Fortpflanzung, der Reproduktionsforschung, wird außerdem auf Reproduzierbarkeit ausgewichen. In wieder anderen Fällen wie der @OpenScienceCollaboration.2015 wird bei Replikationen (neue Daten) von "Reproducibility" gesprochen und wiederholte Tests mit denselben Daten werden "komputationale Reproduzierbarkeit" (computational reproducibility) genannt. Zuletzt verschwimmen in manchen Bereichen die Grenzen, wenn zum Beispiel bei einer Replikation der Befunde der Pisa Studie teilweise dieselben Daten und teilweise neue verwendet werden oder wenn die Daten computergeneriert sind und dasselbe Programm fähig ist mittels Pseudozufallszahlengenerator andere Daten zu generieren, die aber dieselbe Struktur haben.
:::

Seit wenigen Jahren führt die Zeitschrift Meta-Psychology als eine der ersten für alle veröffentlichten Artikel Reproduzierbarkeits-Prüfungen durch. Diese werden durch Forschende freiwillig oder im Rahmen ihrer Tätigkeit bei der Zeitschrift durchgeführt. Während diese Praxis bereits für andere Zeitschriften gefordert wurde [@lindsay2023plea], ist es jedoch noch immer die Ausnahme. Reproduktionschecks aller möglicher Disziplinen können bei Rescience veröffentlicht werden (<http://rescience.github.io>). Für das Jahr 2024 hat das Institute for Replications angekündigt, Studien aus der Zeitschrift Nature Human Behavior zu reproduzieren [@noauthor_2024-ta]. Nature Human Behavior ist eine der angesehensten Zeitschriften bei der Erforschung menschlichen Verhaltens, wobei angesehen nicht mit wissenschaftlicher Qualität gleichzusetzen ist. Sie wird vom Springer Verlag verwaltet und fordert mit Publikationskosten in Höhe von circa 9000€ pro Artikel die höchste Gebühr. Die strategische Entscheidung, sich auf die dortigen Artikel zu konzentrieren hat den Vorteil, dass Personen, die die Reproduzierbarkeits-Checks durchführen, diese eventuell dort veröffentlichen können und dass Reproduktionen große Aufmerksamkeit erfahren. Angesichts des Qualitätsanspruches solcher Zeitschriften an ihre Qualität und der Tatsache, dass kostenlose Zeitschriften wie Meta-Psychology die Prozedur ohne externe Hilfe durch das Institute for Replication durchführen können, bildet sich hier wieder das bekannte Bild ab, bei dem Verlage ihr Prestige dafür missbrauchen, kostenlose und profit-generierende Arbeit aus der Wissenschaft zu ziehen. Am Ende ist es wieder nicht die Zeitschrift selbst, die zur wissenschaftlichen Qualitätssicherung beiträgt, sondern das Institute for Replication.

Eine Abkürzung bei der Prüfung von Korrektheit, welche bei vielen Zeitschriften verwendet wird, ist das Programm [statcheck](statcheck.io). Es erkennt automatisch klassische statistische Tests und prüft auf Basis der berichteten Werte, ob diese konsistent sind. @hartgerink2016688 hat Ergebnisse aus über 50.000 Artikeln mit dem Programm geprüft und die Artikel mittels Pubpeer kommentieren lassen. Weil der Algorithmus in seltenen Fällen - wie in den Kommentaren offen dargelegt - fälschlicherweise Werte als fehlerhaft markiert und die Autor\*innen der Artikel zuvor nicht vor den Kommentaren gewarnt wurden, hat die [DGPs das Vorgehen verurteilt](https://www.dgps.de/schwerpunkte/stellungnahmen-und-empfehlungen/stellungnahmen/details/stellungnahme-des-dgps-vorstands-zur-praxis-der-automatischen-plausibilitaetsueberpruefung-wissenschaftlicher-arbeiten-mit-statcheck/). Die Antworten der Statcheck-Gruppe und von Christ Hartgerink sind nicht mehr verfügbar.

::: callout-note
## Reproduktion auf Knopfdruck

Mit sogenannten *Push-Button-Replications* ist gemeint, dass Ergebnisse ohne großen Aufwand und von allen Forschenden nachgerechnet werden können - auf Knopfdruck eben. Während sozialwissenschaftliche Zeitschriften mehr und mehr fordern, Daten und Analysecode so zu veröffentlichen, dass die Ergebnisse nachgerechnet werden, verkörpert die Zeitschrift Image Processing Online (IPOL, https://www.ipol.im) das Ideal dieses Vorgehen: Zu jedem dort veröffentlichten Artikel ist eine Demo verfügbar, bei der nach Auswahl eines Bildes, der in dem Artikel veröffentlichte Algorithmus live durchgeführt wird.
:::

#### Großangelegte Reproduktionsprojekte
=======
###### Funnel Plot, Eggers Test

(Light & Pillemer, 1984)

###### P-Curve

@Simonsohn.2014; @Simonsohn.2014b; @Simonsohn.2015b

Kritik: @Erdfelder.2019

###### Z-Curve

@Bartos.2022b

###### Sensitivitätsanalyse

Schrotflinten-Ansatz: @Kepes.2017; @Korner.2022

###### Weitere Meta-Analytische Korrekturen

@Carter.2019

##### Daumenregeln für Beurteilung einzelner Artikel

###### Viele Signifikante Studien

@Lakens.2017b

###### Toolbox

@Adler.2023

### Prüfugn auf Reproduzierbarkeit
>>>>>>> fafde0efa311638f4b2cb750dea40208e22560f3

In verschiedenen Forschungsdisziplinen gibt es großangelegte Projekte, Reproduzierbarkeit für vollständige Disziplinen zu schätzen. Ein Pionier auf dem Gebiet war das ReplicationWiki von Höffler (https://replication.uni-goettingen.de/). Nachfolgende Projekte wie das Replication Network (replicationnetwork3.wordpress.com) stützten sich weitesgehend auf die dort zusammengefassten Daten. Für die Wirtschaftswissenschaften berichteten @brodeur2024mass eine Reproduzierbarkeitsrate von 70% und in den Management Sciences bei 55% [@fivsar2024reproducibility]. Mit dem Insitute for Replication (I4R) überschneidet sich außerdem die Social Science Reproduction Platform des Berkley Initiative for Transparency in the Social Sciences (BITSS); https://www.bitss.org/resources/social-science-reproduction-platform/). Während das I4R voraussichtlich 2024 eine Datenbank mit allen Ergebnissen veröffentlicht, ist die Plattform der BITSS bereits verfügbar.

#### Open Code

Öffentlich verfügbare Daten und Code sind notwendig für Reproduktions- und Robustheits-Checks. Zeitschriften stehen hier zwischen der Entscheidung, Einreichungen schwieriger und sich selbst damit weniger attraktiv zu machen, indem sie höhere Anforderungen stellen, und die wissenschaftliche Qualitätssicherung zu fördern. Ein ähnliches Problem herrscht auch bei Betreibern von Panels, in denen regelmäßig große Befragungen oder Leistungstest, wie zum Beispiel die PISA Studie oder das Sozio-Ökonomische-Panel (SOEP). Bei Analysen der SOEP-Daten wird der Code nur in 20% der Fälle geteilt (<https://www.wifa.uni-leipzig.de/fileadmin/Fakultät_Wifa/Institut_für_Theoretische_Volkswirtschaftslehre/Professur_Makroökonomik/Economics_Research_Seminar/ERS-Paper_Marcus.pdf>)

### Robustheits-Analysen

Ähnlich wie die Sensitivitäts- oder Robustheitsanalysen lassen sich auch bei einzelnen Studien weitere Wege im "Garden of Forking Paths" gehen. Zur Erinnerung: Der Weg von Daten zu Ergebnissen ist lang und beinhaltet viele verschiedene Entscheidungen. Um zu zeigen, dass das Ergebnis eben nicht von diesen Entscheidungen abhängt, kann gezeigt werden, wie die Ergebnisse aussehen, wenn andere Entscheidungen getroffen werden würden. Der Extremfall dieser Robustheits-Analysen ist die *Multiversum-Analyse.* Hier wird versucht, alle möglichen Entscheidungen gleichzeitig zu treffen. Die daraus resultierenden Ergebnisse werden dann wieder in irgendeiner Form analysiert (z.B. gemittelt) oder dargestellt [@jacobsen2024preprocessing]. Eine weitere Möglichkeit ist die der *Multi-Analyst-Study*. Dabei geht es um die Abhängigkeit der Ergebnisse von den Entscheidungen verschiedener Forschenden und viele Personen analysieren die Daten unabhängig voneinander. Es wird schließlich geprüft, wie stark die Ergebnisse zwischen den Forschenden übereinstimmen.

In der folgenden Abbildung wurde für einen festen Datensatz (Fantasiedaten) verschiedene Analysemethoden verwendet. Dabei wurden verschiedene Typen von Korrelationen, verschiedene Stichprobenumfänge, und verschiedene Hypothesen verwendet. Das Ergebnis ändert sich dabei jedes mal ein bisschen, sodass der Wert zwischen 0,20 und 0,35 liegt, die positive (und signifikante) Korrelation bleibt aber erhalten.

```{r}
#| code-fold: true
library(MASS) # Paket laden

set.seed(1) # Zufallszahl festlegen, damit Ergebnisse immer identisch sind 
r_det <- .4 # einprogrammierte Korrelation
ds <- MASS::mvrnorm(n = 197, mu = c(0,0), Sigma = matrix(c(1, r_det, r_det, 1), ncol = 2, byrow = TRUE))
ds <- as.data.frame(ds)
names(ds) <- c("x", "y")


r <- data.frame("p", "estimate")

for (i in c(nrow(ds), 150)) {
  for (j in c("two.sided", "greater")) {
    for (k in c("pearson", "spearman", "kendall")) {
      for (l in F) {
        
        r <- rbind(r, unlist(cor.test(ds$x[1:i], ds$y[1:i], alternative = j, method = k, continuity = l)[3:4]))
        
      }
    }
  }
}

r <- r[-1, ]
names(r) <- c("p", "estimate")
plot(1:nrow(r), r$estimate
     , pch = 20
     # , col = ifelse(as.numeric(r$p) < .05, "red", "blue")
     , ylim = c(0, r_det+.2)
     , xaxt = 'n'
     , ylab = "Korrelation"
     , xlab = ""
     , bty = "l"
     )
abline(0,0)
```

#### Reproduzierbare Manuskripte

In der online-Version dieses Buches gibt es die Möglichkeit, vor Abbildungen einen Code anzeigen zu lassen. Mit diesem Code lässt sich die Abbildungen ganz einfach rekonstruieren bzw. reproduzieren. Auch Forschungsartikel können auf diese Weise geschrieben werden. Text, Programmiercode, sowie Ergebnisse als Zahlen, Tabellen, und Diagramme werden in einem einzelnen Programm geschrieben und Forschende ersparen sich das Kopieren oder Abtippen von Zahlen. Das Nachrechnen von Ergebnissen wird außerdem stark vereinfacht. Programmiersprachen basieren auf der sogenannten Markdown Sprache und in Programmen können unzählige weitere Programmiersprachen eingebettet sein.

Während Forschende oft nicht die Expertise oder die Zeit haben, ihre Manuskripte reproduzierbar zu gestalten, gibt es bereits Pilotprojekte [@baker2023reproduceme] und Zeitschriften [@Carlsson.2017], die Forschende unterstützen. In Kombination mit Multiversum-Analysen ist es in Forschungsartikeln im Internet außerdem möglich, den Text so zu erstellen, dass er interaktiv auf alternative Analyse-Entscheidungen reagiert. Leser\*innen können also im Manuskript Entscheidungen treffen und direkt sehen, wie sich die Ergebnisse ändern.

### Statistik

Es sind wahrscheinlich alle Forschungsdisziplinen, die Statistik verwenden, von der Replikationskrise betroffen. Ganz im Sinne von "post hoc ergo propter hoc" (danach, also deswegen), wird diese Tatsache häufig so interpretiert, dass das Verwenden statistischer Methoden die Ursache für die Replikationsprobleme ist. Während dagegen argumentiert wird, dass die Methoden nur *falsch* verwendet werden [@Lakens.2021b], schlagen manche Forschende auch Veränderungen oder Alternativen vor. Eine Gruppe 72 von Psycholog\*innen hat beispielsweise gefordert, die Signifikanzgrenze für neue Befunde von 5% auf 0,5% herunterzusetzen [@Benjamin.2018], sodass p-hacking erschwert wird. Andere schlagen vor, das Null Hypothesis Significance Testing (NHST, Null-Hypothesen-Signifikanztesten) komplett zu verbannen und andere Methoden zu verwenden: @Wagenmakers.2007 plädiert für Bayesianische Statistik, und die Zeitschfit Basic and Applied Psychology verbietet die Verwendung von Signifikanztests, was das Problem von falsch-positiven Befunden möglicherweise noch vergrößert hat [@fricker2019assessing].

### Open Data und Open Materials

-          Abbruch einer Promotion, Wechsel der Institution, usw. können dazu führen, dass wertvolle Zeit & Arbeit verloren gehen; im Extremfall wurden Tiere aufgezogen, operiert, und Forschung kann nicht weitergeführt werden wegen schlechter Dokumentierung

-          Vor Durchführung der Studie sollte klar sein, wie die Daten aussehen werden (Variablen, Bedeutung, Umfang, usw.)

-          <https://www.datalad.org>  

falsifizierte Daten können nur durch Sichtung identifiziert werden https://associationofanaesthetists-publications.onlinelibrary.wiley.com/doi/full/10.1111/anae.15263

-          Myths: <https://www.sortee.org/blog/2024/04/12/2024_open_data_myths/>

-          <https://leibniz-psychology.org/das-institut/drittmittelprojekte/datawiz-ii>

-          originalstudie hierzu,

-          Vanpaemel et al., 205,

-          und das: <https://www.researchgate.net/publication/370001752_Data_sharing_upon_request_and_statistical_consistency_errors_in_psychology_A_replication_of_Wicherts_Bakker_and_Molenaar_2011>

-          Repositorien für daten

o   Osf.io

o   Psych archives zpid

o   Re3

o   Universitätsspezifische (z.B. datastore der Uni Münster)

o   Fachspezifische (info auch hier: <https://forschungsdaten.info/themen/veroeffentlichen-und-archivieren/repositorien/>)

§  Economics & social sciences: data.gesis.org

§  Earth science and life science: Pangaea.de

§  Humanities, cultural studies: de.dariah.eu

§  Linguistics: [www.clarin.eu](http://www.clarin.eu)

§  Biology: gfbio.org

§  Materialwissenschaften: nomad-lab-eu

§  Psychologie: Researchbox.org

o   Coscine <https://about.coscine.de>

o   Zenodo (interdisziplinär)

o   Git

o   Open Source Lösungen statt proprietärer Software, <https://www.mkw.nrw/hochschule-und-forschung/digitalisierung-hochschule-und-wissenschaft/forschungsdatenmanagement-fdm> <https://www.nfdi.de>

o   <https://www.researchobject.org/ro-crate/>

o   <https://www.protocols.io>

o   <https://www.icpsr.umich.edu/web/pages/ICPSR/index.html>

o   Für Qualitative Daten: <https://qdr.syr.edu/deposit/data>

Daten automatisch anonymisieren lassen mit [AMNESIA](https://amnesia.openaire.eu/)

#### FAIR Kriterien für Forschungsdaten

o   Pyramid of requirements <https://content.iospress.com/articles/information-services-and-use/isu805>

o   Halbautomatische Extraktion von Meta-Daten <https://www.nfdi4chem.de/de/lister-halbautomatische-metadatenextraktion-aus-kommentierter-experimentdokumentation-in-elabftw/>

o   Datenstruktur: <https://psych-ds.github.io>

o   Fair Aware Tool zum Testen des Wissens zu FAIR <https://fairaware.dans.knaw.nl>

o   Ohne Fair kostet es Europäische Wirtschaft jedes Jahr 10.2 Milliarden Euro <https://publications.europa.eu/resource/cellar/d375368c-1a0a-11e9-8d04-01aa75ed71a1.0001.01/DOC_1>

o   Feld-spezifische Templates für FAIRe daten: <https://www.cos.io/blog/cedar-embeddable-editor?utm_source=linkedin&utm_medium=social>

#### CARE Kriterien für Forschungsdaten

o   Collective benefit: Ergebnisse sind auch von Gesellschaft verwendbar, Bürgerbeteiligung

o   Authority to control:

o   Responsibility: verschiedene Sprachenu und Weltanschauungen werden berücksichtigt

o   Ethics: Wertschätzung

#### Zeitschriften und Datenbanken für Forschungsdaten

Zeitschriften

o   JOPD (psychologie)

o   Ing.grid (ingenieurswissenschaften) <https://www.inggrid.org>

o   Weitere wichtige?

Beispiele für Datenbanken

o   <https://db.mocoda2.de/c/home> Chatverläufe

o   OpAQ Ankereffekte

o   Cooperation database

o   Stanford meta analysis aggregator

o   [http://metabus.org#](http://metabus.org)

o   Listen

§  Aaron Charlton Mktgo.org

§  <https://replications.clearerthinking.org/replications/>

-          Teilen von qualitativen Daten, dort ist Anonymisierung relative schwierig: <https://doi.org/10.1177/25152459231205832>

-          Data parasites / data police / data terrorists

-          Nachfragen: <https://www.whatdotheyknow.com/request/trial_protocols_behavioural_insi_2>

-          Mehr Zitate? <https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0230416>

-          Workshop-Unterlagen: <https://j-5chneider.github.io/PTOS-open-data/>

-          Infosammlung zu Forschungsdaten: <https://github.com/UB-Mannheim/awesome-RDM>

-          Daten auch wertvoll als Gedächtnis der Gesellschaft, soetwas sollte archiviert werden, z.B. <https://liebesbriefarchiv.de>

o   Daten von Personen aus allen Feldern der Gesellschaft (crowdsourced) und öffentlich verfügbar

o   Andere Beispiele für Citizen Science: <https://www.buergerschaffenwissen.de> und <https://wissenschaft-im-dialog.de/mitmachen/>

::: callout-note
## Was bedeutet Langzeitarchivierung?

Langzeitarchivierung vs. Archivierung

o   1000 Jahre <https://archiveprogram.github.com>
:::

### Replizierbarkeit erhöhen

Die bisherigen Ansätze wie Meta-Analysen oder Reproduzierbarkeits-Prüfungen können häufig für bestehende Projekte durchgeführt werden. Bei den folgenden gestaltet sich das allerdings schwieriger - sie sind vor allem für neue Forschung geeignet und haben das Ziel, die Replizierbarkeit von neu veröffentlichten Studien zu erhöhen. Wie im Fazit des Buches erörtert, ist eine allgemeine, fächerübergreifende Aussage darüber, ob diese Vorschläge sich tatsächlich auf Replizierbarkeit auswirken aufgrund von seltenen Replikationsstudien schwierig. So oder so ist ihr Sinn im Hinblick auf die Probleme von Nachvollziehbarkeit und P-Hacking deutlich.

\#### Assuring Replicability in Primary Research

1.  Replikationsstudien in Sozialpsychologie schwierig wegen Anreizstruktur und Ressourcen https://doi.org/10.1027/1864-9335/a000548

2.  Andere Felder, Soziologie, Präregistrierung: <https://www.researchgate.net/publication/368454140_Preregistration_and_Registered_Reports_in_Sociology_Strengths_Weaknesses_and_Other_Considerations>

3.  

4.  Irise project (EU-gefördertes Projekt): <https://irise-project.eu/research-outputs>

5.  

6.  Messungenauigkeit: <https://osf.io/2me7t>

::: callout-note
## Transparenz-Checkliste

@Aczel.2020 haben eine Checkliste entworfen, die Forschende Punkt für Punkt durchgehen können, um zu prüfen, ob ihr Forschungsbericht transparent ist. In der Online-App (https://www.shinyapps.org/apps/TransparencyChecklist/) kann anschließend ein Bericht daraus generiert werden, der an den Forschungsartikel angehängt werden kann.

Die Checkliste ist in die Themen Präregistrierung, Methoden, Ergebnisse, und Daten/Code/Materialien eingeteilt. Beispielsweise wird in Bezug auf die Ergebnisse gefragt, ob die Anzahl an Beobachtungen für alle Gruppen angegeben wurde. Die Checkliste ist aktuell in ca. 30 Sprachen verfügbar, darunter auch Deutsch. Die Liste von @Aczel.2020 ist allerdings primär für quantitative Studien geeignet. Für qualitative und gemischte Studien haben @symonds2024quality ein Bewertungsschema entworfen.

Eine weitere und kürzere Variante ist die 21-Worte-Lösung. Dabei wird eine vorgeschlagene Erklärung [@Simmons.2012] in den Bericht aufgenommen und versichert, dass keine Studien(ergebnisse) vorenthalten wurden. Sie ist bei weitem nicht so sicher und umfangreich wie die Transparenz-Checkliste, fördert aber niedrigschwellig die Auseinandersetzung mit Transparenz von Forschungsberichten.
:::

#### Preregistration / Präregistrierung

-          Erklärung

-          Gütekriterien: Struktur + Vollständigkeit (simmons nelson vs. Pham consumer psych)

-          Analyseplan wichtig, um wirklich P-Hacking vorzubeugen: <https://www.journals.uchicago.edu/doi/abs/10.1086/730455>

-          <https://www.researchgate.net/publication/375575020_Preregistration_in_practice_A_comparison_of_preregistered_and_non-preregistered_studies_in_psychology>

-          High replicability neues project <https://www.nature.com/articles/s41562-023-01749-9> \[hierzu Infobox, dass es umstritten ist, weil sie intransparent von der Präregistrierung abgewichen sind\]

-          Prävalenz: <https://datacolada.org/115>

-          Es sollte nicht nur über Bereitschaft von Forschenden gehen, sondern auch im Peer Review verankert werden, sauber zu präregistrieren: <https://journal.trialanderror.org/pub/reflections-on-preregistration/release/2>

##### Abweichungen von Präregistrierungen

o   Alle weichen ab: <https://osf.io/preprints/psyarxiv/nj4es> , <https://osf.io/f2z7y> slide 12/21

o   niemand prüft auf Abweichungen: <https://osf.io/preprints/psyarxiv/nh7qw>

o   Transparent changes: <https://osf.io/6fk87>

o   How to deviate: <https://osf.io/preprints/psyarxiv/ha29k>

##### Wo werden Studien präregistriert?

o   [https://meta-meta-resources.org/running-studies/preparation/pre-reg-repos/#](https://meta-meta-resources.org/running-studies/preparation/pre-reg-repos/)!

o   <https://www.alltrials.net>

o   <https://clinicaltrials.gov>

o   <http://www.crd.york.ac.uk/PROSPERO/>

##### Nachweise der Effektivität von Präregistrierungen

Effektivität wofür? Replizierbarkeit oder File-Drawer Problem?

o   Wahrscheinlichkeit positiven Befunden statt \>90% dadurch nur 40-50%, Scheel, Schijen, & Lakens(2021) \[siehe auch <https://osf.io/f2z7y>, slide 17/21\]

o   <https://drive.google.com/file/d/1gcyBE78tb9zerl4M35uS3npVGvMp-MPZ/view> The effectiveness of preregistration in psychology: Assessing preregistration strictness and preregistration-study consistency olmo van den akker[\[LR2\]](#_msocom_2) 

\##### Pre-Registration templates what is a good prereg? (see Simmons et al) What to preregister? Analysis script

OSF arbeitet neue ein <https://www.cos.io/blog/call-for-submission-for-preregistration-templates>

-   

-   Diskrepanz zwischen Präregistrierung/Prereg und Publikation: <https://bmjopen.bmj.com/content/13/10/e076264.long>

-   

-   Kritik: <https://journals.sagepub.com/doi/10.1037/gpr0000135>

-   

-   Prereg als Mittel zur Transparenz in der Forschungsplanung auch für Exploration sinnvoll, bei Hypothesentesten v.a. Eliminierung von Freiheitsgraden und Erhöhung von Vertrauen

-   

#### Power Analysis

-          Power

-          Small-telescopes approach

-          Bayesianischer Ansatz

o   So lange erheben, bis Bayes-Faktor konvergiert; kann Stichprobeunumfang reduzieren, wurde so zB für Verhaltensforschung bei Tieren empfohlen (<https://www.nature.com/articles/s41684-023-01308-9>)

#### New statistics

-          Frequentistische Statistik wird benutzt, ist oft missverstanden und Inhalte sind mit anderen Perpsektiven vermischt (<https://journals.sagepub.com/doi/10.1177/0959354314546157?icid=int.sj-full-text.similar-articles.3>)

o   Gigerenzer P-Value unwissen

-          Bayesian, Kritik: <https://journals.sagepub.com/doi/10.1177/25152459231213371>

-          Effektstärken statt p-Werte

o   Equivalence testing @Lakens.2017; @Lakens.2018

o   Rölle von kleinen Effekten: <https://journals.sagepub.com/doi/full/10.1177/17456916221100420>

o   practical relevance of small effect sizes <https://www.researchgate.net/publication/352412241_Evaluating_the_practical_relevance_of_observed_effect_sizes_in_psychological_research>

<<<<<<< HEAD
o   effect sizes guide jane guide to effect sizes and confidence intervals @jané2024
=======
o   effect sizes guide jane guide to effect sizes and confidence intervals
>>>>>>> fafde0efa311638f4b2cb750dea40208e22560f3

o    

-          Alternative lakens paper "alternative to p-value is correctly used p-value" @Lakens.2021b

o   Philosophische Diskussion über Nutzen von P-Werten: <https://journals.sagepub.com/doi/full/10.1177/09593543231160112> und hier <https://journals.sagepub.com/doi/10.1177/0959354312465483?icid=int.sj-full-text.similar-articles.1>

### Qualitative Forschung und Open Science

-          Pseudonymisierung von Daten ist hier sehr besonders, denn...: [https://doi.org/10.1177/160940692110346](https://doi.org/10.1177/16094069211034641)

-          Präregistrierungs-Template: <https://docs.google.com/document/d/16vLoRAs6RmXKy0v1RCx6osSaYk4r64Hgl6S4_KtWbeM/edit#heading=h.fwbi14d4b65g>

-          Sharing sensitive qualitative data: <https://www.youtube.com/watch?v=-Pha3EINF3s>

o   Berichte von Opfern von Vergewaltigungen à müssen anonymisiert werden, extrem aufwändig, kann stellvertretene Traumata auslösen

### Open Source Software

-          Research Software Engineering (RSE) \[siehe auch oben bei reproducible code, aber hier sollte das meiste dazu stehen\]

-          Non proprietär à Gefahr vor Lock-in

-          GNU Lizenz

-          Open Source Bedeutung

-          Bibliotheken, die Wissen verwalten, positionieren sich neu, um nun Infrastruktur zu stellen (z.B. [https://www.cambridge.org/core/books/abs/data-science-in-the-library/toward-reproducibility-academic-libraries-and-open-science/6FB18D7BCD8B82E597ED0D4B390C72EE#](https://www.cambridge.org/core/books/abs/data-science-in-the-library/toward-reproducibility-academic-libraries-and-open-science/6FB18D7BCD8B82E597ED0D4B390C72EE))

-          Von Chan-Zuckerberg-Initiative Unternehmen: <https://chanzuckerberg.com/rfa/essential-open-source-software-for-science/>

-          Alternative zu Clarivate Analytics Web of Science Literaturdatenbank: <https://openalex.org> https://arxiv.org/abs/2205.01833

|                                   |                               |                                 |
<<<<<<< HEAD
|-----------------------------------|-------------------------------|---------------------------------|
=======
|---------------------------|--------------------------|-----------------------|
>>>>>>> fafde0efa311638f4b2cb750dea40208e22560f3
| **Infrastruktur**                 | **Kommerzielle Software**     | **Open Source Software**        |
| Literaturdatenbank                | SCOPUS                        | <https://openalex.org>          |
| Literaturverwaltung               | Citavi, Mendeley              | Zotero                          |
| Datenerhebung                     | Unipark, Millisecond Inquisit | PsychoPy                        |
| Datenanalyse                      | IBM SPSS, Stata               | GNU R, Python, Jupyter Notebook |
| Begutachtung und Veröffentlichung | Editorial Manager             | Open Journal System             |

### Slow Science

Viele der Entwicklungen lassen sich als „langsame Wissenschaft" (oder engl. Slow Science) zusammenfassen. Der traditionellen Produktion von Forschungsartikeln steht die achtsame Auseinandersetzung entgegen. Dass viele der vorgeschlagenen Lösungen im Wettbewerb um veröffentlichte Forschungsartikel nachteilig sind, kritisiert beispielsweise Hyman (2024 <https://www.researchgate.net/publication/377965519_Freeing_social_and_medical_scientists_from_the_replication_crisis> ). Er schlägt stattdessen vor, die Probleme mittels künstlicher Intelligenz zu lösen. Verlage wie Elsevier benutzen diese beispielsweise bereits für Peer Review ([https://www.elsevier.com/about/policies-and-standards/the-use-of-generative-ai-and-ai-assisted-technologies-in-the-review-process](#0){.uri}), wenngleich Modelle wie ChatGPT 4.0 nicht dafür geeignet sind https://arxiv.org/abs/2402.05519#:\~:text=Practical%20implications%3A%20Overall%2C%20ChatGPT%20does,steps%20to%20control%20its%20use. ).

### Big Team Science

-          CRediT und Übersetzungen <https://github.com/contributorshipcollaboration/credit-translation>

goodbye author hello contributor

### Weiterführende Informationen

-   @kepes2023assessing haben einen Anfänger-Leitfaden für die Einschätzung von Publikationsbias entwickelt.

-   @harrer2021doing haben ein kostenlos verfügbares Buch zur Durchführung von Meta-Analysen geschrieben: https://bookdown.org/MathiasHarrer/Doing_Meta_Analysis_in_R/

-   @Adler.2023 haben verschiedene Tools zur heuristischen Beurteilung von Vertrauenswürdigkeit wissenschaftlicher Befunde gesammelt. Auf https://mktg.shinyapps.io/Toolbox_App/ können Forschende Daten eingeben und die verschiedenen Verfahren durchführen.

-   @wilson2017good sammeln Empfehlungen, denen alle Forschenden für das Sicherstellen von Reproduzierbarkeit nachkommen sollten.

### Weiterführende Informationen

-   @kepes2023assessing haben einen Anfänger-Leitfaden für die Einschätzung von Publikationsbias entwickelt.

### Literatur
