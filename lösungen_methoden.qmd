---
title: "Methoden"
editor: visual
format: html
bibliography: references.bib
bibliographystyle: apa
---

In der Wissenschaft gibt es nicht *die eine Methode* [@Feyerabend.19752002], sondern Methoden werden für Probleme entwickelt, Probleme gelöst und Methoden weiterentwickelt oder fallen gelassen. Methoden sind also wie Werkzeuge, und nicht alles lässt sich mit einem Schraubendreher zusammenbauen. Mit Open Science Reformen kommen unzählige Methodische Neuerungen, Verbesserungen, und Vorschläge, die für Forschende häufig überwältigend sind: Forschungsprojekte voranbringen, Seminare und Vorlesungen halten, Drittmittel einwerben, und jetzt auch noch Open Science? Während viele Probleme auf eine unzureichende Methodenausbildung zurückgeführt werden [@lakens2021practical], die Forschende zu ihrer eigenen Last nachholen müssen, erleichtern einige Methoden die Arbeit. Für Promovierende wurde in der biologischen Psychologie beispielsweise der Wegweise ARIADNE entwickelt (https://igor-biodgps.github.io/ARIADNE/graph/graph.html). Dieses Kapitel bietet einen Überblick über methodische Entwicklungen und Diskussionen in den Sozialwissenschaften und Disziplinen, die vorrangig mit statistischen Methoden arbeiten.

#### Meta-Analysen

Unter Meta-Analysen werden Studien über Studien verstanden. Dabei extrahieren Forschende üblicherweise Ergebnisse aus bereits veröffentlichten Studien, schreiben andere Forschende aus einem Feld an und fragen nach unveröffentlichten Studien, und analysieren mit statistischen Methoden die Gemeinsamkeiten und Unterschiede zwischen den Ergebnissen. @fletcher2022replication argumentiert, dass nur mithilfe von Meta-Analysen die Allgemeingültigkeit von (statistischen) Phänomenen nachgewiesen werden kann. Im Idealfall könnten alle so weitermachen wie bisher und meta-analytische Modelle würden die Probleme korrigieren. Angesichts verheerender Publikationsbiases ist das allerdings aktuell nicht möglich. Wie Meta-Analysen dennoch informativ sein können, empfehlen @carlsson2024beginner allgemein und liste ich im Folgenden für spezifische Probleme auf.

##### Publikationsbias und P-Hacking einschätzen

Bei Meta-Analysen gilt: "Garbage in, garbage out". Wer viele schlecht durchgeführte Studien in einer Meta-Analyse zusammenfasst, erhält eine schlechte Zusammenfassung. Das hatte beispielsweise zur Folge, dass @Hagger.2010 ihrer Meta-Analyse einen deutlichen Effekt für ein Modell über die Willensstärke finden konnten, nachfolgende, groß angelegte Replikationsversuche und Analysen jedoch alle scheiterten, einen ebenso großen Effekt zu finden [@Hagger.2016b; @Friese.2018; @Dang.2020; @vohs2021multisite]. Was trotzdem möglich ist und auch in jeder Meta-Analyse getan werden sollte, ist eine Einschätzung der Datenqualität, beispielsweise der Stärke des Publikationsbiases. Dabei gibt es Methoden, die prüfen, ob es nicht veröffentlichte Studien gibt, und Methoden, die für die potenziell fehlenden Studien korrigieren. Teilweise funktionieren diese erst bei über 200 Studien, manche lassen sich jedoch auch schon bei einem Dutzend Studien anwenden.

###### Funnel Plot

Eine der ältesten Methoden ist der Funnel Plot (Trichter-Diagramm) [@light1984summing]. Dabei werden Präzision und Effektstärke der einzelnen Studien in einem Diagramm dargestellt. Im Idealfall sollten die Punkte das Bild eines Trichters bilden: Je präziser eine Studie ist (zum Beispiel durch eine große Stichprobe), desto eher sollte der dort gemessene Zusammenhang im Mittel liegen. Unpräzisere Studien verschätzen sich unsystematisch, also sie liegen mal drüber und mal drunter. Dadurch, dass nicht signifikante Ergebnisse selten veröffentlicht werden, bildet sich in dem Trichter-Diagramm fast nie ein Trichter, sondern nicht-signifikante Ergebnisse fehlen einfach.

In der folgenden Abbildung ist ein Trichter-Diagramm für eine Studie zum Zusammenhang zwischen Angst vor Mathematik und Leistung in Mathematik. Das Muster ist fast symmetrisch, es liegt also nur ein schwacher Publikationsbias vor. Die kleinen Zusammenhänge am unteren Rand sind etwas nach rechts verzerrt, und die präzisen Effekte ganz oben sind nicht alle gleich groß, sondern variieren stark.

```{r}
# Funnel plot example
library(psymetadata)
ds <- psymetadata::barroso2021

model <- metafor::rma.mv(yi = ds$yi, V = ds$vi, random = ~1 | study_id, data = ds)
metafor::funnel(model, xlab = "Beobachtete Effektstärke", ylab = "Präzision (Standardfehler)")

```

###### P-Curve

Beim P-Hacking werden Daten auf verschiedene Weisen ausgewertet und es wird diejenige berichtet, die einen niedrigen und damit signifikanten P-Wert zur Folge hat. Ergebnisse werden also nicht signifikant, weil die Hypothesen korrekt sind, sondern weil die Daten so lange ausgewertet wurden, bis sie signifikant wurden. Im Kapitel P-Hacking konnten wir sehen, wie P-Werte verteilt sind, je nachdem ob die Hypothese korrekt ist oder nicht. Diese Tatsache macht sich die P-Curve (@Simonsohn.2014; @Simonsohn.2014b; @Simonsohn.2015b) zu Nutzen. P-Werte eines Sets and Studien werden in einem Diagramm abgebildet und ihre Verteilung wird geprüft. Liegt kein P-Hacking vor sind die Werte entweder gleich verteilt (alle P-Werte kommen gleich häufig vor) oder sammeln sich bei 0 (kleinere P-Werte kommen häufiger vor). P-Hacking hat jedoch zur Folge, dass sich die Werte an der 5% Grenze tummeln, denn weiter als bis dort müssen die Daten nicht "gehackt" werden. Die Methode gewann vor allem deshalb an Bekanntheit, weil @Simmons.2017 die Methode auf eines der berühmtesten psychologischen Phänomene, dem Power Posing, angewandt haben und herausfanden, dass dort wahrscheinlich P-Hacking vorlag. Kritiker stellten später weitere Möglichkeiten vor, wie auch ohne P-Hacking eine suspekte P-Curve entstehen kann und das Verfahren wird inzwischen kaum mehr verwendet [@Erdfelder.2019].

###### Z-Curve

Statt P-Werte zu nehmen, können meta-analytische Befunde auch in sogenannte Z-Werte umgerechnet werden. Sie sind normalverteilt und mittels zusätzlicher Algorithmen lässt sich auf Basis von beobachteten Effekten schätzen, wie viele weitere Effekte es geben müsste. Dieses Verfahren namen Z-Curve [@Bartos.2022b] kann also für den File-Drawer Effekt und für P-Hacking korrigieren. Das Ergebnis daraus ist auch eine Schätzung, wie hoch die Replikationsrate wäre, wenn alle analysierten Studien erneut durchgeführt würden. Aktuellen Studien zufolge funktionieren diese Schätzungen ziemlich gut, obgleich sie eine große Menge an Daten benötigen [@Sotola.2022; @Sotola.2023; @roseler2023predicting].

###### Sensitivitätsanalyse

Ein Ansatz, welcher nicht nur bei Meta-Analysen sondern bei fast allen statistischen Auswertungen funktioniert, sind sogenannte Sensitivitäts- oder Robustheits-Analysen. Es werden dabei verschiedene Auswertungswege durchgegangen und dabei geprüft, wie stark sie sich auf die Ergebnisse auswerten. Bei Meta-Analysen können zum Beispiel viele mögliche Verfahren gleichzeitig gerechnet werden. Einen solchen Schrotschuss-Ansatz hat @Kepes.2017 geprägt, woraufhin er in anderen Studien übernommen wurde [@Korner.2022]. Einen Überblick über verschiedene Verfahren und unter welchen Bedingungen sie für Daten geeignet sind, bieten @Carter.2019.

##### Daumenregeln für Beurteilung einzelner Artikel

Eine Meta-Analyse ist aufwändig und kann mehrere Jahre dauern. Selbst Forschende, die keine Mittel für Studentische Hilfskräfte haben, die ihnen beim Codieren und Prüfen von hunderten bis tausenden Studien helfen, haben hier kaum eine Chance, eine ordentliche Analyse durchzuführen. Die folgenden Daumenregeln - und mehr als das sollen sie auch nicht sein - bieten Abkürzungen zur Beurteilung wissenschaftlicher Qualität.

###### Viele Signifikante Studien

Seit einer Vertrauenskrise in der Sozialpsychologie in den 1960er Jahren [@Lakens.2023] werden in Forschungsartikeln seitens vieler Zeitschriften mehrere Studien gefordert. Das hatte zur Folge, dass die Ressourcen statt in eine ordentliche Studie in mehrere kleinere Studien investiert wurden. Die meisten dieser "Multi-Study-Paper" haben dann ausschließlich signifikante Ergebnisse über bis zu 10 Studien hinweg. Während viele Studien mit durchweg signifikanten Ergebnissen auf den ersten Blick beeindruckend aussehen, lösen Sie beim näheren Hinsehen jedoch Skepsis aus: Einzelne Studien haben üblicherweise eine Wahrscheinlichkeit von 80-95%, dass dabei signifikante Ergebnisse bei der zentralen Analyse herauskommen. Diese Wahrscheinlichkeit (Statistische Teststärke oder Power) nimmt ab, wenn man mehrere Studien nacheinander durchführt. Es ist vergleichbar mit einem Schützen, der in 99% der Fälle mit einem Gewehr eine Glasflasche trifft. Die Wahrscheinlichkeit, dass er bei einem Schuss eine Glasflasche trifft ist also 99%. Die Wahrscheinlichkeit, dass er mit 50 Schüssen 50 Glasflaschen trifft ist weniger, nämlich 99%\^50 (hoch fünfzig) = 60,5%. Bei wissenschaftlichen Studien kommt es auf ähnliche Weise zu einer "Power-Deflation". Die Wahrscheinlichkeit, 4 signifikante Studien mit jeweils 80% Power durchzuführen, ist 40,96%. Dann eine genau solche Studie zu veröffentlichen ist extrem unwahrscheinlich [@Lakens.2017b].

### Prüfung auf Reproduzierbarkeit

Wird ein Befund mit denselben Daten und idealerweise demselben Programm bzw. Analysecode erneut getestet und geprüft, ob dieselben Zahlen dabei rauskommen - also nicht nur, ob die Hypothese erneut bestätigt wird - dann handelt es sich um eine Reproduktion der Ergebnisse. Im Gegensatz zu einer Replikation werden also keine neuen Daten erhoben. Dass Ergebnisse reproduzierbar sind, sollte das absolute Minimum für wissenschaftliche Berichte sein, ist es jedoch noch längst nicht.

::: callout-important
## Begriffs-Wirrwarr

Während der Begriff *Replikation* in den Wirtschaftswissenschaften sowohl die Prüfung einer vorliegenden Studie mit neuen Daten, als auch die erneute Prüfung mit denselben Daten meint, wird für letzteres in der Psychologie *Reproduktion* verwendet. In der biologischen Forschung über Fortpflanzung, der Reproduktionsforschung, wird außerdem auf Reproduzierbarkeit ausgewichen. In wieder anderen Fällen wie der @OpenScienceCollaboration.2015 wird bei Replikationen (neue Daten) von "Reproducibility" gesprochen und wiederholte Tests mit denselben Daten werden "komputationale Reproduzierbarkeit" (computational reproducibility) genannt. Zuletzt verschwimmen in manchen Bereichen die Grenzen, wenn zum Beispiel bei einer Replikation der Befunde der Pisa Studie teilweise dieselben Daten und teilweise neue verwendet werden oder wenn die Daten computergeneriert sind und dasselbe Programm fähig ist mittels Pseudozufallszahlengenerator andere Daten zu generieren, die aber dieselbe Struktur haben.
:::

Seit wenigen Jahren führt die Zeitschrift Meta-Psychology als eine der ersten für alle veröffentlichten Artikel Reproduzierbarkeits-Prüfungen durch. Diese werden durch Forschende freiwillig oder im Rahmen ihrer Tätigkeit bei der Zeitschrift durchgeführt. Während diese Praxis bereits für andere Zeitschriften gefordert wurde [@lindsay2023plea], ist es jedoch noch immer die Ausnahme. Reproduktionschecks aller möglicher Disziplinen können bei Rescience veröffentlicht werden (<http://rescience.github.io>). Für das Jahr 2024 hat das Institute for Replications angekündigt, Studien aus der Zeitschrift Nature Human Behavior zu reproduzieren [@noauthor_2024-ta]. Nature Human Behavior ist eine der angesehensten Zeitschriften bei der Erforschung menschlichen Verhaltens, wobei angesehen nicht mit wissenschaftlicher Qualität gleichzusetzen ist. Sie wird vom Springer Verlag verwaltet und fordert mit Publikationskosten in Höhe von circa 9000€ pro Artikel die höchste Gebühr. Die strategische Entscheidung, sich auf die dortigen Artikel zu konzentrieren hat den Vorteil, dass Personen, die die Reproduzierbarkeits-Checks durchführen, diese eventuell dort veröffentlichen können und dass Reproduktionen große Aufmerksamkeit erfahren. Angesichts des Qualitätsanspruches solcher Zeitschriften an ihre Qualität und der Tatsache, dass kostenlose Zeitschriften wie Meta-Psychology die Prozedur ohne externe Hilfe durch das Institute for Replication durchführen können, bildet sich hier wieder das bekannte Bild ab, bei dem Verlage ihr Prestige dafür missbrauchen, kostenlose und profit-generierende Arbeit aus der Wissenschaft zu ziehen. Am Ende ist es wieder nicht die Zeitschrift selbst, die zur wissenschaftlichen Qualitätssicherung beiträgt, sondern das Institute for Replication.

Eine Abkürzung bei der Prüfung von Korrektheit, welche bei vielen Zeitschriften verwendet wird, ist das Programm [statcheck](statcheck.io). Es erkennt automatisch klassische statistische Tests und prüft auf Basis der berichteten Werte, ob diese konsistent sind. @hartgerink2016688 hat Ergebnisse aus über 50.000 Artikeln mit dem Programm geprüft und die Artikel mittels Pubpeer kommentieren lassen. Weil der Algorithmus in seltenen Fällen - wie in den Kommentaren offen dargelegt - fälschlicherweise Werte als fehlerhaft markiert und die Autor\*innen der Artikel zuvor nicht vor den Kommentaren gewarnt wurden, hat die [DGPs das Vorgehen verurteilt](https://www.dgps.de/schwerpunkte/stellungnahmen-und-empfehlungen/stellungnahmen/details/stellungnahme-des-dgps-vorstands-zur-praxis-der-automatischen-plausibilitaetsueberpruefung-wissenschaftlicher-arbeiten-mit-statcheck/). Die Antworten der Statcheck-Gruppe und von Christ Hartgerink sind nicht mehr verfügbar.

::: callout-note
## Reproduktion auf Knopfdruck

Mit sogenannten *Push-Button-Replications* ist gemeint, dass Ergebnisse ohne großen Aufwand und von allen Forschenden nachgerechnet werden können - auf Knopfdruck eben. Während sozialwissenschaftliche Zeitschriften mehr und mehr fordern, Daten und Analysecode so zu veröffentlichen, dass die Ergebnisse nachgerechnet werden, verkörpert die Zeitschrift Image Processing Online (IPOL, https://www.ipol.im) das Ideal dieses Vorgehen: Zu jedem dort veröffentlichten Artikel ist eine Demo verfügbar, bei der nach Auswahl eines Bildes, der in dem Artikel veröffentlichte Algorithmus live durchgeführt wird.
:::

#### Großangelegte Reproduktionsprojekte

In verschiedenen Forschungsdisziplinen gibt es großangelegte Projekte, Reproduzierbarkeit für vollständige Disziplinen zu schätzen. Ein Pionier auf dem Gebiet war das ReplicationWiki von Höffler (https://replication.uni-goettingen.de/). Nachfolgende Projekte wie das Replication Network (replicationnetwork3.wordpress.com) stützten sich weitesgehend auf die dort zusammengefassten Daten. Für die Wirtschaftswissenschaften berichteten @brodeur2024mass eine Reproduzierbarkeitsrate von 70% und in den Management Sciences bei 55% [@fivsar2024reproducibility]. Mit dem Insitute for Replication (I4R) überschneidet sich außerdem die Social Science Reproduction Platform des Berkley Initiative for Transparency in the Social Sciences (BITSS); https://www.bitss.org/resources/social-science-reproduction-platform/). Während das I4R voraussichtlich 2024 eine Datenbank mit allen Ergebnissen veröffentlicht, ist die Plattform der BITSS bereits verfügbar.

#### Open Code

Öffentlich verfügbare Daten und Code sind notwendig für Reproduktions- und Robustheits-Checks. Zeitschriften stehen hier zwischen der Entscheidung, Einreichungen schwieriger und sich selbst damit weniger attraktiv zu machen, indem sie höhere Anforderungen stellen, und die wissenschaftliche Qualitätssicherung zu fördern. Ein ähnliches Problem herrscht auch bei Betreibern von Panels, in denen regelmäßig große Befragungen oder Leistungstest, wie zum Beispiel die PISA Studie oder das Sozio-Ökonomische-Panel (SOEP). Bei Analysen der SOEP-Daten wird der Code nur in 20% der Fälle geteilt (<https://www.wifa.uni-leipzig.de/fileadmin/Fakultät_Wifa/Institut_für_Theoretische_Volkswirtschaftslehre/Professur_Makroökonomik/Economics_Research_Seminar/ERS-Paper_Marcus.pdf>)

### Robustness checks

-          Multiverse analysis <https://www.researchgate.net/publication/367204012_EEGManyPipelines_A_large-scale_grass-root_multi-analyst_study_of_EEG_analysis_practices_in_the_wild>

#### Markdown Documents

-   

-   Computational Reproducibility Pilot Project: <https://psyarxiv.com/k8d4u/> \| HIER AUCH DISKUTIERT ALS SERVICE (könnte ULB machen)

-   

interactive multiverse / multiverse analysis

Statistik

-          Bayesianische Statistik

-          Verbannung von NHST bei Basic and Applied Psychology, bisher mit negativen Auswirkungen, die die Probleme eventuell noch verschärfen (<https://www.tandfonline.com/doi/full/10.1080/00031305.2018.1537892#:~:text=Part%20of%20the%20BASP%20editors,their%20expository%20text%20with%20a>)

\#### Transparency Checklist

### Open Data

-          Abbruch einer Promotion, Wechsel der Institution, usw. können dazu führen, dass wertvolle Zeit & Arbeit verloren gehen; im Extremfall wurden Tiere aufgezogen, operiert, und Forschung kann nicht weitergeführt werden wegen schlechter Dokumentierung

-          Vor Durchführung der Studie sollte klar sein, wie die Daten aussehen werden (Variablen, Bedeutung, Umfang, usw.)

-          <https://www.datalad.org>  

falsifizierte Daten können nur durch Sichtung identifiziert werden https://associationofanaesthetists-publications.onlinelibrary.wiley.com/doi/full/10.1111/anae.15263

-          Myths: <https://www.sortee.org/blog/2024/04/12/2024_open_data_myths/>

-          <https://leibniz-psychology.org/das-institut/drittmittelprojekte/datawiz-ii>

-          originalstudie hierzu,

-          Vanpaemel et al., 205,

-          und das: <https://www.researchgate.net/publication/370001752_Data_sharing_upon_request_and_statistical_consistency_errors_in_psychology_A_replication_of_Wicherts_Bakker_and_Molenaar_2011>

-          Repositorien für daten

o   Osf.io

o   Psych archives zpid

o   Re3

o   Universitätsspezifische (z.B. datastore der Uni Münster)

o   Fachspezifische (info auch hier: <https://forschungsdaten.info/themen/veroeffentlichen-und-archivieren/repositorien/>)

§  Economics & social sciences: data.gesis.org

§  Earth science and life science: Pangaea.de

§  Humanities, cultural studies: de.dariah.eu

§  Linguistics: [www.clarin.eu](http://www.clarin.eu)

§  Biology: gfbio.org

§  Materialwissenschaften: nomad-lab-eu

§  Psychologie: Researchbox.org

o   Coscine <https://about.coscine.de>

o   Zenodo (interdisziplinär)

o   Git

o   Open Source Lösungen statt proprietärer Software, <https://www.mkw.nrw/hochschule-und-forschung/digitalisierung-hochschule-und-wissenschaft/forschungsdatenmanagement-fdm> <https://www.nfdi.de>

o   <https://www.researchobject.org/ro-crate/>

o   <https://www.protocols.io>

o   <https://www.icpsr.umich.edu/web/pages/ICPSR/index.html>

o   Für Qualitative Daten: <https://qdr.syr.edu/deposit/data>

-          Kriterien für Daten teilen: FAIR

o   Pyramid of requirements <https://content.iospress.com/articles/information-services-and-use/isu805>

o   Halbautomatische Extraktion von Meta-Daten <https://www.nfdi4chem.de/de/lister-halbautomatische-metadatenextraktion-aus-kommentierter-experimentdokumentation-in-elabftw/>

o   Datenstruktur: <https://psych-ds.github.io>

o   Fair Aware Tool zum Testen des Wissens zu FAIR <https://fairaware.dans.knaw.nl>

o   Ohne Fair kostet es Europäische Wirtschaft jedes Jahr 10.2 Milliarden Euro <https://publications.europa.eu/resource/cellar/d375368c-1a0a-11e9-8d04-01aa75ed71a1.0001.01/DOC_1>

o   Feld-spezifische Templates für FAIRe daten: <https://www.cos.io/blog/cedar-embeddable-editor?utm_source=linkedin&utm_medium=social>

-          CARE

o   Collective benefit: Ergebnisse sind auch von Gesellschaft verwendbar, Bürgerbeteiligung

o   Authority to control:

o   Responsibility: verschiedene Sprachenu und Weltanschauungen werden berücksichtigt

o   Ethics: Wertschätzung

-          Zeitschriften

o   JOPD (psychologie)

o   Ing.grid (ingenieurswissenschaften) <https://www.inggrid.org>

o   Weitere wichtige?

-          Beispiele für Datenbanken

o   <https://db.mocoda2.de/c/home> Chatverläufe

o   OpAQ Ankereffekte

o   Cooperation database

o   Stanford meta analysis aggregator

o   [http://metabus.org#](http://metabus.org)

o   Listen

§  Aaron Charlton Mktgo.org

§  <https://replications.clearerthinking.org/replications/>

-          Teilen von qualitativen Daten, dort ist Anonymisierung relative schwierig: <https://doi.org/10.1177/25152459231205832>

-          Data parasites / data police / data terrorists

-          Nachfragen: <https://www.whatdotheyknow.com/request/trial_protocols_behavioural_insi_2>

-          Mehr Zitate? <https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0230416>

-          Workshop-Unterlagen: <https://j-5chneider.github.io/PTOS-open-data/>

-          Infosammlung zu Forschungsdaten: <https://github.com/UB-Mannheim/awesome-RDM>

-          Daten auch wertvoll als Gedächtnis der Gesellschaft, soetwas sollte archiviert werden, z.B. <https://liebesbriefarchiv.de>

o   Daten von Personen aus allen Feldern der Gesellschaft (crowdsourced) und öffentlich verfügbar

o   Andere Beispiele für Citizen Science: <https://www.buergerschaffenwissen.de> und <https://wissenschaft-im-dialog.de/mitmachen/>

-          Langzeitarchivierung vs. Archivierung

o   1000 Jahre <https://archiveprogram.github.com>

Daten automatisch anonymisieren lassen mit [AMNESIA](https://amnesia.openaire.eu/)

### Replicability

\#### Assuring Replicability in Primary Research

1.  Replikationsstudien in Sozialpsychologie schwierig wegen Anreizstruktur und Ressourcen https://doi.org/10.1027/1864-9335/a000548

2.  Andere Felder, Soziologie, Präregistrierung: <https://www.researchgate.net/publication/368454140_Preregistration_and_Registered_Reports_in_Sociology_Strengths_Weaknesses_and_Other_Considerations>

3.  

4.  Irise project (EU-gefördertes Projekt): <https://irise-project.eu/research-outputs>

5.  

6.  Messungenauigkeit: <https://osf.io/2me7t>

7.  

#### Preregistration / Präregistrierung

-          Erklärung

-          Gütekriterien: Struktur + Vollständigkeit (simmons nelson vs. Pham consumer psych)

-          Analyseplan wichtig, um wirklich P-Hacking vorzubeugen: <https://www.journals.uchicago.edu/doi/abs/10.1086/730455>

-          <https://www.researchgate.net/publication/375575020_Preregistration_in_practice_A_comparison_of_preregistered_and_non-preregistered_studies_in_psychology>

-          High replicability neues project <https://www.nature.com/articles/s41562-023-01749-9> \[hierzu Infobox, dass es umstritten ist, weil sie intransparent von der Präregistrierung abgewichen sind\]

-          Prävalenz: <https://datacolada.org/115>

-          Es sollte nicht nur über Bereitschaft von Forschenden gehen, sondern auch im Peer Review verankert werden, sauber zu präregistrieren: <https://journal.trialanderror.org/pub/reflections-on-preregistration/release/2>

-          Deviations:

o   Alle weichen ab: <https://osf.io/preprints/psyarxiv/nj4es> , <https://osf.io/f2z7y> slide 12/21

o   niemand prüft auf Abweichungen: <https://osf.io/preprints/psyarxiv/nh7qw>

o   Transparent changes: <https://osf.io/6fk87>

o   How to deviate: <https://osf.io/preprints/psyarxiv/ha29k>

-          Repositorien

o   [https://meta-meta-resources.org/running-studies/preparation/pre-reg-repos/#](https://meta-meta-resources.org/running-studies/preparation/pre-reg-repos/)!

o   <https://www.alltrials.net>

o   <https://clinicaltrials.gov>

o   <http://www.crd.york.ac.uk/PROSPERO/>

-          Auswirkung auf Ergebnisse

o   Wahrscheinlichkeit positiven Befunden statt \>90% dadurch nur 40-50%, Scheel, Schijen, & Lakens(2021) \[siehe auch <https://osf.io/f2z7y>, slide 17/21\]

o   <https://drive.google.com/file/d/1gcyBE78tb9zerl4M35uS3npVGvMp-MPZ/view> The effectiveness of preregistration in psychology: Assessing preregistration strictness and preregistration-study consistency olmo van den akker[\[LR2\]](#_msocom_2) 

\##### Pre-Registration templates what is a good prereg? (see Simmons et al) What to preregister? Analysis script

OSF arbeitet neue ein <https://www.cos.io/blog/call-for-submission-for-preregistration-templates>

-   

-   Diskrepanz zwischen Präregistrierung/Prereg und Publikation: <https://bmjopen.bmj.com/content/13/10/e076264.long>

-   

-   Kritik: <https://journals.sagepub.com/doi/10.1037/gpr0000135>

-   

-   Prereg als Mittel zur Transparenz in der Forschungsplanung auch für Exploration sinnvoll, bei Hypothesentesten v.a. Eliminierung von Freiheitsgraden und Erhöhung von Vertrauen

-   

#### Internal Replications

<https://perception.yale.edu/papers/17-Scholl-APSObserver.pdf>

#### Power Analysis

-          Power

-          Small-telescopes approach

-          Bayesianischer Ansatz

o   So lange erheben, bis Bayes-Faktor konvergiert; kann Stichprobeunumfang reduzieren, wurde so zB für Verhaltensforschung bei Tieren empfohlen (<https://www.nature.com/articles/s41684-023-01308-9>)

#### New statistics

-          Frequentistische Statistik wird benutzt, ist oft missverstanden und Inhalte sind mit anderen Perpsektiven vermischt (<https://journals.sagepub.com/doi/10.1177/0959354314546157?icid=int.sj-full-text.similar-articles.3>)

o   Gigerenzer P-Value unwissen

-          Bayesian, Kritik: <https://journals.sagepub.com/doi/10.1177/25152459231213371>

-          Effektstärken statt p-Werte

o   Equivalence testing @Lakens.2017; @Lakens.2018

o   Rölle von kleinen Effekten: <https://journals.sagepub.com/doi/full/10.1177/17456916221100420>

o   practical relevance of small effect sizes <https://www.researchgate.net/publication/352412241_Evaluating_the_practical_relevance_of_observed_effect_sizes_in_psychological_research>

o   effect sizes guide jane guide to effect sizes and confidence intervals

o    

-          Alternative lakens paper "alternative to p-value is correctly used p-value"

o   Philosophische Diskussion über Nutzen von P-Werten: <https://journals.sagepub.com/doi/full/10.1177/09593543231160112> und hier <https://journals.sagepub.com/doi/10.1177/0959354312465483?icid=int.sj-full-text.similar-articles.1>

#### 21 Word Solution

#### Open Materials

#### Transparency

-          Bewertungskriterien: <https://osf.io/preprints/psyarxiv/djmcq>

-          Transparency Checklist

### Qualitative Forschung und Open Science

-          Pseudonymisierung von Daten ist hier sehr besonders, denn...: [https://doi.org/10.1177/160940692110346](https://doi.org/10.1177/16094069211034641)

-          Präregistrierungs-Template: <https://docs.google.com/document/d/16vLoRAs6RmXKy0v1RCx6osSaYk4r64Hgl6S4_KtWbeM/edit#heading=h.fwbi14d4b65g>

-          Sharing sensitive qualitative data: <https://www.youtube.com/watch?v=-Pha3EINF3s>

o   Berichte von Opfern von Vergewaltigungen à müssen anonymisiert werden, extrem aufwändig, kann stellvertretene Traumata auslösen

### Open Source Software

-          Research Software Engineering (RSE) \[siehe auch oben bei reproducible code, aber hier sollte das meiste dazu stehen\]

-          Non proprietär à Gefahr vor Lock-in

-          GNU Lizenz

-          Open Source Bedeutung

-          Bibliotheken, die Wissen verwalten, positionieren sich neu, um nun Infrastruktur zu stellen (z.B. [https://www.cambridge.org/core/books/abs/data-science-in-the-library/toward-reproducibility-academic-libraries-and-open-science/6FB18D7BCD8B82E597ED0D4B390C72EE#](https://www.cambridge.org/core/books/abs/data-science-in-the-library/toward-reproducibility-academic-libraries-and-open-science/6FB18D7BCD8B82E597ED0D4B390C72EE))

-          Von Chan-Zuckerberg-Initiative Unternehmen: <https://chanzuckerberg.com/rfa/essential-open-source-software-for-science/>

-          Alternative zu Clarivate Analytics Web of Science Literaturdatenbank: <https://openalex.org> https://arxiv.org/abs/2205.01833

|                                   |                               |                                 |
|-----------------------------------|-------------------------------|---------------------------------|
| **Infrastruktur**                 | **Kommerzielle Software**     | **Open Source Software**        |
| Literaturdatenbank                | SCOPUS                        | <https://openalex.org>          |
| Literaturverwaltung               | Citavi, Mendeley              | Zotero                          |
| Datenerhebung                     | Unipark, Millisecond Inquisit | PsychoPy                        |
| Datenanalyse                      | IBM SPSS, Stata               | GNU R, Python, Jupyter Notebook |
| Begutachtung und Veröffentlichung | Editorial Manager             | Open Journal System             |

### Slow Science

Viele der Entwicklungen lassen sich als „langsame Wissenschaft" (oder engl. Slow Science) zusammenfassen. Der traditionellen Produktion von Forschungsartikeln steht die achtsame Auseinandersetzung entgegen. Dass viele der vorgeschlagenen Lösungen im Wettbewerb um veröffentlichte Forschungsartikel nachteilig sind, kritisiert beispielsweise Hyman (2024 <https://www.researchgate.net/publication/377965519_Freeing_social_and_medical_scientists_from_the_replication_crisis> ). Er schlägt stattdessen vor, die Probleme mittels künstlicher Intelligenz zu lösen. Verlage wie Elsevier benutzen diese beispielsweise bereits für Peer Review ([https://www.elsevier.com/about/policies-and-standards/the-use-of-generative-ai-and-ai-assisted-technologies-in-the-review-process](#0){.uri}), wenngleich Modelle wie ChatGPT 4.0 nicht dafür geeignet sind https://arxiv.org/abs/2402.05519#:\~:text=Practical%20implications%3A%20Overall%2C%20ChatGPT%20does,steps%20to%20control%20its%20use. ).

### Big Team Science

-          CRediT und Übersetzungen <https://github.com/contributorshipcollaboration/credit-translation>

 [\[LR1\]](#_msoanchor_1)<https://journals.sagepub.com/doi/10.1177/20531680241233439>

 [\[LR2\]](#_msoanchor_2)viele paper von denen dazu

 [\[LR3\]](#_msoanchor_3)https://arxiv.org/abs/2205.01833

 [\[LR4\]](#_msoanchor_4)<https://www.researchgate.net/publication/377965519_Freeing_social_and_medical_scientists_from_the_replication_crisis>

 [\[LR5\]](#_msoanchor_5)https://arxiv.org/abs/2402.05519#:\~:text=Practical%20implications%3A%20Overall%2C%20ChatGPT%20does,steps%20to%20control%20its%20use.

### Weiterführende Informationen

-   @kepes2023assessing haben einen Anfänger-Leitfaden für die Einschätzung von Publikationsbias entwickelt.

-   @harrer2021doing haben ein kostenlos verfügbares Buch zur Durchführung von Meta-Analysen geschrieben: https://bookdown.org/MathiasHarrer/Doing_Meta_Analysis_in_R/

-   @Adler.2023 haben verschiedene Tools zur heuristischen Beurteilung von Vertrauenswürdigkeit wissenschaftlicher Befunde gesammelt. Auf https://mktg.shinyapps.io/Toolbox_App/ können Forschende Daten eingeben und die verschiedenen Verfahren durchführen.

-   @wilson2017good sammeln Empfehlungen, denen alle Forschenden für das Sicherstellen von Reproduzierbarkeit nachkommen sollten.

### Literatur
