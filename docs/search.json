[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Willkommen",
    "section": "",
    "text": "Literatur",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Willkommen</span>"
    ]
  },
  {
    "objectID": "index.html#literatur",
    "href": "index.html#literatur",
    "title": "Willkommen",
    "section": "",
    "text": "Mede, Niels G., Mike S. Schäfer, Ricarda Ziegler, und Markus Weißkopf. 2020. „The replication crisis in the public eye: Germans’ awareness and perceptions of the (ir)reproducibility of scientific research“. Public understanding of science (Bristol, England), 963662520954370. https://doi.org/10.1177/0963662520954370.\n\n\nNelson, Leif D., Joseph Simmons, und Uri Simonsohn. 2018. „Psychology’s Renaissance“. Annual review of psychology 69: 511–34. https://doi.org/10.1146/annurev-psych-122216-011836.\n\n\nNosek, Brian A, Charles R Ebersole, Alexander C DeHaven, und David T Mellor. 2018. „The preregistration revolution“. Proceedings of the National Academy of Sciences 115 (11): 2600–2606.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Willkommen</span>"
    ]
  },
  {
    "objectID": "about.html#footnotes",
    "href": "about.html#footnotes",
    "title": "Über das Buch",
    "section": "",
    "text": "Das bedeutet, dass meine (Groß-)Eltern keine Doktortitel haben.↩︎",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Über das Buch</span>"
    ]
  },
  {
    "objectID": "wasistopenscience.html",
    "href": "wasistopenscience.html",
    "title": "Was ist Open Science?",
    "section": "",
    "text": "Literatur",
    "crumbs": [
      "Einleitung",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Was ist Open Science?</span>"
    ]
  },
  {
    "objectID": "wasistopenscience.html#literatur",
    "href": "wasistopenscience.html#literatur",
    "title": "Was ist Open Science?",
    "section": "",
    "text": "Cole, Nicki Lisa, Eva Kormann, Thomas Klebel, Simon Apartis, und Tony Ross-Hellauer. 2024. „The societal impact of Open Science: a scoping review“. Royal Society Open Science 11 (6): 240286.\n\n\nFecher, Benedikt, und Sascha Friesike. 2014. Open science: one term, five schools of thought. Springer International Publishing.\n\n\nGopalakrishna, Gowri, Jelte M. Wicherts, Gerko Vink, Ineke Stoop, Olmo van den Akker, Gerben ter Riet, und Lex Bouter. 2021. „Prevalence of responsible research practices and their potential explanatory factors: a survey among academic researchers in The Netherlands“. https://doi.org/10.31222/osf.io/xsn94.\n\n\nOpen Science Collaboration. 2015. „Psychology: Estimating the reproducibility of psychological science“. Science (New York, N.Y.) 349 (6251): aac4716. https://doi.org/10.1126/science.aac4716.\n\n\nSatinsky, Emily N., Tomoki Kimura, Mathew V. Kiang, Rediet Abebe, Scott Cunningham, Hedwig Lee, Xiaofei Lin, u. a. 2021. „Systematic review and meta-analysis of depression, anxiety, and suicidal ideation among Ph.D. students“. Scientific reports 11 (1): 14370. https://doi.org/10.1038/s41598-021-93687-7.",
    "crumbs": [
      "Einleitung",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Was ist Open Science?</span>"
    ]
  },
  {
    "objectID": "umgehenmitopenscience.html",
    "href": "umgehenmitopenscience.html",
    "title": "Wie ist mit Open Science umzugehen?",
    "section": "",
    "text": "Weiterführende Informationen",
    "crumbs": [
      "Einleitung",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Wie ist mit Open Science umzugehen?</span>"
    ]
  },
  {
    "objectID": "umgehenmitopenscience.html#footnotes",
    "href": "umgehenmitopenscience.html#footnotes",
    "title": "Wie ist mit Open Science umzugehen?",
    "section": "",
    "text": "Damit ist die Tatsache gemeint, dass ein Großteil der veröffentlichten Replikationsstudien (also Studien, die Bekanntes erneut prüfen) zu anderen Ergebnissen kommt, sich Original-Studien also nicht replizieren lassen (siehe auch die folgenden Kapitel zur Geschichte und Inventur).↩︎",
    "crumbs": [
      "Einleitung",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Wie ist mit Open Science umzugehen?</span>"
    ]
  },
  {
    "objectID": "anfänge.html",
    "href": "anfänge.html",
    "title": "Anfänge einer Revolution",
    "section": "",
    "text": "Stapel-Affäre\nDurch einen Zufall entdeckten Nachwuchswissenschaftler im Jahr 2011, dass die Daten einer Studie ihres Kollegen, Diederik Stapel, von niemandem jemals erhoben wurden. Sie waren ausgedacht bzw. fabriziert. Stand Juli 2024 wurden 58 von Stapels Fachartikel identifiziert und zurückgezogen, deren Daten fabriziert oder geschönt wurden.1 Wissenschaftliche Institutionen wie der Begutachtungsprozess von Artikeln durch Fachkolleg*inen, deren Zweck die Qualitätssicherung war, hatten versagt. Seit dem Vorfall sind einige weitere Fälle bekannt geworden, teilweise durch erneute Analyse von Daten der jeweiligen Studien (O’Grady 2021) und oft durch Whistleblower, also durch wissende Personen, die zu ihrem Schutz anonym bleiben wollen. Umfragen in den Niederlanden unter Forschenden haben ergeben, dass Fälschung oder Schönigung von Daten von bis zu 10% aller Personen durchgeführt wird (Gopalakrishna u. a. 2021). Dabei ist zu beachten, dass Studien durch gefälschte Daten besonders innovativ, überraschend, oder klar werden - Eigenschaften, die die Veröffentlichung in einer Fachzeitschrift wahrscheinlicher machen.",
    "crumbs": [
      "Die Geschichte der Open Science-Bewegung",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Anfänge einer Revolution</span>"
    ]
  },
  {
    "objectID": "anfänge.html#footnotes",
    "href": "anfänge.html#footnotes",
    "title": "Anfänge einer Revolution",
    "section": "",
    "text": "Mittels der Retraction Database lassen sich nach Thema, Autor*in, Zeitschrift, usw. zurückgezogene Artikel durchsuchen: http://retractiondatabase.org/↩︎",
    "crumbs": [
      "Die Geschichte der Open Science-Bewegung",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Anfänge einer Revolution</span>"
    ]
  },
  {
    "objectID": "bestandsaufnahme.html",
    "href": "bestandsaufnahme.html",
    "title": "Bestandsaufnahme",
    "section": "",
    "text": "Eine ähnliche Vertrauenskrise gab es in der Sozialpsychologie in den 1960er-Jahren (Lakens 2023). Ein entscheidender Unterschied zwischen der alten Krise und der aktuellen ist Bestandsaufnahme: Parallel zu den eigenartigen Befunden zur Zukunftsvorhersage von Bem vernetzten sich Psycholog*innen um Brian Nosek international und untersuchten die Replizierbarkeit von 100 Studien aus namhaften psychologischen Fachzeitschriften (Open Science Collaboration 2015). Sie fanden heraus, dass sich nur 39 der 100 Originalbefunde replizieren ließen. Bei allen anderen Studien waren die Replikationsergebnisse anders als die ursprünglichen Ergebnisse. Viele weitere Großprojekte folgten, alle mit ähnlichen Ergebnissen: Die Replikationsraten lagen weit unter den gewünschten.\n\n\n\n\n\n\nKritische Betrachtung der Open Science Collaboration, 2015\n\n\n\nObgleich dieses „Reproducibility Project Psychology” die gesamte Fachgemeinschaft zutiefst erschütterte und den Weg für einen Paradigmenwechsel ebnete, bemängeln manche Forschende auch negative Auswirkungen auf nachfolgende Replikationsforschung. Indem 100 Studien gleichzeitig von einer Gruppe aus über 100 Forschenden veröffentlicht wurden, setzte das Projekt unrealistische Maßstäbe für Replikationsforschung. Gleichzeitig war die Qualitätskontrolle dabei weniger streng, da die einzelnen Studien nicht alle in dem Maße begutachtet werden konnten, wie es bei einer traditionellen Veröffentlichung der Fall gewesen wäre (z.B. Röseler u. a. (2022)). Einige gleichermaßen ambitionierte Vorhaben wurden veröffentlicht, wie zum Beispiel die ManyLabs Studien (z.B. Klein u. a. (2014); Klein u. a. (2018)), oder Versuche, bei denen unabhängige Gruppen dieselben Hypothesen testeten und replizierten (Landy u. a. 2020). Oft beschränken sich diese Vorhaben auf Studien, die sich im Rahmen einer Online-Befragung replizieren lassen. Formate wie Längsschnittstudien oder Verhaltensbeobachtungen sind dabei aufgrund der schwierigeren Machbrakeit unterrepräsentiert.\n\n\nZahlreiche Verbünde folgten. Einige Projekte konzentrierten sich auf einzelne Phänomene. Beispielsweise haben sich 17 Forschungsgruppen zusammengetan, um den Befund des Facial Feedback (Strack, Martin, und Stepper 1988) zu replizieren (Wagenmakers u. a. 2016). Bei diesem Experiment geht darum, dass Personen einen Stift mit den Zähnen festhalten und dabei je nach Ausrichtung des Stiftes entweder diejenigen Muskeln anspannen, die sie auch zum Lachen benötigen oder eben nicht. In der “Lachen”-Bedingung fanden die Versuchspersonen im Anschluss Comics witziger. Die Replikation schlug fehl. 2022 wurde eine weitere Studie mit über 3000 Versuchspersonen aus 19 Ländern veröffentlicht - diesmal auch mit direkter Beteiligung von Fritz Strack, der die Originalstudie durchgeführt hatte (Coles u. a. 2022). Wieder zeigte sich, dass die Position eines Stiftes im Mund sich nicht auf die Bewertung von Stimuli auswirkt. Jenseits von sozialpsychologischen Befunden konzentrierten sich Forschende auch auf Bereiche wie Forschung mit Babys (Byers-Heinlein u. a. 2020) oder auf Studien bestimmter Zeitschriften (Camerer u. a. 2018).\nListe großer Replikationsprojekte (siehe auch FORRT Replication Hub)\n\n\n\nProjekt\nThema\nLink\n\n\nReproducibility Project: Psychology\nPsychologie\nhttps://osf.io/ezcuj/\n\n\nCORE\nEntscheidungsforschung\nhttps://osf.io/5z4a8/\n\n\nData Replicada\nKonsumentenverhalten und Entscheidungsforschung\nhttps://datacolada.org/archives/category/replication\n\n\nMany Labs 1\nPsychologie\nhttps://osf.io/wx7ck/\n\n\nMany Labs 2\nPsychologie\nhttps://osf.io/8cd4r/\n\n\nMany Labs 3\nPsychologie\nhttps://osf.io/ct89g/\n\n\nMany Labs 4\nPsychologie\nhttps://osf.io/8ccnw/\n\n\nMany Labs 5\nPsychologie\nhttps://osf.io/7a6rd/\n\n\nSoto\nPersönlichkeitspsychologie\nhttps://doi.org/10.1177/0956797619831612\n\n\nSocial Sciences Replication Project\nVerhaltensforschung\nhttp://www.socialsciencesreplicationproject.com\n\n\nRegistered Replication Reports\nVerschiedene\n–\n\n\nMany Babies 1\nEntwicklungspsychologie\nhttps://manybabies.org\n\n\nSports Sciences Replications\nSportwissenschaften\nhttps://ssreplicationcentre.com\n\n\nHagen Cumulative Science Project\nPsychologie\nhttps://osf.io/d7za8/\n\n\nI4R Replications\nPolitikwissenschaften\nhttps://i4replication.org/reports.html\n\n\nExperimental Philosophy\nExperimentelle Philosophie\nhttps://doi.org/10.1007/s13164-018-0400-9\n\n\nReproducibility Project: Cancer\nKrebsforschung (Medizin)\nhttps://www.cos.io/rpcb\n\n\nSCORE\nSozialwissenschaften\nhttps://www.cos.io/score\n\n\nREPEAT\nGesundheitssystem\nhttps://www.repeatinitiative.org\n\n\nCREP\nPsychologie\nhttps://www.crep-psych.org\n\n\nBoyce et al., 2023\nPsychologie\nhttps://doi.org/10.1098/rsos.231240\n\n\nReproSci\nBiologie\nhttps://reprosci.epfl.ch\n\n\nBoyce et al. 2024\nPsychologie\nhttps://doi.org/10.31234/osf.io/an3yb\n\n\n\n\nDefinition von Replizierbarkeit\nZu sagen, was repliziert werden konnte und was nicht, ist erst nach einer Definition von Replikationserfolg möglich. Im Sprachgebrauch von Forschenden wird mit „wurde repliziert” gemeint, dass ein eine Studie, die dieselbe Forschungsfrage untersucht hat, zu gleichen Ergebnissen wie eine Originalstudie gekommen ist. Zu Replikationsfehlschlägen wird „konnte nicht repliziert werden” gesagt. Subtil davon abweichend kann „wurde nicht repliziert” meinen, dass keine Replikationsversuche existieren oder sie gar nicht erst durchgeführt werden konnten, weil die Originalstudie zentrale Punkte nicht ausreichend beschrieben haben (Errington u. a. 2021). Für eine Wissenschaft, die über 100 Jahre alt ist, scheint es überraschend, dass noch immer keine klare Definition wichtiger Konzepte rund um das Thema Replikation vorliegt, geschweige denn es zur Routine gehört, Studien zu replizieren. Während sich in verschiedenen Feldern abweichende Taxonomien, also Modelle zur Einordnung verschiedener Arten von Replikationen, durchgesetzt haben, werden Begriffe rund um Replikation in diesem Buch wie in der Tabelle beschrieben verwendet. Je nachdem, ob dieselben oder andere Daten und dieselben oder andere Analysen verwendet werden ergibt sich, ob von Reproduzierbarkeit, Replizierbarkeit, Robustheit, oder Verallgemeinerbarkeit die Rede ist.\n\nReplikations-Taxonomie nach Turing Way (The Turing Way Community und Scriberia 2024)\n\n\n\n\nDaten\n\n\n\n\n\n\n\ngleich\nunterschiedlich\n\n\nAnalyse\ngleich\nreproduzierbar\nreplizierbar\n\n\n\nunterschiedlich\nrobust\nverallgemeinerbar\n\n\n\n\n\n\n\n\n\nStatistischer Vergleich von Original- und Replikationsbefunden\n\n\n\nJede Messung hat eine Unschärfe. In Sozialwissenschaften sind Messungen von Eigenschaften oder Phänomenen wie Entscheidungsheuristiken extrem schwierig. Werden dann Original-Ergebnisse mit Replikations-Ergebnissen verglichen, haftet beiden Ergebnissen eine Unschärfe an. Das macht es schwierig zu sagen, ob Unterschiede aufgrund von Zufallsschwankungen oder aufgrund von Problemen der Originalstudie entstanden sind. Statistische Methoden, um die beiden Ergebnisse zu vergleichen gibt es zahlreiche. Sie unterscheiden sich typischerweise darin, wie sehr die Unschärfe der Originalstudie berücksichtigt wird. Aufgrund älterer Methodenstandards sind Original-Ergebnisse meistens extrem unscharf und je stärker sie gewichtet werden, desto positiver fällt der Vergleich aus bzw. desto höher ist die geschätzte Replikationsrate. Je nach Methode können Replikationsraten dann zwischen 40% und 80% liegen. Ein Vergleich von Kriterien mit der FORRT Replikationendatenbank ist online verfügbar.\n\n\n\n\nWeiterführende Literatur\nFür eine systematischere, in den Informationswissenschaften verankerte Taxonomie zur Art der Replikation siehe Plesser (2018). Eine an den statistischen Methoden angelehnte Taxonomie für die Ergebnisse von Replikationsstudien haben LeBel et al. (LeBel u. a. 2019) vorgeschlagen. Philosophisch diskutiert wird Replikationsnähe zum Beispiel von Choi (2023) und Leonelli (2023).\n\nReplikationstaxonomie\n\n\n\n\n\n\nUnterscheidungskriterium\nAusprägungen\n\n\n\n\nErgebnisse einer Replikationsstudie\nErfolgreich\nFehlgeschlagen\nUnklar oder gemischt\n\n\nNähe einer Replikationsstudie zur Originalstudie (in Anlehnung an Lebel REF und Hüffmeier et al REF)\nDirekte Replikation\n(selbe Versuchsleiter*innen,\nselbe Versuchsmaterialien,\nneue Versuchspersonen)\nNahe Replikation\n(andere Versuchsleiter*innen,\nmöglichst ähnliche Versuchsmaterialien,\nneue Versuchspersonen)\nKonzeptuelle oder konstruktive Replikation\n(andere Versuchsleiter*innen,\nandere Versuchsmaterialien,\nneue Versuchspersonen)\n\n\nZiel der Replikation\nReproduktion\nMit selben Daten und selbem Programmiercode zu denselben Ergebnissen gelangen\nReplikation\nMit anderen Daten zu denselben Ergebnissen gelangen\n\n\n\n\n\n„Eine Schwalbe macht noch keinen Sommer”\nOb an einem wissenschaftlichen Befund „etwas dran ist”, er also einen Wahrheitsanspruch hat, hängt – neben seiner eigentlichen Art der Etablierung – bei der Replikationsforschung von vielen Faktoren ab. Was waren die Ergebnisse der Replikationsstudie? Wie viele und wie unterschiedliche Studien wurden durchgeführt? Wie sahen die genauen Methoden aus? Was waren die Unterschiede zwischen Replikationen und Originalstudie? Während Einzelstudien immer einen Erkenntnisgewinn liefern (mindestens, ob eine bestimmte Methode praktikabel ist, (Sikorski und Andreoletti 2023), können sie je nach Forschungsgebiet stark variieren Landy u. a. (2020). Für das Gesamtbild braucht es mehr, wie zum Beispiel eine statistische Zusammenfassung vieler einzelner Studien zu einer Gesamtstudie (Meta-Analyse). Ein Beispiel mit Fantasiedaten befindet sich dazu in der folgenden Abbildung.\nBetrachtet man viele Studien, die den Zusammenhang zweier Dinge, wie zum Beispiel Einkommen und Bildungsabschluss, untersucht haben, so werden sich die Studien hinsichtlich der Details unterscheiden: Was wurde alles zum Einkommen gezählt (Netto, Brutto, Sozialleistungen, Einkommen von Familienangehörigen, Werte über einen Zeitraum oder von einem bestimmten Zeitpunkt aus der Vergangenheit, usw.) oder auch welche Personen befragt wurden (Studierende, Berufstätige, wurden Befragte bezahlt, usw.). All diese Unterschiede wirken sich möglicherweise auf den Zusammenhang aus, und selbst wenn sie es nicht tun, unterliegen Zusammenhänge oft Schwankungen, die sich durch die Messmethoden ergeben. In dem Beispiel lässt sich die Stärke des Zusammenhangs auf eine Zahl und eine dazugehörige Präzision herunterbrechen. Die Zahl heißt hier “Korrelation” und die Schärfe “Fehlerbalken”. In dem Wald-Diagramm (Forest Plot / Blobbogram) sind mögliche Korrelationen aus verschiedenen Studien abgebildet. Im Rahmen einer Meta-Analyse können Studienergebnisse anschließend kombiniert und Unterschiede untersucht werden.\n\n\nCode\nlibrary(ggplot2)\nlibrary(metafor)\n\n\nLade nötiges Paket: Matrix\n\n\nLade nötiges Paket: metadat\n\n\nLade nötiges Paket: numDeriv\n\n\n\nLoading the 'metafor' package (version 4.6-0). For an\nintroduction to the package please type: help(metafor)\n\n\nCode\nset.seed(10)\nk &lt;- 15\ncors &lt;- data.frame(\"Stichprobenumfang\" = round(rchisq(n = k, df = 2, ncp = 0)*5+10, digits = 0)\n                   , \"Korrelation\" = rnorm(n = k, mean = .05, sd = .3)\n                   , \"Studie\" = paste(\"Studie\", toupper(letters[1:k]), sep = \" \")\n                   , \"yi\" = NA\n                   , \"vi\" = NA\n                   )\n\ncors[, 4:5] &lt;- metafor::escalc(ni = cors$Stichprobenumfang, ri = cors$Korrelation, measure = \"COR\")\ncors$ucb &lt;- cors$yi + qnorm(.975)*cors$vi\ncors$lcb &lt;- cors$yi - qnorm(.975)*cors$vi\n\n\n\nggplot(cors, aes(x = Korrelation, y = reorder(Studie, Korrelation))) + \n  geom_point() + geom_errorbar(xmin = cors$lcb, xmax = cors$ucb) + \n  geom_vline(xintercept = 0, lty = 2) + theme_classic() + ylab(\"\") + \n  xlim(c(-.4, .6))\n\n\n\n\n\n\n\n\n\n\n\nPhänomen-zentrierte Replikationsprojekte\nIm Gegensatz zu dem breit gefächerten Reproducibility Project Psychology (Open Science Collaboration 2015) und anderen Versuchen, die Replikationsrate für ganze Felder zu schätzen (Camerer u. a. 2016; Feldman 2021; Brodeur, Mikola, und Cook 2024), haben sich andere Versuche auf grundlegende Phänomene fokussiert. Dutzende Gruppen auf der ganzen Welt haben sich in solchen Fällen zusammengeschlossen, auf einen Versuchsaufbau geeinigt, und führen die Studien mit einer enormen Anzahl an Versuchspersonen durch. Die meisten dieser Vorhaben stammen aus der Psychologie. Während die dabei gefundenen Effektstärken, also sozusagen die Deutlichkeit eines Zusammenhanges oder Befundes, in fast allen Fällen weit unter denen bisheriger Studien lagen (Kvarven, Strømland, und Johannesson 2020), waren sie zudem beim Großteil der Studien null, die Phänomene waren also „nicht sichtbar” (Alogna u. a. 2014; Eerland u. a. 2016; Bouwmeester u. a. 2017; O’Donnell u. a. 2018; Wagenmakers u. a. 2016.; Cheung u. a. 2016; Vaidis u. a. 2024; Rife u. a. 2024). So konnte beispielsweise mit einer enormen Präzision gezeigt werden, dass eine Geschichte über einen Professor Versuchspersonen in einem anschließenden Leistungstest nicht schlauer macht (O’Donnell u. a. 2018).\n\n\n\n\n\n\nEffiziente Nutzung von Ressourcen?\n\n\n\nWie geht man mit Ressourcen bei Replikationen um? Bei Zusammenschlüssen vieler Forschenden stellt sich diese Frage unweigerlich. Erstellen alle Gruppen unabhängig voneinander die Studie? Halten sich alle an ein zuvor abgestimmtes Protokoll? Führen sie die Studie nacheinander durch, um voneinander zu lernen? Bei Registered Replication Reports wird für gewöhnlich zuvor mit anderen Forschenden (z.B. den Autor*innen der Originalstudie) ein Versuchsaufbau abgestimmt. In anderen Fällen wird gemeinsam ein Versuchsaufbau erarbeitet, der zum Testen der Theorie ideal sein sollte (Creative Destruction Approach, Tierney u. a. 2020). Teams in verschiedenen Ländern übersetzen das Protokoll dann und halten sich bei der Durchführung eng daran. Diese Protokolle sind manchmal nicht im Vorhinein getestet (Buttliere 2024), basieren oft aber auf erfolgreichen, namhaften Studien. Das hat den Vorteil, dass Unterschiede zwischen den Gruppen nicht auf Unterschiede in der Durchführung zurückzuführen sind und sich Kulturen vergleichen lassen (Kakinohana, Pilati, und Klein 2022). Ein Nachteil dabei ist jedoch, dass, wenn an einem, zwei, oder fünf Standorten das Experiment schon nicht funktioniert, es fraglich ist, ob die übrigen 30 Gruppen es auch probieren sollten. In den Worten von Buttliere (2024): “Wer bekommt bessere Ergebnisse? 39 Personen, die etwas zum ersten Mal tun, oder eine Person, die etwas 39 Mal tut?”\n\n\n\n\nDisziplin-zentrierte Replikationsprojekte\nUngefähr die Hälfte aller psychologischen Befunde ist also nicht replizierbar. Heißt das, alle sozialwissenschaftlichen Lehrbücher aus allen Disziplinen sind zur Hälfte falsch? Die klare Antwort heißt nein. Die akkurate Antwort lautet kommt darauf an.\n\nJenseits der Psychologie\nInwiefern es auf die Disziplin innerhalb der Sozialwissenschaften ankommt, wurde bisher vor allem in der Psychologie untersucht. Aktuelle Tendenzen weisen darauf hin, dass Replikationsraten in der Persönlichkeitspsychologie und kognitiven Psychologie (Soto 2019) höher liegen als die in der Sozialpsychologie (Open Science Collaboration 2015) oder im Marketing (Charlton 2022). Während schon hunderte Replikationsversuche für sozialpsychologische Studien veröffentlicht sind, sind es in anderen Bereichen wie dem Marketing aktuell weniger – Stand Oktober 2022 sogar nur neun. Bereiche außerhalb der Psychologie sind von Replikations-Problemen ebenfalls betroffen. Von Problemen der Replizierbarkeit, Reproduzierbarkeit, und Nachvollziehbarkeit sind fast alle Disziplinen betroffen. Neue Lösungsansätze werden in Medizin, Biologie, Chemie, Physik, Geschichtswissenschaften, Politikwissenschaften, Erziehungswissenschaften, Informatik, und vielen weiteren Bereichen diskutiert.\n\n\n\nLiteratur\n\n\n\n\nAlogna, V. K., M. K. Attaya, P. Aucoin, Štěpán Bahník, S. Birch, A. R. Birt, B. H. Bornstein, u. a. 2014. „Registered Replication Report: Schooler and Engstler-Schooler (1990)“. Perspectives on psychological science : a journal of the Association for Psychological Science 9 (5): 556–78. https://doi.org/10.1177/1745691614545653.\n\n\nBouwmeester, S., P. P. J. L. Verkoeijen, B. Aczel, F. Barbosa, L. Bègue, P. Brañas-Garza, T. G. H. Chmura, u. a. 2017. „Registered Replication Report: Rand, Greene, and Nowak (2012)“. Perspectives on psychological science : a journal of the Association for Psychological Science 12 (3): 527–42. https://doi.org/10.1177/1745691617693624.\n\n\nBrodeur, Abel, Derek Mikola, und Nikolai Cook. 2024. „Mass Reproducibility and Replicability: A New Hope“.\n\n\nButtliere, Brett. 2024. „Was this Registered Report pilot tested? Examination of Vaidis, Sleegers, Van Leeuwen, DeMarree, ... & Priolo, D. (2024)“. PsyArXiv.\n\n\nByers-Heinlein, Krista, Christina Bergmann, Catherine Davies, Michael C Frank, J Kiley Hamlin, Melissa Kline, Jonathan F Kominsky, u. a. 2020. „Building a collaborative psychological science: Lessons learned from ManyBabies 1.“ Canadian Psychology/Psychologie Canadienne 61 (4): 349.\n\n\nCamerer, Colin F., Anna Dreber, Eskil Forsell, Teck-Hua Ho, Jürgen Huber, Magnus Johannesson, Michael Kirchler, u. a. 2016. „Evaluating replicability of laboratory experiments in economics“. Science (New York, N.Y.) 351 (6280): 1433–36. https://doi.org/10.1126/science.aaf0918.\n\n\nCamerer, Colin F., Anna Dreber, Felix Holzmeister, Teck-Hua Ho, Jürgen Huber, Magnus Johannesson, Michael Kirchler, u. a. 2018. „Evaluating the replicability of social science experiments in Nature and Science between 2010 and 2015“. Nature human behaviour 2 (9): 637–44. https://doi.org/10.1038/s41562-018-0399-z.\n\n\nCharlton, Aaron. 2022. „Replications of Marketing Studies“. OpenMKT.org. https://openmkt.org/research/replications-of-marketing-studies/.\n\n\nCheung, Irene, Lorne Campbell, Etienne P. LeBel, Robert A. Ackerman, Bülent Aykutog˘lu, Štěpán Bahník, Jeffrey D. Bowen, u. a. 2016. „Registered Replication Report: Study 1 From Finkel, Rusbult, Kumashiro, & Hannon (2002)“. Perspectives on psychological science : a journal of the Association for Psychological Science 11 (5): 750–64. https://doi.org/10.1177/1745691616664694.\n\n\nChoi, Hong Hui. 2023. „In defense of the resampling account of replication.“ Journal of Theoretical and Philosophical Psychology 43 (4): 249–51. https://doi.org/10.1037/teo0000224.\n\n\nColes, Nicholas A., David S. March, Fernando Marmolejo-Ramos, Jeff T. Larsen, Nwadiogo C. Arinze, Izuchukwu L. G. Ndukaihe, Megan L. Willis, u. a. 2022. „A multi-lab test of the facial feedback hypothesis by the Many Smiles Collaboration“. Nature human behaviour. https://doi.org/10.1038/s41562-022-01458-9.\n\n\nEerland, Anita, Andrew M. Sherrill, Joseph P. Magliano, Rolf A. Zwaan, Jack D. Arnal, Philip Aucoin, Stephanie A. Berger, u. a. 2016. „Registered Replication Report: Hart & Albarracín (2011)“. Perspectives on psychological science : a journal of the Association for Psychological Science 11 (1): 158–71. https://doi.org/10.1177/1745691615605826.\n\n\nErrington, Timothy M., Maya Mathur, Courtney K. Soderberg, Alexandria Denis, Nicole Perfito, Elizabeth Iorns, und Brian A. Nosek. 2021. „Investigating the replicability of preclinical cancer biology“. eLIFE 10. https://doi.org/10.7554/eLife.71601.\n\n\nFeldman, Gilad. 2021. „Replications and extensions of classic findings in Judgment and Decision Making“. https://doi.org/10.17605/OSF.IO/5Z4A8.\n\n\nKakinohana, Regis K., Ronaldo Pilati, und Richard A. Klein. 2022. „Does anchoring vary across cultures? Expanding the Many Labs analysis“. European Journal of Social Psychology. https://doi.org/10.1002/ejsp.2924.\n\n\nKlein, Richard Anthony, Kate A. Ratliff, Michelangelo Vianello, Reginald B. Adams, Štěpán Bahník, Michael J. Bernstein, Konrad Bocian, u. a. 2014. „Investigating Variation in Replicability“. Social Psychology 45 (3): 142–52. https://doi.org/10.1027/1864-9335/a000178.\n\n\nKlein, Richard Anthony, Michelangelo Vianello, Fred Hasselman, Byron Gregory Adams, Reginald B. Adams, Sinan Alper, Mark Aveyard, u. a. 2018. „Many Labs 2: Investigating Variation in Replicability Across Sample and Setting“. https://doi.org/10.31234/osf.io/9654g.\n\n\nKvarven, Amanda, Eirik Strømland, und Magnus Johannesson. 2020. „Comparing meta-analyses and preregistered multiple-laboratory replication projects“. Nature human behaviour 4 (4): 423–34. https://doi.org/10.1038/s41562-019-0787-z.\n\n\nLakens, Daniel. 2023. „Concerns about Replicability, Theorizing, Applicability, Generalizability, and Methodology across Two Crises in Social Psychology“. https://doi.org/10.31234/osf.io/dtvs7.\n\n\nLandy, Justin F., Miaolei Liam Jia, Isabel L. Ding, Domenico Viganola, Warren Tierney, Anna Dreber, Magnus Johannesson, u. a. 2020. „Crowdsourcing hypothesis tests: Making transparent how design choices shape research results“. Psychological Bulletin 146 (5): 451–79. https://doi.org/10.1037/bul0000220.\n\n\nLeBel, Etienne P., Wolf Vanpaemel, Irene Cheung, und Lorne Campbell. 2019. „A Brief Guide to Evaluate Replications“. Meta-Psychology 3. https://doi.org/10.15626/MP.2018.843.\n\n\nLeonelli, Sabina. 2023. Philosophy of Open Science. Cambridge University Press. https://doi.org/10.1017/9781009416368.\n\n\nMcShane, Blakeley B, Ulf Böckenholt, und Karsten T Hansen. 2022. „Variation and covariation in large-scale replication projects: An evaluation of replicability“. J. Am. Stat. Assoc. 117 (540): 1605–21.\n\n\nO’Donnell, Michael, Leif D. Nelson, Evi Ackermann, Balazs Aczel, Athfah Akhtar, Silvio Aldrovandi, Nasseem Alshaif, u. a. 2018. „Registered Replication Report: Dijksterhuis and van Knippenberg (1998)“. Perspectives on psychological science : a journal of the Association for Psychological Science 13 (2): 268–94. https://doi.org/10.1177/1745691618755704.\n\n\nOpen Science Collaboration. 2015. „Psychology: Estimating the reproducibility of psychological science“. Science (New York, N.Y.) 349 (6251): aac4716. https://doi.org/10.1126/science.aac4716.\n\n\nPlesser, Hans E. 2018. „Reproducibility vs. Replicability: A Brief History of a Confused Terminology“. Frontiers in Neuroinformatics 11 (Januar). https://doi.org/10.3389/fninf.2017.00076.\n\n\nRife, Sean, Quinn Lambert, Robert Calin-Jageman, Adamkovic Matus, Gabriel Banik, Itxaso Barberia, Jennifer Beaudry, u. a. 2024. „Registered Replication Report: Study 3 from Trafimow and Hughes (2012)“. Advances in Methods and Practices in Psychological Science.\n\n\nRöseler, Lukas, Taisia Gendlina, Josefine Krapp, Noemi Labusch, und Astrid Schütz. 2022. „Successes and Failures of Replications: A Meta-Analysis of Independent Replication Studies Based on the OSF Registries“. https://doi.org/10.31222/osf.io/8psw2.\n\n\nSikorski, Michał, und Mattia Andreoletti. 2023. „Epistemic functions of replicability in experimental sciences: Defending the orthodox view“. Found. Sci., Februar.\n\n\nSoto, Christopher J. 2019. „How Replicable Are Links Between Personality Traits and Consequential Life Outcomes? The Life Outcomes of Personality Replication Project“. Psychological Science 30 (5): 711–27. https://doi.org/10.1177/0956797619831612.\n\n\nStrack, Fritz, Leonard L. Martin, und Sabine Stepper. 1988. „Inhibiting and facilitating conditions of the human smile: A nonobtrusive test of the facial feedback hypothesis“. Journal of Personality and Social Psychology 54 (5): 768–77. https://doi.org/10.1037/0022-3514.54.5.768.\n\n\nThe Turing Way Community, und Scriberia. 2024. „Illustrations from The Turing Way: Shared under CC-BY 4.0 for reuse“. Zenodo. https://doi.org/10.5281/ZENODO.3332807.\n\n\nTierney, Warren, Jay H Hardy III, Charles R Ebersole, Keith Leavitt, Domenico Viganola, Elena Giulia Clemente, Michael Gordon, u. a. 2020. „Creative destruction in science“. Organizational Behavior and Human Decision Processes 161: 291–309.\n\n\nVaidis, David C, Willem WA Sleegers, Florian Van Leeuwen, Kenneth G DeMarree, Bjørn Sætrevik, Robert M Ross, Kathleen Schmidt, u. a. 2024. „A multilab replication of the induced-compliance paradigm of cognitive dissonance“. Advances in Methods and Practices in Psychological Science 7 (1): 25152459231213375.\n\n\nWagenmakers, Eric-Jan, Titia Beek, Laura Dijkhoff, Quentin F. Gronau, Alberto Acosta, Reginald B. Adams, Daniel N. Albohn, u. a. 2016. „Registered Replication Report: Strack, Martin, & Stepper (1988)“. Perspectives on psychological science : a journal of the Association for Psychological Science 11 (6): 917–28. https://doi.org/10.1177/1745691616674458.",
    "crumbs": [
      "Die Geschichte der Open Science-Bewegung",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Bestandsaufnahme</span>"
    ]
  },
  {
    "objectID": "wandel.html",
    "href": "wandel.html",
    "title": "Wandel im System",
    "section": "",
    "text": "Hat sich die Replizierbarkeitsrate erhöht?\nDie Replikationskrise dauert nun schon über ein Jahrzehnt an und einiges hat sich geändert. Ist dadurch auch das Problem der Replizierbarkeit gelöst? Zum einen ist für viele Bereiche noch gar nicht klar, was sich replizieren lässt. Im Marketing führte die Zeitschrift Journal of Business Research kurzzeitig eine Replikationsecke ein (Easley und Madden 2013), welche dann jedoch in eine andere Zeitschrift verlagert wurde. Aktuell sind Projekte in Arbeit, die die Replizierbarkeit für verschiedene Disziplinen schätzen. Deren Ergebnisse sind größtenteils noch vorläufig und unklar. In einem weiteren Projekt werden Replikationsergebnisse gesammelt, um langfristig je Disziplin und über die Zeit zu schauen, wie sich Replikationsraten verändert haben. Tagesaktuelle Werte sind online verfügbar [https://forrt-replications.shinyapps.io/fred_explorer/; Röseler u. a. (2024)]. Eine Evaluation über mehrere Disziplinen und Jahre erfordert noch hunderte weitere Replikationsstudien.",
    "crumbs": [
      "Die Geschichte der Open Science-Bewegung",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Wandel im System</span>"
    ]
  },
  {
    "objectID": "wandel.html#footnotes",
    "href": "wandel.html#footnotes",
    "title": "Wandel im System",
    "section": "",
    "text": "Sozialwissenschaftler*innen wissen, dass das Aufsummieren der Werte unsinnig ist, da Level 2 nicht “doppelt so gut” wie Level 1 ist, und die verschiedenen Faktoren nicht immer gleich gewichtet werden sollten – rechnerisch wird aber genau das angenommen. Als heuristische Schätzung der Offenheit von Zeitschriften ist dieses Vorgehen jedenfalls sinnvoller als Maße wie Impact-Factor und Hirsch-Index (d.h. bibliometrische Maße).↩︎",
    "crumbs": [
      "Die Geschichte der Open Science-Bewegung",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Wandel im System</span>"
    ]
  },
  {
    "objectID": "struktur.html",
    "href": "struktur.html",
    "title": "Struktur der Vertrauenskrise",
    "section": "",
    "text": "Im Gespräch über Präregistrierungen – eine Methode zur Erhöhung der Replizierbarkeit eines Befundes – entgegnete ein Kollege: “Schön und gut, aber solange sich das System nicht ändert, wird sich an Replikationsraten nichts ändern.” Wie lässt sich dieser Einwand verstehen? Zäumen (bzw. trensen) wir dazu das Pferd von hinten auf: Spätestens seit dem Reproducibility Project Psychology (Open Science Collaboration, 2015) ist den meisten Psycholog*innen klar, dass es große Probleme bei der Replizierbarkeit von Befunden gibt. Woher kommen diese genau? Ein wichtiges Problem ist hierbei der Publikationsbias, womit gemeint ist, dass bei der Veröffentlichung von wissenschaftlichen Berichten eine Auswahl getroffen werden muss und dadurch nur bestimmte Ergebnisse publiziert werden (z.B. Ergebnisse, die eine bestimmte Theorie stützen). Dadurch entsteht ein verzerrtes Bild der Realität. Oft schreiben Wissenschaftler*innen sogar nur bestimmte Befunde zu Berichten zusammen. “Fehlgeschlagene Experimente”, also solche, bei denen eine Hypothese nicht bestätigt oder eine Theorie nicht gestützt werden konnte, landen in der Schublade (engl. File-Drawer-Problem; Rosenthal (1979); Sterling (1959)). In extremeren Fällen bedienen sich Wissenschaftler*innen verschiedener, größtenteils anerkannter Methoden, die Daten so darzustellen, dass sie die Hypothese stützen oder tun so, als ob das, was sich aus den Daten lesen lässt, von Beginn an die Hypothese gewesen wäre (HARKing, Parsons u. a. (2022)). Aber was treibt Personen, die sich für einen Beruf in der Wissenschaft vor allem wegen ihrem Interesse an der Funktionsweise der Welt entschieden haben, die Wahrheit unbewusst zu schönen oder sogar bewusst zu manipulieren? Am Anfang des komplexen Prozesses, welcher zur Replikationskrise geführt hat, steht das aktuelle wissenschaftliche System: Ein Großteil der in der Wissenschaft Beschäftigten arbeitet unter extremem Druck und prekären Arbeitsbedingungen. Um nach ein bis zwei Jahren eine Vertragsverlängerung zu erhalten, müssen Publikationen von Artikeln in möglichst angesehenen Fachzeitschriften nachgewiesen werden. Diese kriegt man durch besonders aussagekräftige und spannende Ergebnisse. Das Anreizsystem der Wissenschaft belohnt also nicht Wahrheit, Genauigkeit, Bescheidenheit, oder Transparenz, sondern vor allem diejenigen Dinge, die nicht in der Hand eine*r Wissenschaftler*in liegen: Spannende und eindeutige Ergebnisse (Bakker, van Dijk, und Wicherts 2012).\nDer Prozess von prekären Arbeitsbedingungen bis zur niedrigen Replikationsrate ist hier veranschaulicht. In den folgenden Kapiteln werden die Probleme und Lösungsansätze im Detail diskutiert.\nAbbildung 2\n\n\n\nDas Anreizsystem der Wissenschaft als Ursache für Replikationsfehlschläge\n\n\n\nLiteratur\n\n\n\n\nBakker, Marjan, Annette van Dijk, und Jelte M. Wicherts. 2012. „The Rules of the Game Called Psychological Science“. Perspectives on psychological science : a journal of the Association for Psychological Science 7 (6): 543–54. https://doi.org/10.1177/1745691612459060.\n\n\nParsons, Sam, Flávio Azevedo, Mahmoud M. Elsherif, Samuel Guay, Owen N. Shahim, Gisela H. Govaart, Emma Norris, u. a. 2022. „A community-sourced glossary of open scholarship terms“. Nature Human Behaviour 6 (3): 312–18. https://doi.org/10.1038/s41562-021-01269-4.\n\n\nRosenthal, Robert. 1979. „The file drawer problem and tolerance for null results“. Psychological Bulletin 86 (3): 638–41. https://doi.org/10.1037/0033-2909.86.3.638.\n\n\nSterling, Theodore D. 1959. „Publication decisions and their possible effects on inferences drawn from tests of significance—or vice versa“. Journal of the American statistical association 54 (285): 30–34.",
    "crumbs": [
      "Die Geschichte der Open Science-Bewegung",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Struktur der Vertrauenskrise</span>"
    ]
  },
  {
    "objectID": "probleme_system.html",
    "href": "probleme_system.html",
    "title": "Das System",
    "section": "",
    "text": "Probleme des wissenschaftlichen Systems\nNeben dem Idealbild davon, was Wissenschaft sein sollte oder wie sie funktionieren sollte, existiert die Wissenschaft, wie sie in unser Gesellschaftssystem integriert ist. Besonderheiten sind dabei, dass Wissenschaftler*innen ihre Tätigkeit als Beruf ausüben, also Geld dabei verdienen. Wie das Geld, das größtenteils aus Steuergeldern stammt, verteilt wird, entscheiden Gremien, die wiederum selbst aus Wissenschaftler*innen bestehen (akademische Selbstverwaltung). Durch die hohe Arbeitsbelastung, gleichzeitig Wissenschaft zu betreiben und zu verwalten, vereinfachen sich die Entscheider*innen die Arbeit und verwenden zur Auswahl hochqualifizierter Personen Abkürzungen. So konnte es passieren, dass die Währung in der Wissenschaft zum Großteil die Anzahl der in Fachzeitschriften veröffentlichten Forschungsartikel (Paper) ist. Ein weiterer verwendeter Indikator ist die Anzahl, in wie vielen weiteren wissenschaftlichen Artikeln auf die Forschung einer Person verwiesen wird, also die Zitationszahlen. Vorbilder wie Charles Darwin oder William James und selbst aktuelle Nobelpreisträger hätten nach dem heutigen Maßstab keine Chance auf eine unbefristete Stelle in der Wissenschaft - sie haben einfach nicht genug Paper geschrieben. Viele der hier diskutierten Problemen sind beispielsweise in der Psychologie seit mehreren Jahrzehnten bekannt, sodass eine Replikationskrise vorprogrammiert war (Cronbach, Snow, und Wiley 1991; Greenwald 1976; Baker 2016).",
    "crumbs": [
      "Probleme",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Das System</span>"
    ]
  },
  {
    "objectID": "probleme_system.html#probleme-des-wissenschaftlichen-systems",
    "href": "probleme_system.html#probleme-des-wissenschaftlichen-systems",
    "title": "Das System",
    "section": "",
    "text": "Wissenschaft versus Academia\nWährend einige Natur- und Sozialwissenschaften in ihrer Anfangszeit oft von Buchveröffentlichungen lebten und darin eine umfangreiche Basis von meist einzelnen Personen erarbeitet wurde (z.B. Galileo Galilei für die Physik oder William James für die Psychologie), stehen heutzutage wissenschaftliche Fachzeitschriften im Fokus. Diese Zeitschriften sind vergleichbar mit solchen, die es im Kiosk und im Supermarkt gibt, nur bestehen sie halt aus (meist englischsprachigen) Artikeln, die Wissenschaftler*innen verfasst haben, und sind in vielen Fällen nur noch online oder in Hochschulbibliotheken erhältlich. Forschende laden sich dann einzelne Artikel aus den Zeitschriften aus dem Internet über das Hochschulnetezwerk herunter und Bibliotheken haben Verträge mit Verlagen und zahlen Geld, damit Universitätsangehörige Zugriff zu den Katalogen haben. Jeder eingereichte Artikel befasst sich mit einer Fragestellung, die von den Forschenden selbst festgelegt wurde (z.B. Wie viele psychologische Studien lassen sich im Mittel replizieren?). In den Artikeln werden meistens Studien mit Ergebnissen berichtet. Vor der Veröffentlichung werden die Artikel begutachtet (Review) – nicht von Mitarbeitenden der Zeitschrift sondern von Kolleg*innen (Peers). Mit diesem Peer-Review soll die Qualität von Forschung sichergestellt werden. Üblicherweise wird dabei darauf geachtet, dass die Schlussfolgerungen auf Basis der erhobenen Daten gerechtfertigt sind, die Fragestellung klar beantwortet wird, der Artikel verständlich ist, und die Befunde spannend oder überraschend sind. Zeitschriften unterscheiden sich darin, welche Themen sie abdecken (z.B. Sozialpsychologie, Konsumentenverhalten, Angewandte Sportwissenschaft), wie streng das Peer-Review ist, und von wie vielen Forschenden sie gelesen und zitiert werden. Wissenschaften sind also stark integriert in ein System, das Forschenden und Verlagen erlaubt, ein festes Gehalt zu verdienen.\n\n\nPrekäre Arbeitsbedingungen\nSoweit der Rahmen: Wissenschaftler*innen forschen und teilen ihre Ergebnisse meist in Form von Publikationen wie Zeitschriftenartikel- oder Buch-Veröffentlichungen. Was die Jobs in der Wissenschaft (also vorwiegend an Hochschulen) angeht, so sind sie hierarchisch strukturiert. Es gibt wenige unbefristete Stellen, meist Professuren, und darunter befristete Stellen für Promovierende und bereits promovierte Post Docs. Wer in der Wissenschaft arbeitet, befindet sich meistens in einem harten Konkurrenzkampf um eine der wenigen unbefristeten Stellen (Rahal u. a. 2023). Der Hintergedanke ist dabei, dass Konkurrenz zwischen Forschenden die Produktivität steigern möge. Vom Start der Promotion1 bis zur Berufung auf eine Professur, also eine der raren unbefristeten Stellen, dauern Verträge meistens nur ein bis drei Jahre und die Stellen haben einen Umfang von weniger als 100%. Gleichzeitig ist es in vielen Fächern unüblich, mit weniger als 40 Stunden pro Woche innerhalb von den typischen drei Jahren die Promotion erfolgreich abzuschließen. Während ein Großteil aller wissenschaftlichen Veröffentlichungen auf Studien beruht, die im Rahmen von Doktorarbeiten durchgeführt wurden, sind Doktorand*innen gleichzeitig diejenigen Personen im System, die den geringsten Wert haben bzw. deren Arbeitskraft am günstigsten ist. Forschende auf befristeten Stellen sind also einem enormen Leistungsdruck ausgesetzt. Psychische Probleme wie Burnout oder Depressionen sind unter Promovierenden weit verbreitet (Jaremka u. a. 2020; Liu u. a. 2019). Den Weg zur Professur schaffen vor allem die Personen erfolgreich, die viele Artikel in prestigeträchtigen Zeitschriften veröffentlichen. Durch die immense Arbeitsbelastung und große Zahl an Artikeln, die bei Fachzeitschriften eingereicht werden, ist keine Zeit mehr, Ergebnisse genau zu prüfen und nachzurechnen (Nuijten u. a. 2017), sondern es wird vor allem darauf geachtet, wie eindeutig die Ergebnisse die Fragestellung beantworten - oder genauer gesagt: bestätigen (Giner-Sorolla 2012; Mynatt, Doherty, und Tweney 1977). Mit anderen Worten: Es wird ausgerechnet der Teil der wissenschaftlichen Arbeit belohnt, der nicht in der Hand der Forschenden liegt, nämlich die Ergebnisse von Untersuchungen. Veröffentlichte Artikel und Prestige statt Qualität (Brembs 2018) sind ab dort die Währung der Wissenschaft: Auf ihrer Basis wird entschieden, wer Forschungsgelder erhält, und auf Basis von Forschungsgeldern und Publikationen werden Professuren vergeben. In den darüber entscheidenden Berufungskommissionen lesen die Beteiligten üblicherweise nicht die Artikel der Bewerbenden, sie zählen bloß, wie viele in welchen Zeitschriften aufgelistet werden. Teilweise werden die Bewerber*innen gebeten, Zitationszahlen anzugeben. Manche dieser Zahlen (z.B. Impact Factor) gehören Unternehmen an und der Zugang muss über die Institution erkauft werden.\nZu diesen verheerenden Problemen kommen außerdem systemische Probleme der sexuellen Belästigung (Hoebel u. a. 2022) und des Machtmissbrauches, die in dem aktuell streng hierarchischen Aufbau des Systems nur schwierig zu lösen sind (Forster und Lund (2018), siehe auch https://www.netzwerk-mawi.de/ und https://www.jmwiarda.de/2023/11/20/das-stille-leiden-der-betroffenen/). Laut Berichten des Netzwerkes gegen Machtmissbrauch in der Wissenschaft werden Fälle verschwiegen und die Schuldigen wechseln stillschweigend die Universität, sodass das Problem nicht gelöst wird. Wer es dabei besonders leicht hat, erklären Elsherif u. a. (2022) anschaulich an dem Academic Wheel of Privilege (“Akademisches Rad der Privilegierten”; S. 85; siehe auch https://www.psychologicalscience.org/observer/gs-navigating-academia-as-neurodivergent-researchers). Beispielsweise haben Doktorandinnen in den Niederlanden vor allem dann schlechtere Noten als Doktoranden bekommen, wenn der Promotionsausschuss, also die Gruppe an Professor*innen, die die Promotion beurteilt, nur aus Männern bestand (Bol 2023). Schwerbehindertenquoten weit unter den Quoten anderer Berufe kommen ebenfalls hinzu.\n\n\nZu viel Forschung\nIm Rahmen von Promotionen müssen Forschende in insgesamt 3-6 Jahren zuzüglich Elternzeiten üblicherweise drei wissenschaftliche Artikel veröffentlichen (bzw. in fairen Fällen drei veröffentlichungs-würdige Artikel vorweisen). Bei Post Docs (siehe Prekäre Arbeitsbedingungen) müssen es noch mehr sein. Dass Personen vor und nach der Promotion jeweils maximal 6 Jahre an deutschen Hochschulen angestellt sein dürfen, ist gesetzlich festgelegt. Für die weitere Qualifikation Habilitation, für die eine ähnliche Zeit angesetzt ist, sind zum Beispiel in der Psychologie circa sechs Artikel die Daumenregel. Dabei spielt es eine nachrangige Rolle, wie umfangreich die Artikel sind. Beispielsweise dauert die Durchführung einer Meta-Analyse, in der bisherige Befunde zu einem bestimmten Thema systematisch gesammelt und statistisch zusammengefasst bzw. verglichen werden, oft mehrere Jahre. Eine Längsschnitterhebung kann je nach Forschungsfrage sogar Jahrzehnte dauern. Im Kontrast dazu lässt sich eine Querschnittserhebung über einen Online-Fragebogen in wenigen Wochen durchführen. Eine Doktorandin, die eine einzige Meta-Analyse durchführt, könnte damit nicht promovieren. Hätte sie stattdessen drei einfache Online-Studien durchgeführt und einzeln veröffentlicht, wäre das einfacher.2 Diese willkürlichen Vorgaben haben dazu geführt, dass sich Wissenschaftler*innen alleine durch die Begutachtung der Artikel ihrer Kolleg*innen einen enormen Arbeitsaufwand auferlegen, der den wissenscchaftlichen Fortschritt behindert (Hanson u. a. 2023).\nZur Veranschaulichung des Aufgabenpensums nun ein Gedankenspiel: Angenommen es gäbe 10 Wissenschaftler*innen, die gemeinsam 10 Artikel im Jahr veröffentlichen würden - manchmal alleine, manchmal in einer Gruppe - und jeder der Artikel würde von zwei Personen begutachtet, so müsste jede*r zwei Artikel begutachten. Damit das System funktioniert, müsste jede Person die Anzahl der im Schnitt veröffentlichten Artikel mal die Anzahl der benötigten Gutachtenden begutachten. Bei 10 Veröffentlichungen pro Person und drei Gutachtenden wären es 10x3=30 Gutachten. Nun werden aber nicht alle Artikel von der Zeitschrift, bei der sie eingereicht werden, veröffentlicht, noch werden sie sofort veröffentlicht. Wissenschaftler*innen reichen ihre Artikel oft bei den “hochrangigsten” Zeitschriften ein. Nachdem dort mehrere Gutachter*innen den Artikel geprüft haben, wird er abgelehnt (Jaremka u. a. 2020). Im Mindestfall werden Revisionen angefordert, welche oft eine weitere Runde Peer Review auslösen und nicht immer werden Artikel danach veröffentlicht. Unsere Rechnung geht also nicht auf: Nehmen wir vorsichtshalber an, ein Artikel würde neun Mal begutachtet (z.B. einmal drei Gutachtende, dann Ablehnung, dann erneut drei Gutachtende, Revision, zweites Gutachten, Akzeptanz). Aus 10x3 wird 10x9, bei etwas Urlaubszeit also etwas mehr als zwei Gutachten pro Woche, idealerweise bis zu zwei Arbeitstage. Bei dieser Rechnung bleibt weniger Zeit für Lehre, Wissenstransfer, Betreuung von Studierenden oder Promovierenden, Einwerbung von Forschungsgeldern, universitäre Selbstverwaltung, usw. Durch die vielen zu publizierenden Artikel und das strenge Review-System bürgt sich die Wissenschaft einen großen Berg Arbeit auf – einen, der realistisch nicht machbar ist und unter dem am Ende die Qualität der Forschung leidet. Beispielsweise fiel es weder den Gutachtenden, noch den Herausgebern von Zeitschriften auf, dass in über 30 Artikeln mitten im Text „Regenerate response” stand – ein Satz, der in OpenAIs ChatGTP Programm erlaubt, einen von einer künstlichen Intelligenz erstellten Text auf Knopfdruck umzuformulieren (https://retractionwatch.com/2023/10/06/signs-of-undeclared-chatgpt-use-in-papers-mounting/). In manchen Artikeln hieß es sogar „As an AI language model, I …” (https://pubpeer.com/search?q=“As+an+AI+language+model%2C+I“). In einem Fall wurde der Artikel von dem Verlag Elsevier geändert, und zwar nicht auf dem empfohlenen Weg3 mittels eines transparenten Errandum oder Corrigendum, also einer öffentlichen Mitteilung über die Änderung und ihre Gründe, sondern ohne Erklärung oder Zustimmung der Autor*innen (https://predatory-publishing.com/elsevier-changed-a-published-paper-without-any-explanation/).\n\n\nPublish or Perish – Veröffentlichen oder Verenden\nWer in der Wissenschaft arbeitet, sollte die wichtigste Spielregel kennen: Wer überleben will, muss Artikel veröffentlichen. Kurz: Veröffentlichen oder Verenden (engl. publish or perish). Zur Promotion, Habilitation, Einwerbung von Forschungsgeldern, und zur Berufung auf eine Professur sind Veröffentlichungen das oberste Kriterium. Kennzeichen einer Währung ist, dass sich Dingen ein Wert zuweisen lässt. Wie sieht der Wert in der Forschung aus? Bibliometriker*innen entwarfen zur Beschreibung und zur Auswahl von Zeitschriftenabonnements (also explizit nicht zur Bewertung) verschiedener Forschungsgebiete verschiedene Kennzahlen, wie den Impact Factor, oder Hirsch-Index. Beide Zahlen bestehen aus Verrechnungen davon, wie oft Artikel je nach Zeitschrift oder je nach Person zitiert wurden. Beispielsweise werden beim Journal Impact Factor die Gesamtzahl der Zitationen in einem bestimmten Jahr durch die Anzahl der zitierbaren Veröffentlichungen in Bezugsjahren (z.B. den drei vorangegangenen Jahren) geteilt. Zeitschriften werden mit hohen Impact Factors beworben und erlauben teilweise nicht die Zitation von Kommentaren (bzw. veröffentlichen Kommentare unter derselben Referenz wie Originalstudien), um möglichst hohe Impact Factors zu erhalten. Sie können natürlich auch entscheiden, ob sie den 3-Jahres- oder 2-Jahres Impact Factor berichten. Berechnungsweisen unterscheiden sich außerdem darin, auf Basis welcher Daten sie errechnet werden. Die Datenbank des Journal Impact Factors gehört dem Unternehmen Clarivate Analytics und ist nicht öffentlich zugängig. Das heißt, die genauen Zahlen lassen sich nicht einfach nachrechnen und prüfen. Durch die wichtige und intransparente Rolle von Zitationsmetriken ist wenig überraschend, dass sie manipuliert werden und dass 3 der Top 10 Zeitschriften in einem Bereich gar keine wissenschaftlichen Zeitschriften sind. Klar ist auch, dass Zitationsmetriken entweder gar nicht oder negativ mit wissenschaftlicher Qualität zusammenhängen [Brembs (2018); Etzel u. a. (2024); Tabelle 3]. Zum Beispiel gab es das Problem, dass das Programm Microsoft Excel Genomnamen als Datumsangaben erkannt hat, umformatiert hat, und die eigentlichen Namen nicht mehr erkennbar waren. Somit waren Teile der jeweiligen Ergebnisse nutzlos (Ziemann, Eren, und El-Osta 2016). Dieses Problem kam vor allem bei angesehenen Zeitschriften vor. Sie haben nichts dagegen unternommen, stattdessen wurde Excel nach ein paar Jahren angepasst.\nNeben Zeitschriften-Listen können auch Ranglisten von Personen erstellt werden. Forschende aus Stanford veröffentlichten eine solche Liste der 20 meistzitierten Forschenden. Abgesehen davon, dass sie zum Missbrauch verführt, enthält sie zahlreiche Fehler (Abduh 2023) wie zum Beispiel Forschende, die hunderte Jahre lang Artikel veröffentlicht haben – vor und nach ihrem Tod. Unternehmen, denen Verlage und andere Werkzeuge für die Wissenschaft (z.B. Programme zur Verwaltung von Literatur) gehören, sammeln darüber hinaus Daten über die Forschenden (z.B. welche Artikel wie lange aufgerufen werden, welche Textpassagen markiert werden). Teilweise werden Bibliotheken aufgefordert, Überwachungsprogramme von Verlagen zu installieren. Die gesammelten Daten verkaufen die Verlage dann zurück an die Forschenden. Diese Praxis gefährdet die Wissenschaftsfreiheit, da Staaten Verlage auffordern können, die Namen von Forschenden zu nennen, die zu politisch brisanten Themen forschen. Die Initiative Stop Tracking Science setzt sich gegen dieses Verhalten ein.\nSeit längerem wird für die verantwortungsvolle Verwendung dieser Metriken plädiert (Hicks u. a. 2015) und zahlreiche Universitäten und Forschende schließen sich zusammen, um Bewertung von Forschung sinnvoll zu gestalten (z.B. mittels der San Francisco Erklärung zur Forschungsbewertung). Wie sich Anreizstrukturen und Karrierestatus auf wissenschaftliches Fehlverhalten auswirken, wird mit gemischten Ergebnissen untersucht. Das Problem: Sobald es in einem System ein klares Bewertungskriterium gibt, wird alles darauf ausgerichtet (gaming the system). Anreizstrukturen, die schlechte Forschung zur Folge haben, herrschen in Bezug auf Bildmanipulationen in der Biologie laut Fanelli u. a. (2022) vor allem in China, weniger jedoch in USA, Großbritannien, und Kanada.\nEin weiteres Produkt der Publish or Perish Struktur ist, dass die meisten in der Psychologie entwickelten Instrumente zur Messung von Persönlichkeitseigenschaften nur wenige Male verwendet werden – und das auch hauptsächlich von ihren eigenen Entwickler*innen (Elson u. a. 2023). Elson u. a. (2023) plädieren: Psychologische Messinstrumente sind keine Zahnbürsten! Ein noch extremeres Ausmaß ist bei sogenannten Paper Mills zu sehen (van Noorden 2023): Personen erstellen dabei automatisiert große Mengen von wissenschaftlichen Artikeln, ohne die darin beschriebenen Untersuchungen wirklich durchzuführen. Wissenschaftler*innen können dann Ko-Autor*innenschaften kaufen, um ihre Anzahl an veröffentlichten Artikeln zu erhöhen und mehr Zitationen zu bekommen. Je nach Zeitschrift werden diese Artikel nicht im Peer Review, also der Begutachtung durch andere Forschende, entlarvt. Es wird befürchtet, dass Käufer*innen solcher Artikel selbst sehr erfolgreich werden können, selbst Herausgeber*innen von Zeitschriften werden, und sich dadurch selbst schützen, ertappt zu werden. Der genaue Ausmaß des Paper-Mill-Problems ist unklar und weitgehend unerforscht (Byrne und Christopher 2020). Ein Indiz, mithilfe künstlicher Intelligenz erstellte Artikel zu erkennen, sind sogenannte tortured phrases (Cabanac, Labbé, und Magazinov 2021), welche grammatisch korrekt sind, im üblichen Sprachgebrauch jedoch selten vorkommen oder wenig Sinn machen.\n\n\nFlaschenhals-Hypothese und Innovationsdrang\nNamhafte wissenschaftliche Zeitschriften erhalten täglich unzählige Einreichungen, veröffentlichen aber nur eine begrenzte Anzahl an Artikeln. Sie müssen also streng selektieren, was begutachtet und gegebenenfalls veröffentlicht wird. Weil das Ziel einer Zeitschrift ist, viel gelesen zu werden, wählen Herausgeber*innen von Zeitschriften diejenigen Artikel, welche möglichst großes Potenzial haben, bekannt und viel zitiert zu werden (Giner-Sorolla 2012). Das betrifft zum Beispiel Beiträge mit besonderen praktischen Implikationen, überraschenden Befunden, oder besonders konsistenten Befunden. Studien, deren Ergebnisse keine eindeutigen Schlüsse zulassen – oder deren Autor*innen mit zu großer Vorsicht Schlüsse ziehen – kommen also nicht infrage. In den Neurowissenschaften kommunizieren manche Zeitschriften beispielsweise öffentlich, dass sie keine Replikationsstudien veröffentlichen und nach Neuheit selektieren, während die meisten anderen Zeitschrfiten keine Stellung dazu nehmen (Yeung 2017). In der Psychologie nahmen 2017 nur 33 von 1151 Zeitschriften Stellung dazu, dass sie Replikationen akzeptieren (Martin und Clarke 2017). Innovative Befunde oder Befunde, die als innovativ dargestellt werden, (Stavrova u. a., o. J.) werden häufiger zitiert, die Zeitschrift erhält also mehr Leser*innen, mehr Einreichungen, und damit mehr Geld über Abonnements und Veröffentlichungskosten, vielzitierte Artikel lassen sich tendenziell jedoch schlechter replizieren als weniger zitierte (Serra-Garcia und Gneezy 2021) und prestigereiche Zeitschriften sind Magneten für fragwürdige Forschungspraktiken (Kepes u. a. 2022) und nachweisbar gleichwertige oder sogar qualitativ schlechtere Forschung (Brembs 2018). \nWie kommt es dazu? Es ist die Rede von einem Flaschenhals (Bottleneck; viele Einreichungen aber wenige Veröffentlichungen). Das führt gemeinsam mit dem Anreiz in eben solchen Zeitschriften zu publizieren dazu, dass Forschende alle möglichen Mittel nutzen, um eine Chance auf eine Publikation zu erhalten. In manchen Instituten gilt, wer in einer bestimmten Zeitschrift veröffentlicht, erhält automatisch die Bestnote auf die Promotion. Andere Institute erklären schon in der Stellenausschreibung für eine Promotionsstelle, in welcher Zeitschrift die Ergebnisse des Forschungsprojektes veröffentlicht werden müssen. Die Tatsache, dass prestigereiche Zeitschriften Nature oder Science vor allem Artikel mit klaren Botschaften veröffentlichen - also Artikel, die auch häufiger gelesen werden (Stavrova u. a. 2024) - spornt also Forschende an, klare Ergebnisse zu erschaffen. Passt mal ein Befund nicht zu der geprüften Hypothese, wird er entweder nach den aktuell vielerorts geduldeten Regeln (Questionable Research Practices) der Datenanalyse so verzerrt, dass er passt, oder gar nicht veröffentlicht und landet in der Schublade (Gopalakrishna u. a. 2021; Schneider u. a. 2024).\n\n\nSchubladen-Problem\nSeit mehreren Jahrzehnten ist bekannt, dass Wissenschaftler*innen vor allem diejenigen Studien veröffentlichen, die ihre Theorien stützen (Rosenthal 1979; Sterling 1959). Im Extremfall hat jemand zum Prüfen einer Theorie fünf Studien durchgeführt, in nur einer davon die Theorie bestätigt, und nur diese veröffentlicht. Andere Forschende, die dann die (veröffentlichte) Literatur durchsuchen, sehen nur die “erfolgreiche” Studie. Es entsteht der Eindruck, dass die Theorie stimmt, während die Mehrheit der Studien diesen Schluss eigentlich nicht nahelegt. Dass Studienergebnisse natürlichen, statistischen Schwankungen unterliegen, führt dazu, dass bei vielen Studien auch eine dabei sein kann, die das gewünschte Ergebnis zeigt. Durch das Schubladen-Problem konnten sich ganze Forschungsstränge entwickeln, die seit dem Bewusstsein für Replikationsstudien komplett ausgestorben sind (Brockman 2022; Mac Giolla u. a. 2022).\nFür Meta-Analysen, also Studien, die bisherige Befunde zusammenfassen, wurden bereits verschiedene Methoden entwickelt, die Stärke des Schubladen-Problems (engl. File-Drawer-Problem) zu prüfen. Auch Methoden, diese Verzerrung zu korrigieren, existieren bereits vielzählige (Fisher, Tipton, und Zhipeng 2017; Hedges und Vevea 1996; Schimmack 2020; Simonsohn, Nelson, und Simmons 2014; van Aert und van Assen 2018). Allerdings funktioniert keine der Methoden in allen möglichen Szenarien (Carter u. a. 2019). Um eine Veröffentlichung der fehlgeschlagenen Studien kommen Forschende also nicht herum.\nIn der Medizin gibt es den besonderen Fall, dass alle dort durchgeführten Studien öffentlich registriert werden müssen. Bei einer Veröffentlichung muss dann eine Registrierungsnummer angegeben werden. Über öffentliche Angaben zu registrierten Studien lässt sich somit nachverfolgen, welche Personen, Institutionen, oder Länder wie viele ihrer tatsächlich durchgeführten Studien veröffentlichen. Forschende in Berlin haben dazu eine Website mit einem sogenannten interaktiven Dashboard entwickelt, um die darüber gesammelten Daten durchsuchen und abbilden zu können (Franzen u. a. 2023; Riedel u. a. 2022). Auf https://quest-cttd.bihealth.org/ ist nach aktuellem Stand (Juli 2024) sichtbar, dass von allen registrierten Studien nur 46% innerhalb der folgenden zwei Jahre und 74% innerhalb der folgenden fünf Jahre veröffentlicht wurden. Personen, die sich für die Studien als Versuchspersonen melden oder Drittmittelgeber erhalten somit Aufschluss über die Größe der Schublade, in der die fehlgeschlagenen Studien und die “nicht so spannenden Ergebnisse landen”.\n\n\nZugängigkeit von Wissen\nDurch die Einbindung von Forschung in das kommerzielle Verlagssystem befindet sich ein Großteil der Wissenschaften in einem sozialen Dilemma, das einen massiv eingeschränkten Zugang zum Wissen zur Folge hat. Das vorherrschende Modell bei wissenschaftlichen Zeitschriften, die zum Großteil Verlagen wie Springer, Elsevier, Sage, oder Taylor and Francis angehören, ist ein Abonnement-Modell. Hochschul-Bibliotheken zahlen regelmäßig Geld an die Verlage, damit die Hochschul-Angehörigen (also Studierende und Mitarbeitende) Zugriff auf die darin veröffentlichten Arbeiten haben. Wer kein Abonnement hat, kann Artikel einzeln kaufen. Versucht man, einen Artikel online herunterzuladen, ohne dass man sich in einem Hochschulnetzwerk befindet, geht das nicht kostenlos: Der Artikel befindet sich hinter einer Bezahlschranke (Paywall). Soll ein Artikel für alle frei zugänglich veröffentlicht werden (Open Access, öffentlicher Zugang), kostet das extra, nämlich je Artikel zwischen 2000€ und 9000€. Um Forschung also lesen zu können, müssen Hochschulen Abonnements oder Artikel kaufen. Infolge dessen sind finanziell schlechter ausgestattete einzelner Personen, Institutionen, Länder, oder sogar gesamte Regionen wie der globale Süden in ihrer Teilnahme am internationalen Wissenschaftsdiskurs systematisch benachteiligt.\nDas soziale Dilemma besteht nun in der Schwierigkeit, dieses System zu verändern. Brembs u. a. (2023) beschreiben es wie folgt: Bibliotheken schließen die Abos mit Verlagen ab, um Forschung an ihren Hochschulen zu ermöglichen. Würden sie die Verträge kündigen, könnte das die Forschung verlangsamen und die Stellung einer Hochschule verschlechtern. Forschende können die namhaften Zeitschriften der kommerziellen Verlage nicht boykottieren, da sie sonst ihre Karriere gefährden würden. Sie sind darauf angewiesen, in prestigereichen Zeitschriften zu publizieren. Zudem ist es extrem schwierig, das Verhalten von Millionen von Forschenden, von denen die meisten viele Jahre lang mit dem aktuellen System gelebt haben, schlagartig zu verändern. Die Zeitschriften, als dritter Akteur in diesem Dilemma, profitieren von der Abhängigkeit und können Preise beliebig steigen lassen. Abgesehen vom Markenwert der Zeitschriften, also dem Ansehen und dem Vertrauen, das ihnen fälschlicherweise (Brembs 2018) entgegengebracht wird, tragen sie zu diesem System fast nichts bei. Universitäten und Länder haben bereits jetzt die Möglichkeit, Forschungsartikel online zu verwalten; die Begutachtung und Qualitätssicherung geschieht durch freiwillige Forschende und nicht durch Angestellte der Zeitschrift; und die Formatierung der Artikel können Forschende wie an bestehenden 4 ersichtlich ist (Carlsson u. a. 2017), mit geringem Aufwand selbst übernehmen. Entsprechende Lösungen werden in Open Access Publikationen erläutert.\n\n\nWissenschaftliche Qualitätskontrolle\nDurch die Probleme des wissenschaftlichen Systems zieht sich das Problem der Qualitätskontrolle. Im Begutachtungsprozess vor der Veröffentlichung werden viele Fehler nicht erkannt und nach der Veröffentlichung sind Berichte so intransparent, dass es oft nicht einmal möglich ist, gefälschte Daten zu entlarven. Gutachten wissenschaftlicher Artikel bleiben meistens unter Verschluss und wenn eine Zeitschrift einen Artikel ablehnt, wird er bei einer anderen eingereicht. Die bereits herausgestellten Kritikpunkt gehen dabei verloren (Aczel, Szaszi, und Holcombe 2021).\nWissenschaftliche Artikel haben am Ende einen Absatz zu möglichen Interessenkonflikten. Dort müssen Forschende angeben, wenn sie Leistungen von Unternehmen für die Forschung bekommen haben, oder in irgendeiner Weise von der Forschung profitieren (z.B. Aktien des Unternehmens besitzen, dessen Medikament sie positiv gestetet haben). In der Glücksspiel-Forschung fehlen diese Angaben oft (Heirene u. a. 2021). Einige große Unternehmen untergraben systematisch den wissenschaftlichen Konsens, indem sie falsche Informationen streuen: Dazu gehört beispielsweise die Tabakindustrie und der sichere negative Effekt vom Rauchen auf die Gesundheit (Reed u. a. 2021).\nZeitschriften machen sich die Lage zu Nutzen, indem sie gegen eine schwache oder gar keine Qualitätskontrolle und hohe Publikationskosten (z.B. 4500€ pro Artikel) Forschung veröffentlichen. Diese Praxis wird Predatory Publishing genannt und meint, dass Forschende sich Publikationen erkaufen und Verlage dadurch Profit schöpfen. Teilweise ist aufgrund des anonymen und unter Verschluss gehaltenen Peer Review Prozesses unklar, ob es sich bei einer Zeitschrift um eine mit oder ohne vorliegende Qualitätskontrolle handelt. Einen Schritt weiter gehen Paper-Mills. Dabei können Personen Ko-Autor*innenschaften bei Artikeln kaufen. Die Artikel sind häufig von einzelnen Personen oder mittels Algorithmen generiert. Fälle, in denen Unsinn-Artikel entlarvt werden, gibt es häufig, allerdings werden die Artikel nur selten zurückgezogen oder öffentlich mit einer Notiz, die den Fehler erklärt versehen (Cabanac und Labbé 2021). Neben Publikationen können Forschende auch Zitationen kaufen oder selbst künstlich erhöhen (Singh Chawla 2024), so wurde laut Angaben von Google Scholar beispielsweise kurzzeitig die Katze Larry 132 Mal zitiert. Durch die Gutachten, die unter Verschluss bleiben, konnten sich auch “Review-Mills” bilden: Dort werden Autor*innen dazu genötigt, bestimmte Forschungsartikel zu zitieren, um den Gutachtenden höhere Zitationszahlen zu verschaffen. Die Gutachten bestehen dabei aus vagen Textbausteinen (Oviedo-Garcı́a 2024).",
    "crumbs": [
      "Probleme",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Das System</span>"
    ]
  },
  {
    "objectID": "probleme_system.html#weiterführende-informationen",
    "href": "probleme_system.html#weiterführende-informationen",
    "title": "Das System",
    "section": "Weiterführende Informationen",
    "text": "Weiterführende Informationen\n\nDer kostenlose Film “Paywall: The Business of Scholarship” behandelt das Thema Paywalls sowie den öffentlichem Zugang von wissenschaftlichem Wissen: https://paywallthemovie.com/paywall\nDas Thema der Neurodiversität in der Forschung diskutieren Elsherif u. a. (2022). Via FORRT.org verwaltet die Gruppe darüber hinaus eine Datenbank neurodivergenter Forschender und weitere Projekte.\nThériault und Forscher haben kürzlich das Missing Majority Dashbaord veröffentlicht (Thériault 2023), das automatisch mittels der OpenAlex Literaturdatenbank darstellt, von welchen Kontinenten Autor*innen stammen. Nordamerika und Europa sind dabei deutlich überrepräsentiert.\nTED Talk zur inklusiven Wissenschaft: https://www.youtube.com/watch?v=tB_HeqnonNM\nRetractions der Retractionwatch Database können im Retraction Dashboard visuell nach Land und nach Typ angeschaut werden: https://retraction-dashboard.netlify.app",
    "crumbs": [
      "Probleme",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Das System</span>"
    ]
  },
  {
    "objectID": "probleme_system.html#literatur",
    "href": "probleme_system.html#literatur",
    "title": "Das System",
    "section": "Literatur",
    "text": "Literatur\n\n\n\n\nAbduh, Akira J. 2023. „A critical analysis of the world’s top 2% most influential scientists: Examining the limitations and biases of highly cited researchers lists“.\n\n\nAczel, Balazs, Barnabas Szaszi, und Alex O Holcombe. 2021. „A billion-dollar donation: estimating the cost of researchers’ time spent on peer review“. Research Integrity and Peer Review 6: 1–8.\n\n\nBaker, Monya. 2016. „1,500 scientists lift the lid on reproducibility“. Nature 533 (7604): 452–54.\n\n\nBol, Thijs. 2023. „Gender inequality in cum laude distinctions for PhD students“. Sci. Rep. 13 (1): 20267.\n\n\nBrembs, Björn. 2018. „Prestigious Science Journals Struggle to Reach Even Average Reliability“. Frontiers in human neuroscience 12: 37. https://doi.org/10.3389/fnhum.2018.00037.\n\n\nBrembs, Björn, Philippe Huneman, Felix Schönbrodt, Gustav Nilsonne, Toma Susi, Renke Siems, Pandelis Perakakis, Varvara Trachana, Lai Ma, und Sara Rodriguez-Cuadrado. 2023. „Replacing academic journals“. Royal Society Open Science 10 (7). https://doi.org/10.1098/rsos.230206.\n\n\nBrockman, John. 2022. „Adversarial Collaboration: An EDGE Lecture by Daniel Kahneman“. https://www.edge.org/adversarial-collaboration-daniel-kahneman.\n\n\nByrne, Jennifer A., und Jana Christopher. 2020. „Digital magic, or the dark arts of the 21st century-how can journals and peer reviewers detect manuscripts and publications from paper mills?“ FEBS letters 594 (4): 583–89. https://doi.org/10.1002/1873-3468.13747.\n\n\nCabanac, Guillaume, und Cyril Labbé. 2021. „Prevalence of nonsensical algorithmically generated papers in the scientific literature“. Journal of the Association for Information Science and Technology 72 (12): 1461–76.\n\n\nCabanac, Guillaume, Cyril Labbé, und Alexander Magazinov. 2021. „Tortured phrases: A dubious writing style emerging in science. Evidence of critical issues affecting established journals“. https://doi.org/10.48550/arXiv.2107.06751.\n\n\nCarlsson, Rickard, Henrik Danielsson, Moritz Heene, Åse Innes-Ker, Daniël Lakens, Ulrich Schimmack, Felix D. Schönbrodt, Marcel van Asssen, und Yana Weinstein. 2017. „Inaugural Editorial of Meta-Psychology“. Meta-Psychology 1: a1001. https://doi.org/10.15626/MP2017.1001.\n\n\nCarter, Evan C., Felix D. Schönbrodt, Will M. Gervais, und Joseph Hilgard. 2019. „Correcting for Bias in Psychology: A Comparison of Meta-Analytic Methods“. Advances in Methods and Practices in Psychological Science 2 (2): 115–44. https://doi.org/10.1177/2515245919847196.\n\n\nCronbach, Lee J., Richard E. Snow, und David E. Wiley. 1991. Improving inquiry in social science: A volume in honor of Lee J. Cronbach. Hillsdale, N.J.: Lawrence Erlbaum Associates.\n\n\nElsherif, Mahmoud Medhat, Sara Lil Middleton, Jenny Mai Phan, Flavio Azevedo, Bethan Joan Iley, Magdalena Grose-Hodge, Samantha Lily Tyler, u. a. 2022. „Bridging Neurodiversity and Open Scholarship: How Shared Values Can Guide Best Practices for Research Integrity, Social Justice, and Principled Education“. https://doi.org/10.31222/osf.io/k7a9p.\n\n\nElson, Malte, Ian Hussey, Taym Alsalti, und Ruben C. Arslan. 2023. „Psychological measures aren’t toothbrushes“. Communications Psychology 1 (1). https://doi.org/10.1038/s44271-023-00026-9.\n\n\nEtzel, Franka, Anna Seyffert-Müller, Felix D Schönbrodt, Lucie Kreuzer, Anne Gärtner, Paula Knischewski, und Daniel Leising. 2024. „Inter-Rater Reliability in Assessing the Methodological Quality of Research Papers in Psychology“.\n\n\nFanelli, Daniele, Matteo Schleicher, Ferric C Fang, Arturo Casadevall, und Elisabeth M Bik. 2022. „Do individual and institutional predictors of misconduct vary by country? Results of a matched-control analysis of problematic image duplications“. PLoS One 17 (3): e0255334.\n\n\nFisher, Zachary, Elizabeth Tipton, und Hou Zhipeng. 2017. „robumeta: Robust Variance Meta-Regression“. https://CRAN.R-project.org/package=robumeta.\n\n\nForster, Nick, und Daniel W Lund. 2018. „Identifying and dealing with functional psychopathic behavior in higher education“. Glob. Bus. Organ. Excel. 38 (1): 22–31.\n\n\nFranzen, Delwen L., Benjamin Gregory Carlisle, Maia Salholz-Hillel, Nico Riedel, und Daniel Strech. 2023. „Institutional dashboards on clinical trial transparency for University Medical Centers: A case study“. PLoS medicine 20 (3): e1004175. https://doi.org/10.1371/journal.pmed.1004175.\n\n\nGiner-Sorolla, Roger. 2012. „Science or Art? How Aesthetic Standards Grease the Way Through the Publication Bottleneck but Undermine Science“. Perspectives on psychological science : a journal of the Association for Psychological Science 7 (6): 562–71. https://doi.org/10.1177/1745691612457576.\n\n\nGopalakrishna, Gowri, Jelte M. Wicherts, Gerko Vink, Ineke Stoop, Olmo van den Akker, Gerben ter Riet, und Lex Bouter. 2021. „Prevalence of responsible research practices and their potential explanatory factors: a survey among academic researchers in The Netherlands“. https://doi.org/10.31222/osf.io/xsn94.\n\n\nGreenwald, Anthony G. 1976. „An editorial“. Journal of Personality and Social Psychology 33 (1): 1–7. https://doi.org/10.1037/h0078635.\n\n\nHanson, Mark A., Pablo Gómez Barreiro, Paolo Crosetto, und Dan Brockington. 2023. „The strain on scientific publishing“. https://doi.org/10.48550/arXiv.2309.15884.\n\n\nHedges, Larry V., und Jack L. Vevea. 1996. „Estimating Effect Size Under Publication Bias: Small Sample Properties and Robustness of a Random Effects Selection Model“. Journal of Educational and Behavioral Statistics 21 (4): 299–332. https://doi.org/10.3102/10769986021004299.\n\n\nHeirene, Robert, Debi LaPlante, Eric R. Louderback, Brittany Keen, Marjan Bakker, Anastasia Serafimovska, und Sally M. Gainsbury. 2021. „Preregistration specificity & adherence: A review of preregistered gambling studies & cross-disciplinary comparison“. https://doi.org/10.31234/osf.io/nj4es.\n\n\nHicks, Diana, Paul Wouters, Ludo Waltman, Sarah de Rijcke, und Ismael Rafols. 2015. „Bibliometrics: The Leiden Manifesto for research metrics“. Nature 520 (7548): 429–31. https://doi.org/10.1038/520429a.\n\n\nHoebel, Merle, Ana Durglishvili, Johanna Reinold, und Daniel Leising. 2022. „Sexual harassment and coercion in German academia: A large-scale survey study“. Sexual Offending: Theory, Research, and Prevention 17. https://doi.org/10.5964/sotrap.9349.\n\n\nJaremka, Lisa M., Joshua M. Ackerman, Bertram Gawronski, Nicholas O. Rule, Kate Sweeny, Linda R. Tropp, Molly A. Metz, Ludwin Molina, William S. Ryan, und S. Brooke Vick. 2020. „Common Academic Experiences No One Talks About: Repeated Rejection, Impostor Syndrome, and Burnout“. Perspectives on psychological science : a journal of the Association for Psychological Science 15 (3): 519–43. https://doi.org/10.1177/1745691619898848.\n\n\nKepes, Sven, Sheila K. Keener, Michael A. McDaniel, und Nathan S. Hartman. 2022. „Questionable research practices among researchers in the most research–productive management programs“. Journal of Organizational Behavior. https://doi.org/10.1002/job.2623.\n\n\nLiu, Chunli, Lie Wang, Ruiqun Qi, Weiqiu Wang, Shanshan Jia, Deshu Shang, Yangguang Shao, u. a. 2019. „Prevalence and associated factors of depression and anxiety among doctoral students: the mediating effect of mentoring relationships on the association between research self-efficacy and depression/anxiety“. Psychology research and behavior management 12: 195–208. https://doi.org/10.2147/PRBM.S195131.\n\n\nMac Giolla, Erik, Simon Karlsson, David A Neequaye, und Magnus Bergquist. 2022. „Evaluating the replicability of social priming studies“. PsyArXiv.\n\n\nMartin, G. N., und Richard M. Clarke. 2017. „Are Psychology Journals Anti-replication? A Snapshot of Editorial Practices“. Frontiers in psychology 8: 523. https://doi.org/10.3389/fpsyg.2017.00523.\n\n\nMynatt, Clifford R., Michael E. Doherty, und Ryan D. Tweney. 1977. „Confirmation bias in a simulated research environment: An experimental study of scientific inference“. Quarterly Journal of Experimental Psychology 29 (1): 85–95. https://doi.org/10.1080/00335557743000053.\n\n\nNuijten, Michele B., Marcel A. L. M. van Assen, Chris Hubertus Joseph Hartgerink, Sacha Epskamp, und Jelte M. Wicherts. 2017. „The Validity of the Tool ‚statcheck‘ in Discovering Statistical Reporting Inconsistencies“. https://doi.org/10.31234/osf.io/tcxaj.\n\n\nOviedo-Garcı́a, M. 2024. „The review mills, not just (self-) plagiarism in review reports, but a step further“. Scientometrics, 1–9.\n\n\nRahal, Rima-Maria, Susann Fiedler, Adeyemi Adetula, Ronnie P-A Berntsson, Ulrich Dirnagl, Gordon B. Feld, Christian J. Fiebach, u. a. 2023. „Quality research needs good working conditions“. Nature Human Behaviour 7 (2): 164–67. https://doi.org/10.1038/s41562-022-01508-2.\n\n\nReed, Genna, Yogi Hendlin, Anita Desikan, Taryn MacKinney, Emily Berman, und Gretchen T Goldman. 2021. „The disinformation playbook: how industry manipulates the science-policy process—and how to restore scientific integrity“. Journal of public health policy 42 (4): 622.\n\n\nRiedel, Nico, Susanne Wieschowski, Till Bruckner, Martin R. Holst, Hannes Kahrass, Edris Nury, Joerg J. Meerpohl, Maia Salholz-Hillel, und Daniel Strech. 2022. „Results dissemination from completed clinical trials conducted at German university medical centers remained delayed and incomplete. The 2014 -2017 cohort“. Journal of clinical epidemiology 144: 1–7. https://doi.org/10.1016/j.jclinepi.2021.12.012.\n\n\nRosenthal, Robert. 1979. „The file drawer problem and tolerance for null results“. Psychological Bulletin 86 (3): 638–41. https://doi.org/10.1037/0033-2909.86.3.638.\n\n\nSchimmack, Ulrich. 2020. „A meta-psychological perspective on the decade of replication failures in social psychology“. Canadian Psychology/Psychologie canadienne. https://doi.org/10.1037/cap0000246.\n\n\nSchneider, Jesper W, Nick Allum, Jens Peter Andersen, Michael Bang Petersen, Emil B Madsen, Niels Mejlgaard, und Robert Zachariae. 2024. „Is something rotten in the state of Denmark? Cross-national evidence for widespread involvement but not systematic use of questionable research practices across all fields of research“. PLoS One 19 (8): e0304342.\n\n\nSerra-Garcia, Marta, und Uri Gneezy. 2021. „Nonreplicable publications are cited more than replicable ones“. Science advances 7 (21). https://doi.org/10.1126/sciadv.abd1705.\n\n\nSimonsohn, Uri, Leif D. Nelson, und Joseph P. Simmons. 2014. „P-curve: A key to the file-drawer“. Journal of experimental psychology. General 143 (2): 534–47. https://doi.org/10.1037/a0033242.\n\n\nSingh Chawla, Dalmeet. 2024. „The citation black market: schemes selling fake references alarm scientists“. Nature, August.\n\n\nStavrova, Olga, Bennett Kleinberg, Anthony M Evans, und Milena Ivanovic. 2024. „Scientific publications that use promotional language receive more citations and social media mentions“. PsyArXiv.\n\n\nStavrova, Olga, Bennett Kleinberg, Anthony Evans, und Milena Ivanovic. o. J. „Scientific publications that use promotional language receive more citations and social media mentions“.\n\n\nSterling, T. D. 1959. „Publication decisions and their possible effects on inferences drawn from tests of significance—or vice versa“. Journal of the American Statistical Association 54 (285): 30–34. http://www.jstor.com/stable/2282137.\n\n\nThériault, Forscher, R. 2023. „The Missing Majority in Behavioral Science Dashboard. https://remi-theriault.com/dashboards/missing_majority“.\n\n\nvan Aert, Robbie Cornelis Maria, und Marcel A. L. M. van Assen. 2018. „P-uniform*“. https://doi.org/10.31222/osf.io/zqjr9.\n\n\nvan Noorden, Richard. 2023. „How big is science’s fake-paper problem?“ Nature 623 (7987): 466–67. https://doi.org/10.1038/d41586-023-03464-x.\n\n\nYeung, Andy W. K. 2017. „Do Neuroscience Journals Accept Replications? A Survey of Literature“. Frontiers in human neuroscience 11: 468. https://doi.org/10.3389/fnhum.2017.00468.\n\n\nZiemann, Mark, Yotam Eren, und Assam El-Osta. 2016. „Gene name errors are widespread in the scientific literature“. Genome biology 17: 1–3.",
    "crumbs": [
      "Probleme",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Das System</span>"
    ]
  },
  {
    "objectID": "probleme_system.html#footnotes",
    "href": "probleme_system.html#footnotes",
    "title": "Das System",
    "section": "",
    "text": "Prozess der Erlangung eines Doktorgrades/Doktortitles, je nach Disziplin und Hochschule durch das Verfassen eines Buches (Monografie) oder mehrerer Zeitschriftenartikel (kumulative Promotion)↩︎\nHier ließe sich einwenden, dass einige Zeitschriften nur Artikel veröffentlichen, in denen mehrere Studien durchgeführt wurden. Das Ziel, nämlich die internale Replikation der eigenen Befunde, verfehlen diese Zeitschriften damit deutlich. Stattdessen reizt es Forschende dazu an, mehrere Studien mit wenigen Versuchspersonen durchzuführen, statt eine Studie mit vielen Befragten.↩︎\nEthische Richtlinien im Publikationsprozess sind zum Beispiel verfügbar über das Committee on Publication Ethics (https://publicationethics.org/guidance/Guidelines).↩︎\nDamit sind Zeitschriften gemeint, die Artikel ohne Abonnement-Zugang und ohne Paywall veröffentlichen, und zwar ohne Kosten für die Autor*innen.↩︎",
    "crumbs": [
      "Probleme",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Das System</span>"
    ]
  },
  {
    "objectID": "probleme_karriere.html",
    "href": "probleme_karriere.html",
    "title": "Karriere in der Wissenschaft",
    "section": "",
    "text": "Ein Beruf in der Wissenschaft setzt üblicherweise einen einschlägigen Studienabschluss (Bachelor und Master) voraus und beginnt mit einer Promotion. Im Rahmen der Promotion wird das wissenschaftliche Handwerk erlernt und der Doktor*ingrad erlangt: Studien werden durchgeführt, Daten analysiert, und Forschungsartikel veröffentlicht. In vielen Fällen arbeiten Promovierende mit einer 50 – 66% Stelle als wissenschaftliche Mitarbeiter*innen, begutachten für ihre Vorgesetzten Artikel und Forschungsgelder-Anträge, schreiben eigene Anträge, verbringen mehrere Monate im Ausland, erstellen, beaufsichtigen, und korrigieren Klausuren, betreuen Abschlussarbeiten, und beteiligen sich an der universitären Selbstverwaltung durch die Teilnahme an Sitzungen und Mitgliedschaften bei Kommissionen. Zeitlich sind dabei bei Stellen oder Stipendien üblicherweise drei Jahre angesetzt. Am Anfang meiner Promotion sagte man mir, mit einer 40-Stunden-Woche schafft man es nicht, in der vorgesehenen Zeit zu promovieren – 50% Gehalt für über 100% Arbeitszeit und einen befristeten Vertrag also. Verträge sind dabei jedoch nicht immer auf die Zeit der Promotion befristet. Viele wissen erst wenige Monate vor Vertragsende, wie viel Prozent Gehalt sie erhalten, wie umfangreich die Lehrverpflichtung ist, und wie lange der nächste Vertrag geht. Kurz: Die Arbeitsbedingungen sind nicht optimal und man muss ein hohes Maß an Flexibilität mitbringen, wenn man sich für den Weg in die Wissenschaft entscheidet.\nDie Arbeitsbedingung in der Wissenschaft werden häufig auch als prekär bezeichnet, also heikel oder problematisch. Der Gedanke hinter dem starkem Wettbewerb ist, dass Druck die Produktivität fördert. Eingebettet in die Regeln des Systems ist jedoch auch klar, dass Druck dabei nicht zu guter Wissenschaft führt. Im (vermutlich seltenen) Extremfall begehen Forschende Betrug (z.B. durch Fälschen von Daten wie bei der Stapel Affäre oder dem Himalaya-Fossilien-Streich).\n\nDoppelabhängigkeit\nFür die Promotionsphase gibt es verschiedene Finanzierungsmöglichkeiten: Über Unternehmen lässt sich berufsbegleitend promovieren oder Stipendien zahlen über eine begrenzte Zeit Geld, das den Grunderhalt sichert (z.B. 1100€ über 36 Monate bei der Graduiertenförderung des Landes Sachsen-Anhalt, dazu kommen noch zusätzliche Kosten für die Sozialversicherung). Der häufigste Weg ist jedoch über eine Stelle als wissenschaftliche*r Mitarbeiter*in. Dabei ist die vorgesetzte Person diejenige Person, die auch die Promotionsarbeit bewertet. Die einem auferlegte Korrektur der 120 Erstsemester-Klausuren steht dann im Extremfall in Konflikt mit der Zeit, die für die Arbeit an der wissenschaftlichen Studie benötigt wird. Promovierende hängen also meistens von den betreuenden Professor*innen in Form der Beschäftigung und über die Bewertung ihrer Arbeit ab, was also als Doppelabhängigkeit bezeichnet wird.\n\n\n\n\n\n\nFamilienfeindliche Stipendien\n\n\n\nUnbefristete Stellen, Doppelabhängigkeit oder geringfügige Stipendien sowie Leistungsdruck haben klare negative Auswirkungen auf die Gesundheit der Forschenden und damit auf die Qualität der Wissenschaft. Wie sieht es mit Promotionsstipendien aus? Stipendien laufen oft 1 Jahr mit Option auf Verlängerung oder bis zu 3 Jahre. Forschende, die direkt nach der Schule und dem Studium promovieren, sind ca. 24 Jahre alt, haben eventuell einen Partner, und wünschen sich eine Familie. Im Rahmen ungewisser Vertragsverhältnisse ist das schwierig. Stipendien haben oft keine Möglichkeit zur Elternzeit und müssen dann pausiert werden – oder sie laufen weiter und es wird keine zusätzliche Zeit angehängt. Sie verbieten zusätzliches Einkommen, weshalb das Elterngeld dann auf den dafür angesetzten Mindestbetrag fällt. Zusätzlich schreiben sie vor, dass die geförderte Person inklusive Ehepartner*in ein bestimmtes Maximalvermögen nicht überschreiten darf, sodass auch keine Finanzierung durch mögliche Rücklagen klappt.\n\n\n\n\nDepressionen, Burnout, und #IchbinHanna\nFächerübergreifend hat sich als Antwort auf ein inzwischen gelöschtes Erklärvideo vom Bundesministerium für Bildung und Forschung zum Wissenschaftszeitvertragsgesetz (WissZeitVG) eine Bewegung unter dem Hashtag #IchbinHanna entwickelt, die die dortige sachliche Erklärung (https://www.youtube.com/watch?v=PIq5GlY4h4E) und die Arbeitsbedingungen in der Wissenschaft stark kritisiert. Es heißt „damit auch nachrückende Wissenschaftlerinnen und Wissenschaftler die Chance auf den Erwerb dieser Qualifizierung haben und nicht eine Generation alle Stellen verstopft, dürfen Hochschulen und Forschungseinrichtungen befristete Verträge nach den besonderen Regeln des WissZeitVG abschließen. So kommt es zur Fluktuation und die fördert die Innovationskraft”. 90% aller Wissenschaftler*innen sind unbefristet angestellt (https://www.youtube.com/watch?v=H1wJmqpGhJc).\nPlanungsunsicherheit und massiver Konkurrenzdruck fördern die Innovationskraft? Das scheint unwahrscheinlich: Unter Forschenden mentale Erkrankungen wie Burnout (Jaremka u. a. 2020) und Anzeichen für Angststörungen und Depression stark verbreitet. Einer Überblicksstudie von Satinsky u. a. (2021) zufolge, leiden zwischen 18 und 31% der Promovierenden unter Depressionen.\n\n\n\n\n\n\nWer hilft?\n\n\n\nSchnelle Hilfe bei psychischen Problemen leistet beispielsweise das Krisentelefon der TelefonSeelsorge. Manche Universitäten haben spezifische Anlaufstellen, Städte haben häufig örtliche Psychotherapie-Ambulanzen und Beratungsstellen.\n\n\n\n\nTop-Down-Wandel\nDiejenigen, die das System ändern können, also alle mit unbefristeten Verträgen, leiden nicht mehr unter ihm. Für diejenigen, die unter dem System leiden, ist es unklug, das System ändern zu wollen und sich an die Professor*innen zu wenden, denn das sind die Leute, die über ihren späteren Verbleib in der Wissenschaft im Rahmen von Berufungskommissionen bei der Entscheidung der Vergabe von Professuren entscheiden. Im Extremfall kann das dazu führen, dass jemand Kritik an Arbeiten von Wissenschaftler*innen in höheren Positionen aus Angst, die eigenen Chancen auf eine Professur zu schmälern, zurückfährt. Auf der anderen Seite haben Professor*innen bewiesen, dass sie sehr gut nach den Spielregeln spielen können. Zuzugeben, dass sie eine der seltenen und heiß begehrten Stellen nicht über Qualität sondern Quantität ihrer Forschung erhalten haben, hieße, sich selbst abzuwerten.\n\n\nWeiterführende Informationen\n\nDer Verein Respect Science e.V. setzt sich für die Verbesserung der Arbeitsbedingungen von Wissenschaftler*innen ein.\nTED Talk über Publikationskultur und Open Science: https://www.youtube.com/watch?v=c-bemNZ-IqA\n\n\n\nLiteratur\n\n\n\n\nJaremka, Lisa M., Joshua M. Ackerman, Bertram Gawronski, Nicholas O. Rule, Kate Sweeny, Linda R. Tropp, Molly A. Metz, Ludwin Molina, William S. Ryan, und S. Brooke Vick. 2020. „Common Academic Experiences No One Talks About: Repeated Rejection, Impostor Syndrome, and Burnout“. Perspectives on psychological science : a journal of the Association for Psychological Science 15 (3): 519–43. https://doi.org/10.1177/1745691619898848.\n\n\nSatinsky, Emily N., Tomoki Kimura, Mathew V. Kiang, Rediet Abebe, Scott Cunningham, Hedwig Lee, Xiaofei Lin, u. a. 2021. „Systematic review and meta-analysis of depression, anxiety, and suicidal ideation among Ph.D. students“. Scientific reports 11 (1): 14370. https://doi.org/10.1038/s41598-021-93687-7.",
    "crumbs": [
      "Probleme",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Karriere in der Wissenschaft</span>"
    ]
  },
  {
    "objectID": "probleme_methoden.html",
    "href": "probleme_methoden.html",
    "title": "Methoden",
    "section": "",
    "text": "Exploratorische versus konfirmatorische Forschung\nZum Verständnis der fragwürdigen Forschungspraktiken (QRP) ist eine wichtige forschungstechnische Unterscheidung unabdingbar: Wie bei einem Spaziergang kann eine wissenschaftliche Untersuchung erkundend oder zielgerichtet sein. Mal wird frei durch die Gegend spaziert und dabei neue Entdeckungen gemacht, mal ist das Ziel und die Route klar und im Vorhinein bestimmt. Im wissenschaftlichen Kontext ist die Rede von exploratorischer und konfirmatorischer Forschung. Bei der Exploration stehen höchstens die Forschungsfrage und grobe Züge der Methode fest, bei einem konfirmatorischen Test ist alles durchdacht: Vorgehen, mögliche Ergebnisse, sowie Erklärungsansätze für jedes mögliche Resultat. Es wird dann eine spezifizierte Hypothese mit der dazugehörigen Theorie bestätigt oder eben nicht. Kein Vorgehen ist dem anderen überlegen. Üblicherweise beginnen Untersuchungen in einem bisher wenig erschlossenen Gebiet mit Exploration, während mehr vorhergehende Forschung mit klareren Erwartungen einhergeht. Es sei dazu gesagt, dass es sich hier um Extremtypen von Forschung handelt, die ein Spektrum bilden. Es erlauben außerdem erst beide Ansätze zusammen Erkenntnisgewinn. Im Rahmen des hermeneutischen Zirkel (einfach gesagt “dem Kreis des Verstehens”) wird aus einzelnen Beobachtungen eine allgemeine Gesetzmäßigkeit formuliert (Induktion) und diese Gesetzmäßigkeit wird im Anschluss bei weiteren Einzelbeobachtungen geprüft (Deduktion). Die Deduktion kann je nach Gesetzmäßigkeit logisch notwendig sein, denn es wird von zuvor bestimmten Aussagen auf eine weitere Aussage geschlossen. Die Vorannahmen heißen Prämissen und aus ihnen folgt die Konklusion. Wenn beide Prämissen korrekt sind, muss also auch die Schlussfolgerung korrekt sein. Die Induktion ist hingegen keine Notwendigkeit [siehe Induktionsproblem; Hume (1748/2011)].\nProblematisch wird es, wenn exploratorische Forschung als konfirmatorische kommuniziert wird, also so getan wird, als hätte eine Einzelbeobachtung eine bereits formulierte Gesetzmäßigkeit bestätigt, statt sie bloß inspiriert. Diese Art Unlogik heißt Zirkelschluss: Die Gesetzmäßigkeit gilt wegen der Beobachtung. Und die Beobachtung entspricht der Gesetzmäßigkeit.",
    "crumbs": [
      "Probleme",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Methoden</span>"
    ]
  },
  {
    "objectID": "probleme_methoden.html#weiterführende-informationen",
    "href": "probleme_methoden.html#weiterführende-informationen",
    "title": "Methoden",
    "section": "Weiterführende Informationen",
    "text": "Weiterführende Informationen\n\nBei diesem englischsprachigen Podcast wird diskutiert, wie und ob sich Betrug in der Wissenschaft stoppen lässt: https://freakonomics.com/podcast/can-academic-fraud-be-stopped/\nWie sich die Replikationskrise aus Perspektive eines jungen Forschers angefühlt hat beschreibt Daniel Lakens: http://daniellakens.blogspot.com/2020/11/why-i-care-about-replication-studies.html\nNagy u. a. (2024) listen fragwürdige Forschungspraktiken (QRP) systematisch in ihrem Bestarium auf.\n\n\nLiteratur\n\n\n\n\nAlbert, Hans. 2010. Traktat über kritische Vernunft. Nachdr. d. 5., verb. und erw. Aufl. Bd. 1609. UTB. Tübingen: Mohr Siebeck.\n\n\nBik, Elisabeth M., Arturo Casadevall, und Ferric C. Fang. 2016. „The Prevalence of Inappropriate Image Duplication in Biomedical Research Publications“. mBio 7 (3). https://doi.org/10.1128/mbio.00809-16.\n\n\nBreznau, Nate, Eike Mark Rinke, Alexander Wuttke, Hung H. V. Nguyen, Muna Adem, Jule Adriaans, Amalia Alvarez-Benjumea, u. a. 2022. „Observing many researchers using the same data and hypothesis reveals a hidden universe of uncertainty“. Proceedings of the National Academy of Sciences of the United States of America 119 (44): e2203150119. https://doi.org/10.1073/pnas.2203150119.\n\n\nCoretta, Stefano, Joseph V. Casillas, Simon Roessig, Michael Franke, Byron Ahn, Ali H. Al-Hoorie, Jalal Al-Tamimi, u. a. 2023. „Multidimensional Signals and Analytic Flexibility: Estimating Degrees of Freedom in Human-Speech Analyses“. Advances in Methods and Practices in Psychological Science 6 (3). https://doi.org/10.1177/25152459231162567.\n\n\nFanelli, Daniele. 2009. „How many scientists fabricate and falsify research? A systematic review and meta-analysis of survey data“. PloS one 4 (5): e5738. https://doi.org/10.1371/journal.pone.0005738.\n\n\nFeyerabend, Paul K. 1975/2002. Against method. Reprinted der 3. ed. 1993. London: Verso.\n\n\nFleck, Ludwik. 1935/2015. Entstehung und Entwicklung einer wissenschaftlichen Tatsache [Formation and development of a scientific fact]: Einführung in die Lehre vom Denkstil und Denkkollektiv [Introduction to thinking style and thinking collective]. 10. Auflage. Bd. 312. Suhrkamp-Taschenbuch Wissenschaft. Frankfurt am Main: Suhrkamp.\n\n\nGopalakrishna, Gowri, Jelte M. Wicherts, Gerko Vink, Ineke Stoop, Olmo van den Akker, Gerben ter Riet, und Lex Bouter. 2021. „Prevalence of responsible research practices and their potential explanatory factors: a survey among academic researchers in The Netherlands“. https://doi.org/10.31222/osf.io/xsn94.\n\n\nGould, Elliot, Hannah Fraser, Timothy Parker, Shinichi Nakagawa, Simon Griffith, Peter Vesk, Fiona Fidler, u. a. 2023. „Same data, different analysts: variation in effect sizes due to analytical decisions in ecology and evolutionary biology“. https://doi.org/10.32942/X2GG62.\n\n\nHoyningen-Huene, Paul. 2013. Systematicity: The nature of science. Oxford studies in philosophy of science. Oxford: Oxford Univ. Press.\n\n\nHume, David. 1748/2011. Eine Untersuchung über den menschlichen Verstand [An Enquiry Concerning Human Understanding]. Bd. 5489. Reclams Universal-Bibliothek. Stuttgart: Reclam.\n\n\nHussey, Ian, und Sean Hughes. 2018. Hidden invalidity among fifteen commonly used measures in social and personality psychology. https://doi.org/10.31234/osf.io/7rbfp.\n\n\nJohn, Leslie K., George Loewenstein, und Drazen Prelec. 2012. „Measuring the prevalence of questionable research practices with incentives for truth telling“. Psychological Science 23 (5): 524–32. https://doi.org/10.1177/0956797611430953.\n\n\nKuhn, Thomas S. 1970/1996. The Structure of Scientific Revolutions. 3. Aufl. Chicago: Univ. of Chicago Press.\n\n\nLopez-Nicolas, Ruben, Daniel Lakens, José A López-López, Marı́a Rubio Aparicio, Alejandro Sandoval-Lentisco, Carmen López-Ibáñez, Desirée Blázquez-Rincón, und Julio Sánchez-Meca. 2022. „Analytical reproducibility and data reusability of published meta-analyses on clinical psychological interventions“. PsyArXiv.\n\n\nMynatt, Clifford R., Michael E. Doherty, und Ryan D. Tweney. 1977. „Confirmation bias in a simulated research environment: An experimental study of scientific inference“. Quarterly Journal of Experimental Psychology 29 (1): 85–95. https://doi.org/10.1080/00335557743000053.\n\n\nNaddaf, Miryam. 2023. „ChatGPT generates fake data set to support scientific hypothesis“. Nature 623 (7989): 895–96. https://doi.org/10.1038/d41586-023-03635-w.\n\n\nNagy, Tamás, Jane Hergert, Mahmoud M Elsherif, Lukas Wallrich, Kathleen Schmidt, Tal Waltzer, Jason W Payne, u. a. 2024. „Bestiary of Questionable Research Practices in Psychology“.\n\n\nNeoh, Michelle Jin Yee, Alessandro Carollo, Albert Lee, und Gianluca Esposito. 2023. „Fifty years of research on questionable research practises in science: quantitative analysis of co-citation patterns“. Royal Society Open Science 10 (10): 230677. https://doi.org/10.1098/rsos.230677.\n\n\nNickerson, Raymond S. 1998. „Confirmation bias: A ubiquitous phenomenon in many guises“. Review of General Psychology 2 (2): 175–220. https://doi.org/10.1037/1089-2680.2.2.175.\n\n\nNuijten, Michèle B, Chris HJ Hartgerink, Marcel ALM Van Assen, Sacha Epskamp, und Jelte M Wicherts. 2016. „The prevalence of statistical reporting errors in psychology (1985–2013)“. Behavior research methods 48: 1205–26.\n\n\nOswald, und Grosjean. 2004. „Oswald, M. E., & Grosjean, S. (2004). Confirmation bias. In R. F. Pohl (Ed.). Cognitive Illusions. A Handbook on Fallacies and Biases in Thinking, Judgement and Memory. Hove and N.Y. Psychology Press“. Unpublished. https://doi.org/10.13140/2.1.2068.0641.\n\n\nRobinaugh, Donald J., Jonas M. B. Haslbeck, Oisín Ryan, Eiko I. Fried, und Lourens J. Waldorp. 2021. „Invisible Hands and Fine Calipers: A Call to Use Formal Theory as a Toolkit for Theory Construction“. Perspectives on psychological science : a journal of the Association for Psychological Science 16 (4): 725–43. https://doi.org/10.1177/1745691620974697.\n\n\nRöseler, Lukas, und Astrid Schütz. 2022. „Open Science“. In Psychologie, herausgegeben von Astrid Schütz, Matthias Brand, Sabine Steins-Loeber, Martin Baumann, Jan Born, Veronika Brandstätter, Claus-Christian Carbon, u. a., 187–98. Stuttgart: Kohlhammer.\n\n\nSarstedt, Marko, und Susanne J Adler. 2023. „An advanced method to streamline p-hacking“. Journal of Business Research 163: 113942.\n\n\nSarstedt, Marko, Susanne J Adler, Christian M Ringle, Gyeongcheol Cho, Adamantios Diamantopoulos, Heungsun Hwang, und Benjamin D Liengaard. 2024. „Same model, same data, but different outcomes: Evaluating the impact of method choices in structural equation modeling“. J. Prod. Innov. Manage., April.\n\n\nSchimmack, Ulrich. 2019. „The Implicit Association Test: A Method in Search of a Construct“. Perspectives on psychological science : a journal of the Association for Psychological Science, 1745691619863798. https://doi.org/10.1177/1745691619863798.\n\n\nSchönbrodt, Felix. 2016. „p-hacker: Train your p-hacking skills!“ http://shinyapps.org/apps/p-hacker/.\n\n\nSimmons, Joseph P., Leif D. Nelson, und Uri Simonsohn. 2011. „False-positive psychology: undisclosed flexibility in data collection and analysis allows presenting anything as significant“. Psychological Science 22 (11): 1359–66. https://doi.org/10.1177/0956797611417632.\n\n\nSmith, Neal, Jr, und Aaron Cumberledge. 2020. „Quotation errors in general science journals“. Proc. Math. Phys. Eng. Sci. 476 (2242): 20200538.\n\n\nStroebe, Wolfgang, Tom Postmes, und Russell Spears. 2012. „Scientific Misconduct and the Myth of Self-Correction in Science“. Perspectives on psychological science : a journal of the Association for Psychological Science 7 (6): 670–88. https://doi.org/10.1177/1745691612460687.\n\n\nWicherts, Jelte M, Coosje L S Veldkamp, Hilde E M Augusteijn, Marjan Bakker, Robbie C M van Aert, und Marcel A L M van Assen. 2016. „Degrees of freedom in planning, running, analyzing, and reporting psychological studies: A checklist to avoid p-hacking“. Front. Psychol. 7 (November): 1832.\n\n\nYu, Erica C., Amber M. Sprenger, Rick P. Thomas, und Michael R. Dougherty. 2014. „When decision heuristics and science collide“. Psychonomic Bulletin & Review 21 (2): 268–82. https://doi.org/10.3758/s13423-013-0495-z.",
    "crumbs": [
      "Probleme",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Methoden</span>"
    ]
  },
  {
    "objectID": "probleme_methoden.html#footnotes",
    "href": "probleme_methoden.html#footnotes",
    "title": "Methoden",
    "section": "",
    "text": "In der Teilchenphysik wird ebenfalls mit statistischen Modellen gearbeitet. Dabei wird statt dem 5% Signifikanzniveau (1,96 Sigma) das 5-Sigma-Kriterium verwendet, was einem Alpha-Niveau von 0.000000573% entspricht.↩︎\nDie Logik hinter dem Begriff ist vergleichbar mit einer Formel mit mehreren Variablen und der Frage: “Die Werte wie vieler Variablen muss ich kennen, um die übrigen auszurechnen?”↩︎",
    "crumbs": [
      "Probleme",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Methoden</span>"
    ]
  },
  {
    "objectID": "probleme_theorien.html",
    "href": "probleme_theorien.html",
    "title": "Theorien",
    "section": "",
    "text": "Deduktion und Induktion\nMethoden werden reformiert und Wissenschaftler*innen diskutieren, wie Wissenschaft funktioniert, ablaufen sollte, und welche Methoden sinnvoll und unsinnig sind. Wie am hermeneutischen Zirkel klar wird, führt ein Erkenntnisweg darüber, eine Menge von Beobachtungen zu einer Regelmäßigkeit oder Gesetzmäßigkeit zusammenzufassen (Induktion) und ein weiterer besteht daraus, aus einer Gesetzmäßigkeit bzw. Theorie Vorhersagen über noch nicht angestellte Beobachtungen zu machen (Deduktion). Immer wieder wird diese Unterscheidung im wissenschaftlichen Diskurs vernachlässigt oder ausgeblendet. Beispielsweise drehte sich ein Dialog in der Konsumentenpsychologie jahrelang darum, welcher Weg besser sei, obwohl beide Wege gleichermaßen legitim sind und einander ergänzen (Calder, Phillips, und Tybout 1981). Ähnlich verhält es sich bei Konflikten zwischen qualitativer und quantitativer Vorgehensweise, die formal betrachtet jeweils eher induktiv oder deduktiv vorgehen (Borgstede und Scholz 2021). Bei Replikationsforschung hat traditionell die induktive Seite mehr Beachtung erfahren (Hüffmeier, Mazei, und Schultze 2016; Yamashita und Neiriz 2024): Jeder Unterschied zwischen Replikation- und Originalstudie wird als mögliche Ursache für ein Scheitern des Replikationsversuches herangezogen, um die Vertrauenswürdigkeit der Originalbefunde aufrechtzuerhalten (Baumeister und Vohs 2016). Dabei gerät außer Acht, dass kleinere Unterschiede zwischen Original- und Replikationsstudie (z.B. Verwendung der Maße, durchschnittliches Alter der Versuchspersonen, Sprache der Instruktion) von Theorien nicht erfasst werden – ihnen zufolge also unerheblich sein sollten – und eine fehlgeschlagene Replikation klar die Grenzen der Theorie aufzeigt und sich aus ihr Empfehlungen für die Modifikation von Theorien ableiten lassen (Cesario 2014; Dijksterhuis 2014). Ein Überblick über die Vorgehensweisen ist in der folgenden Tabelle zu finden.\nMerkmale induktiver und deduktiver Vorgehensweise; entnommen, übersetzt, und angepasst aus einem unveröffentlichten Manuskript von Röseler & Leder",
    "crumbs": [
      "Probleme",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Theorien</span>"
    ]
  },
  {
    "objectID": "probleme_theorien.html#weiterführende-informationen",
    "href": "probleme_theorien.html#weiterführende-informationen",
    "title": "Theorien",
    "section": "Weiterführende Informationen",
    "text": "Weiterführende Informationen\n\nEine philosophie Perspektive auf den Zusammenhang zwischen Theorie, Messungen, und Replikationen diskutiert @ramminger2023vermessen\nYarkoni (2019) argumentiert, dass die Replikationsprobleme in der Verallgemeinerung von Ergebnissen zu Theorien ihren Ursprung haben.\nFanelli diskutiert in einem Vortrag die Komplexität von Forschung als Grund für Replikationsfehlschläge und schlägt eine Theorie zur Messung von Komplexität vor (Fanelli u. a. 2022), Ein Video zu einem Vortrag ist online verfügbar: https://www.youtube.com/watch?v=CEAV7420jBk\n\n\nLiteratur\n\n\n\n\nBanker, Sachin, Sarah E. Ainsworth, Roy F. Baumeister, Dan Ariely, und Kathleen D. Vohs. 2017. „The Sticky Anchor Hypothesis: Ego Depletion Increases Susceptibility to Situational Cues“. Journal of Behavioral Decision Making 87 (1): 23. https://doi.org/10.1002/bdm.2022.\n\n\nBaumeister, Roy F., und Kathleen D. Vohs. 2016. „Misguided Effort With Elusive Implications“. Perspectives on psychological science : a journal of the Association for Psychological Science 11 (4): 574–75. https://doi.org/10.1177/1745691616652878.\n\n\nBorgstede, Matthias, und Marcel Scholz. 2021. „Quantitative and Qualitative Approaches to Generalization and Replication-A Representationalist View“. Frontiers in psychology 12: 605191. https://doi.org/10.3389/fpsyg.2021.605191.\n\n\nBuzbas, Erkan O, und Berna Devezer. 2023. „Tension between theory and practice of replication“. Journal of Trial & Error 4 (1).\n\n\nCalder, Bobby J., Lynn W. Phillips, und Alice M. Tybout. 1981. „Designing Research for Application“. Journal of Consumer Research 8 (2): 197. https://doi.org/10.1086/208856.\n\n\nCesario, Joseph. 2014. „Priming, Replication, and the Hardest Science“. Perspectives on psychological science : a journal of the Association for Psychological Science 9 (1): 40–48. https://doi.org/10.1177/1745691613513470.\n\n\nDijksterhuis, Ap. 2014. „Welcome Back Theory!“ Perspectives on psychological science : a journal of the Association for Psychological Science 9 (1): 72–75. https://doi.org/10.1177/1745691613513472.\n\n\nFanelli, Daniele, Pedro Batista Tan, Olavo Bohrer Amaral, und Kleber Neves. 2022. „A metric of knowledge as information compression reflects reproducibility predictions in biomedical experiments“.\n\n\nFerguson, Christopher J., und Moritz Heene. 2012. „A Vast Graveyard of Undead Theories: Publication Bias and Psychological Science’s Aversion to the Null“. Perspectives on psychological science : a journal of the Association for Psychological Science 7 (6): 555–61. https://doi.org/10.1177/1745691612459059.\n\n\nFrancis, Zoë, Marina Milyavskaya, Hause Lin, und Michael Inzlicht. 2018. „Development of a Within-Subject, Repeated-Measures Ego-Depletion Paradigm“. Social Psychology 49 (5): 271–86. https://doi.org/10.1027/1864-9335/a000348.\n\n\nGlöckner, Andreas, und Tilmann Betsch. 2011. „The empirical content of theories in judgment and decision making: Shortcomings and remedies“. Judgment and Decision Making 6 (8): 711–21.\n\n\nHagger, Martin S., Nikos L. D. Chatzisarantis, Hugo Alberts, Calvin Octavianus Anggono, Cédric Batailler, Angela R. Birt, Ralf Brand, u. a. 2016. „A Multilab Preregistered Replication of the Ego-Depletion Effect“. Perspectives on psychological science : a journal of the Association for Psychological Science 11 (4): 546–73. https://doi.org/10.1177/1745691616652873.\n\n\nHoyningen-Huene, Paul, und Harold Kincaid. 2023. „What makes economics special: orientational paradigms“. J. Econ. Methodol., März, 1–15.\n\n\nHüffmeier, Joachim, Jens Mazei, und Thomas Schultze. 2016. „Reconceptualizing replication as a sequence of different studies: A replication typology“. Journal of Experimental Social Psychology 66: 81–92. https://doi.org/10.1016/j.jesp.2015.09.009.\n\n\nLakens, Daniel. 2023. „Concerns about Replicability, Theorizing, Applicability, Generalizability, and Methodology across Two Crises in Social Psychology“. https://doi.org/10.31234/osf.io/dtvs7.\n\n\nMuthukrishna, Michael, und Joseph Henrich. 2019. „A problem in theory“. Nature Human Behaviour 349 (Suppl 1): aac4716. https://doi.org/10.1038/s41562-018-0522-1.\n\n\nPlatt, John R. 1964. „Strong Inference: Certain systematic methods of scientific thinking may produce much more rapid progress than others.“ science 146 (3642): 347–53.\n\n\nPopper, Karl R. 1959/2008. The Logic of Scientific Discovery. Repr. 2008. Routledge classics. London: Routledge Classics; Routledge.\n\n\nRobinaugh, Donald J., Jonas M. B. Haslbeck, Oisín Ryan, Eiko I. Fried, und Lourens J. Waldorp. 2021. „Invisible Hands and Fine Calipers: A Call to Use Formal Theory as a Toolkit for Theory Construction“. Perspectives on psychological science : a journal of the Association for Psychological Science 16 (4): 725–43. https://doi.org/10.1177/1745691620974697.\n\n\nRöseler, Lukas, Astrid Schütz, Roy F. Baumeister, und Ulrike Starker. 2020. „Does ego depletion reduce judgment adjustment for both internally and externally generated anchors?“ Journal of Experimental Social Psychology 87: 103942. https://doi.org/10.1016/j.jesp.2019.103942.\n\n\nSmaldino, Paul. 2019. „Better methods can’t make up for mediocre theory“. Nature 575 (7781): 9. https://doi.org/10.1038/d41586-019-03350-5.\n\n\nSmaldino, Paul E. 2017. „Models Are Stupid, and We Need More of Them“. In Computational Social Psychology, herausgegeben von Robin R. Vallacher, 311–31. New York : Routledge, 2017. | Series: Frontiers of social psychology: Routledge. https://doi.org/10.4324/9781315173726-14.\n\n\nSmedslund, Jan. 2015. „Why psychology cannot be an empirical science“. Integrative psychological & behavioral science. https://doi.org/10.1007/s12124-015-9339-x.\n\n\nVohs, Kathleen D, Brandon J Schmeichel, Sophie Lohmann, Quentin F Gronau, Anna J Finley, Sarah E Ainsworth, Jessica L Alquist, u. a. 2021. „A multisite preregistered paradigmatic test of the ego-depletion effect“. Psychological Science 32 (10): 1566–81.\n\n\nYamashita, Taichi, und Reza Neiriz. 2024. „Why replicate? Systematic review of calls for replication in Language Teaching“. Research Methods in Applied Linguistics 3 (1): 100091.\n\n\nYarkoni, Tal. 2019. „The Generalizability Crisis“. https://doi.org/10.31234/osf.io/jqw35.",
    "crumbs": [
      "Probleme",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Theorien</span>"
    ]
  },
  {
    "objectID": "probleme_epistemische.html",
    "href": "probleme_epistemische.html",
    "title": "Epistemische Probleme",
    "section": "",
    "text": "Robustheit und Historizität von Phänomenen\nUnter welchen Voraussetzungen ist es wenig überraschend, dass Replikationsversuche fehlschlagen? Eine Möglichkeit ist anzunehmen, dass die untersuchten Phänomene extrem empfindlich oder instabil seien. Regelmäßigkeiten im menschlichen Verhalten analog zu den Planetenbewegungen zu entdecken, könnte schlichtweg nicht möglich sein (Smedslund 2015). Die aktuelle Annahme vieler Sozialwissenschaftler ist die Existenz von Regelmäßigkeiten, die nicht jede Person ausnahmslos betreffen aber „im Schnitt” gelten. Lewin (1930) unterscheidet diesbezüglich in aristotelische und galileische Gesetzmäßigkeiten, wobei die aristotelischen in den Sozialwissenschaften vorwiegend untersucht werden. Eine Regelmäßigkeit bzw. ein Naturgesetz im aristotelischen Sinne kann zum Beispiel sein, dass Männer größer als Frauen sind.\nNoch extremer ist die Theorie, dass Menschen sich des Wissens über sie bewusst sind und ihr Verhalten dynamisch anpassen und Verhaltenswissenschaften immer historisch bzw. zeitgebunden sind: Wird herausgefunden, dass Menschen in ihren Entscheidungen tendenziell dazu neigen, nichts zu ändern, auch wenn sich dadurch ihre Situation verbessern würde, wird ihnen diese Tatsache über die Wissenschaft vor Augen geführt und sie können ihr Verhalten anpassen. Dabei handelt es sich übrigens um den Status Quo Bias, bei dem Menschen die jetzige Situation einer anderen vorziehen (Samuelson und Zeckhauser 1988; Xiao u. a. 2021). Ein anderes Beispiel ist die Gender-Pay-Gap, also Gehaltsunterschiede zwischen Männern und Frauen unabhängig von der Qualifikation. Im Idealfall ändern Menschen das System, sobald sie von dem Problem wissen, dahingehend, dass die Gehaltsunterschiede nicht mehr existieren. Auf alle Bereiche der Sozialwissenschaften lässt sich diese Perspektive nicht übertragen. Beispielsweise hat das Wissen über die eigene Schmerzempfindlichkeit, Intelligenz, oder Persönlichkeit keinen Einfluss auf dieselbe.\nWie stark sich Phänomene durch vermeintlich kleinere Unterschiede im Versuchsaufbau unterscheiden, wurde bereits meta-wissenschaftlich untersucht. Landy u. a. (2020) ließen mehrere Hypothesen von mehreren Forschenden prüfen und Faktoren, die laut den dahinterliegenden Theorien eigentlich keinen Unterschied machen sollten, führten dazu, dass Gegenteilige Ergebnisse entstanden. Auf Replikationsforschung übertragen ist es also möglich, dass in bestimmten Forschungsbereichen völlig unklar ist, unter welchen Bedingungen welche Zusammenhänge zu beobachten sind.",
    "crumbs": [
      "Probleme",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Epistemische Probleme</span>"
    ]
  },
  {
    "objectID": "probleme_epistemische.html#weiterführende-informationen",
    "href": "probleme_epistemische.html#weiterführende-informationen",
    "title": "Epistemische Probleme",
    "section": "Weiterführende Informationen",
    "text": "Weiterführende Informationen\n\nFleck (1935/2015) schlägt eine Theorie des wissenschaftlichen Fortschritts vor, bei der Wissen immer einem sogenannten Denkstil unterliegt, der für die jeweilige gesellschaftliche Situation optimal ist. Dabei bedient er sich Elementen der Evolutionstheorie, Soziologie, und Psychologie.\nShiffrin, Börner, und Stigler (2018) diskutiert das scheinbare Paradox zwischen Fallibilität, also der Tatsache, dass Wissenschaft nicht immer richtig liegt, und wissenschaftlichem Fortschritt.\n\n\nLiteratur\n\n\n\n\nFleck, Ludwik. 1935/2015. Entstehung und Entwicklung einer wissenschaftlichen Tatsache [Formation and development of a scientific fact]: Einführung in die Lehre vom Denkstil und Denkkollektiv [Introduction to thinking style and thinking collective]. 10. Auflage. Bd. 312. Suhrkamp-Taschenbuch Wissenschaft. Frankfurt am Main: Suhrkamp.\n\n\nLandy, Justin F., Miaolei Liam Jia, Isabel L. Ding, Domenico Viganola, Warren Tierney, Anna Dreber, Magnus Johannesson, u. a. 2020. „Crowdsourcing hypothesis tests: Making transparent how design choices shape research results“. Psychological Bulletin 146 (5): 451–79. https://doi.org/10.1037/bul0000220.\n\n\nLewin, Kurt. 1930. „Der übergang von der aristotelischen zur galileischen Denkweise in Biologie und Psychologie“. Erkenntnis 1 (1): 421–66. https://doi.org/10.1007/BF00208633.\n\n\nSamuelson, William, und Richard Zeckhauser. 1988. „Status quo bias in decision making“. Journal of Risk and Uncertainty 1 (1): 7–59. https://doi.org/10.1007/BF00055564.\n\n\nShiffrin, Richard M, Katy Börner, und Stephen M Stigler. 2018. „Scientific progress despite irreproducibility: A seeming paradox“. Proceedings of the National Academy of Sciences 115 (11): 2632–39.\n\n\nSmedslund, Jan. 2015. „Why psychology cannot be an empirical science“. Integrative psychological & behavioral science. https://doi.org/10.1007/s12124-015-9339-x.\n\n\nXiao, Qinyu, Choi Shan Lam, Muhrajan Piara, und Gilad Feldman. 2021. „Revisiting status quo bias“. Meta-Psychology 5. https://doi.org/10.15626/MP.2020.2470.",
    "crumbs": [
      "Probleme",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Epistemische Probleme</span>"
    ]
  },
  {
    "objectID": "lösungen_system.html",
    "href": "lösungen_system.html",
    "title": "Das System",
    "section": "",
    "text": "Ansätze, die darauf abzielen, das System zu verändern, bergen am meisten Potenzial, denn um im System Geld verdienen zu können, müssen Forschende sich an die Regeln halten. Und solange Publikationen die Währung sind und Paper mit knackigen Titeln und eindeutigen Ergebnissen als qualitativ hochwertiger befunden werden, sind Forschende darin motiviert, nach knackigen Titeln und eindeutigen Ergebnissen und nicht nach der Wahrheit zu suchen.\nInsgesamt ist eine positive Entwicklung sichtbar (Korbmacher u. a. 2023) und eine Veränderung der Anreizstruktur wird anvisiert. Sie lässt sich als Angleichung des wissenschaftlichen Systems an die Mertonschen Normen (nach Robert Merton) auffassen (Merton 1973): (1) Kommunismus: Das wissenschaftliche Wissen sollte allen Wissenschaftler*innen gleichermaßen gehören, um die Zusammenarbeit zu fördern. (2) Universalismus: Wissenschaftliche Güte ist unabhängig vom soziopolitischen Status und persönlichen Attributen der Teilhabenden. (3) Desinteresse: Wissenschaftliche Institutionen handeln im Interesse der Wissenschaft und nicht für persönlichen Gewinn. (4) Organisierter Skeptizismus: Wissenschaftliche Behauptungen sollten einer kritischen Prüfung unterzogen werden bevor sie akzeptiert werden.\nNosek empfiehlt in einem Blogpost eine Maßnahmenstruktur, nach welcher die gewünschten Veränderung nacheinander …\n\nmöglich (z.B. durch Infrastruktur wie online Repositorien, in denen Forschungsmaterialien öffentlich und gratis hochgeladen werden können),\neinfach (z.B. durch barrierearme Angebote, mehrsprachige Anleitungen),\nnormativ (z.B. durch Wissenschaftliche Communities, die gemeinsam hinter Forderungen der Verbesserung stehen),\nbelohnend (z.B. durch designierte Preise), und\nnotwendig (z.B. durch Mindeststandards, die von Zeitschriften oder Drittmittelgebern gefordert werden)\n\ngemacht werden sollen. Wie die verschiedenen Ansätze bei den verschiedenen Akteuren, also Politik, Universitäten, oder Zeitschriften konkret aussehen, wird im Folgenden diskutiert.\n\n\n\nKulturwandel in der Wissenschaft nach Nosek (https://www.cos.io/blog/strategy-for-culture-change)\n\n\n\nPolitik\nInternational stehen politische Parteien und Vereinigungen deutlich hinter Open Science und Open Access. Beispielsweise empfiehlt die UNESCO einen universellen Zugang zu wissenschaftlichen Wissen ungeachtet von Herkunftsland, Geschlechterrolle, politischen Grenzen, ethnischer Zugehörigkeit, oder ökonomischen oder technologischen Hürden [UNESCO (2020); p. 3]. Arbeitsgruppen für politische Instrumente, Förderung, und Infrastruktur wurden entsprechend gegründet. Die G7 setzen sich für wissenschaftliche Integrität, akademische Freiheit, und Open Science ein. Offener Zugang aber auch Transparenz des wissenschaftlichen Vorgehens wird auch seitens der Europäischen Union gefordert. Infrastruktur (z.B. die European Open Science Cloud) und diverse Open Science Forschungsprojekte werden gezielt gefördert. Für alle durch die EU geförderten Forschungsprojekte steht zudem die kostenlose Publikations- und Begutachtungs-Plattform Open Research Europe zur Verfügung.\nIn Deutschland hat sich die Regierung der Periode 2021-2025 im Rahmen des Koalitionsvertrages vorgenommen, „Open Access … als gemeinsamen Standard [zu] etablieren.” Einzelne Bundesländer wie Nordrhein-Westfalen haben in Zusammenschlüssen aus den jeweiligen Universitäten darüber hinaus Open Access Strategien entwickelt (Openness 2023a) und arbeiten aktuell an Open Science Strategien. Andere Länder, wie zum Beispiel Schweden, haben bereits nationale Richtlinien zu Open Science entwickelt. Hinsichtlich der Problematik von Machtmissbrauch wird das Problem beispielsweise in einem Eckpunkte-Papier des Landes NRW anerkannt, doch als Einzelfall- statt System-Problem verstanden (Kommentar dazu).\n\n\nUniversitäten\n\n\n\n\n\n\nWelche Universitäten tun etwas? (Beispiele)\n\n\n\nDas Thema Open Science hat bei vielen Universitäten bereits Anklang gefunden. Während die meisten deutschen Universitäten (zeitlich begrenzte) Mittel für Open Access Publikationen haben, existieren an einigen darüber hinaus Open Science Policys (z.B. FAU Erlangen), Open Science Centers (z.B. LMU Open Science Center; Köln Open Science Center; Münster Center for Open Science; Mannheim Open Science Office). Darüber hinaus unterstützen das Leibniz-Informationszentrum Wirtschaft in Kiel und das Leibniz-Institut für Psychologie Replikationsforschung beispielsweise mit einer Replikationszeitschrift (https://www.jcr-econ.org) oder im Rahmen einer Juniorprofessur für Psychologische Metawissenschaft. An der TU Dortmund wurde eine Bedarfserhebung zum Thema Forschungsdatenmanagement durchgeführt (Kletke u. a. 2024). Eines der größten Zentren für Metawissenschaft n Europas hat sich in den Niederlanden in Tilburg gebildet. Die Berliner Universitäten haben eine gemeinsame Open Access Erklärung entwickelt (https://openaccess.mpg.de/Berliner-Erklaerung) und in Frankreich ist die Universität Sorbonne ein Pionier: Gemeinsam mit der Universität Amsterdam und dem Universitätscollege London wurde eine Erklärung über die die Veröffentlichung von Forschungsdaten unterzeichnet. Seit 2024 hat die Universität Sorbonne darüber hinaus den Vertrag mit Clarivate für die Nutzung der Forschungsdatenbank „Web of Science” gekündigt und arbeitet seitdem mit der Open Source Software „OpenAlex” (Priem, Piwowar, und Orr 2022).\nMithilfe des Road2Openness Tools (https://road2openness.de/tool/) können Institutionen eine Selbsteinschätzung für verschiedene Open Science Themenbereiche durchführen und einen Bericht erhalten.\n\n\nFür die langfristige Entwicklung der Wissenschaften haben Universitäten dadurch eine besondere Verantwortung, dass sie Wissenschaftler*innen beschäftigen und an ihnen die Auswahl für die wenigen unbefristeten Arbeitsplätze in der Wissenschaft fallen. Wenn jahrzehntelang Professuren auf Basis subjektiver, nicht-reproduzierbarer, und für gute Wissenschaft nachrangigen Kriterien gewählt werden (z.B. Anzahl an Publikationen in Fachzeitschriften), kann sich das negativ auf die Entwicklung von Wissenschaften auswirken. Diesem Problem entgegenwirkend wurde ein Forschungspreis des Berlin Institute of Health (BIH), der jährlich für Projekte zur Förderung von wissenschaftlicher Integrität verliehen wird, an ein Projekt, das objektive und sinnvolle Auswahlkriterien für Professor*innen entwickelt vergeben (Schönbrodt, Gärtner, Frank, Gollwitzer, Ihle, Mischkowski, Phan, Schmitt, Scheel, Schubert, und others 2022; Gärtner, Leising, und Schönbrodt 2022a). Um die Rolle quantiativer Indikatoren zu schwächen wird bereits an einigen Universitäten und bei DFG-Anträgen die “N-best” bzw. häufig “5-best” Regel angewandt (Frank 2019): Dabei dürfen nur die 5 besten Forschungsartikel in der Bewerbung genannt werden und die Evaluation darf nur auf Basis von ihnen geschehen.\nInnerhalb von Universitäten spielen außerdem die Bibliotheken eine aufklärerische Rolle hinsichtlich Forschungsdatenmanagement und Publikationskultur (Schmidt u. a. 2024). Über sie kann der Forschungsprozess mit entsprechender Infrastruktur (z.B. zum Lagern und Veröffentlichen von Forschungsmaterialien und -ergebnissen) unterstützt werden (Quan 2021) und verhindert werden, dass sich eine „Abhängigkeit von wenigen kommerziellen Anbietern” ergibt, die „begrenzen, was [bei] der Forschung an Arbeitsmöglichkeiten und Fragestellungen erreichbar ist” (Siems 2024). Eine Pflicht, Mitglieder eine Universität zur Einhaltung von Open Science Strategien zu bewegen, gibt es an Universitäten durch den hohen Stellenwert der „Freiheit der Forschung” kaum. Damit könnten sie Gefahr laufen, für Forschende weniger attraktiv zu werden: Wenn beispielsweise auf namhafte Zeitschriften nicht mehr über die Universität zugegriffen werden kann, weil Verträge mit closed-access Zeitschriften gekündigt wurden, erschwert das den Forschenden die Arbeit. Beispielsweise klagte die juristische Fakultät der Universität Konstanz gegen eine Zweitveröffentlichungspflicht: Forschende in Deutschland haben das Recht zur Zweitveröffentlichung, das heißt, dass sie ihre Forschung, wenn sie in einer Fachzeitschrift veröffentlicht wurde, auch selbst (z.B. über eigene Websites) veröffentlichen dürfen. In Konstanz konnten die Forschenden nicht dazu verpflichtet werden, davon Gebrauch zu machen.\nEin weiteres Stellrad von Universitäten ist die Bezuschussung von Publikationskosten bei Zeitschriften. Wird beispielsweise die wissenschaftliche Qualität einer Zeitschrift angezweifelt, kann eine Universität diese Bezuschussung stoppen.\nEine oft vernachlässigte Rolle kommt außerdem der universitären Lehre hinzu. Durch die Freiheit von Forschung und Lehre und bereits durchgeplanten Studiengängen gestaltet sich die Integration neuer Themen wie Open Science schwierig. Forschende, in deren Lehre die Thematik eine Rolle spielt, teilen proaktiv ihre Materialien, erstellen gemeinsam Curricula, und sind beispielsweise in großen internationalen Initiativen wie dem Framework for Open and Reproducible Research Training (FORRT.org) vernetzt (konkrete Vorschläge zur Integration von Open Science in die Lehre hat zum Beispiel Pennington und Pownall (2024) veröffentlicht.\n\n\nInstitute und Vereinigungen\nWissenschaftliche Gebiete leben vor allem durch Communities, also alle in dem Bereich forschenden Personen. Sie organisieren sich in Vereinen (z.B. Deutsche Gesellschaft für Psychologie), Interessensverbunden, oder ähnlichen Gemeinschaften. Eine besondere Stellung hat in Deutschland die Deutsche Forschungsgemeinschaft, welche staatlich und über die Bundesländer mit mehreren Milliarden Euro ausgestattet Forschungsgelder vergibt. Als eine der wichtigsten nationalen Institution hat ihre Open Science Positionierung einen hohen Stellenwert (Deutsche Forschungsgemeinschaft 2022). Erfahrungsgemäß gehen Veränderungen jedoch nicht von der DFG aus, sondern die DFG wartet auf Anstöße aus den Fächern. In der Psychologie fördert darüber hinaus das ZPID die Infrastruktur durch Zeitschriften, Pre-Print Server, und weitere Methoden und in den Wirtschaftswissenschaften verwaltet das ZBW wichtige Informationen oder Zeitschriften (REF). Auch interdisziplinäre Vereinigungen wie das CERN oder internationale Akteure wie die American Psychological Association verpflichten sich zu Offenheit und Transparenz.\nIm Rahmen der Open Science Reform entstanden außerdem viele neue Vereinigungen. Das interdisziplinäre und besonders von Wissenschaftler*innen in der frühen Karrierephase geleitete FORRT (Azevedo u. a. 2019) setzt sich für eine Verankerung von Open Science in der Lehre ein. Der Verbesserung psychologischer Forschung hat sich die Society for the Improvement of Psychological Science (SIPS) verschrieben. Sogenannte „grassroot”-Initiativen (also von jungen Wissenschaftler*innen ausgehende Bewegungen) haben sich an zahlreichen Universitäten herausgebildet und zu Netzwerken wie dem Netzwerk der Open Science Initiativen (NOSI) und „Reproducibility Networks” wie dem deutschen Netzwerk GRN (https://reproducibilitynetwork.de), dem im Vereinigten Königreich UKRN (https://www.ukrn.org) und weiteren zusammengeschlossen. Aber auch Zusammenschlüsse von Professor*innen zur Änderung von Kurzzeitverträgen existieren (z.B. Netzwerk Nachhaltige Wissenschaft, https://netzwerk-nachhaltige-wissenschaft.de).\n\n\nZeitschriften\nWissenschaftliche Zeitschriften gelten als Bühne des wissenschaftlichen Diskurses und bestimmen maßgeblich, welche Elemente des Forschungsprozesses zum „scientific record” gehören und damit relevant sind. Sie sind darüber hinaus als Organisatorinnen des Begutachtungsprozesses für die Qualitätssicherung in der Wissenschaft verantwortlich. Dem Mangel an Qualität entgegnend existieren bereits ausführliche Empfehlungen zur Gestaltung von Zeitschriften, es bilden sich neue Zeitschriften, und vollständige neue Begutachtungs- und Publikationsmodelle werden vorgeschlagen und vielseitig implementiert. Das Journal of Open Source Software basiert beispielsweise auf einem öffentlich einsehbaren Programmiercode und seine Infrastruktur lässt sich für weitere Zeitschriften einfach kopieren und anpassen.\n\nEmpfehlungen\nHerausgeber*innen, die sich in Bezug auf die von ihnen verwaltete Zeitschrift mit Open Science Praktiken auseinandersetzen möchten, können inzwischen auf einen umfangreichen Leitfaden zurückgreifen (Silverstein u. a. 2023). Über eine Diskussionsplattform (Journal Editors Discussion Interface, JEDI) wurden Vorschläge gesammelt und es wird erklärt, worum es sich bei Dingen wie Registered Reports, Open Peer Review, Diversifizierung, und Open Access handelt und wie diese in eine Zeitschrift implementiert werden können. Eventuelle Sorgen und Ängste werden angesprochen und beantwortet. Das Committee on Publication Ethics (COPE) setzt sich ebenfalls für Aufklärung und Lehre ein, die Herausgeber, Universitäten, und Forschungsinstitute im Umgang mit Problemen im Publikationssystem helfen soll. Es bietet beispielsweise Richtlinien unter welchen Umständen Publikationen zurückgezogen oder korrigiert werden sollten, oder welche ethischen Standards ein Begutachtungsprozess erfüllen sollte. Herausgeber*innen, die Zeitschriften für kommerzielle Verlage verwalten und auf Systeme umsteigen möchten, die vollständig in der Hand der Forschenden liegen, können über Universitätsbibliotheken Hilfe bei der Migration von den kommerziellen zu offenen und kostenfreien Systemen erhalten und Zeitschriften beispielsweise mit dem Open Journal System verwalten (siehe z.B. OJS Netzwerk). Eine Datenbank mit bereits über 20,000 offen zugänglichen Zeitschriften verwaltet das Directory of Open Access Journals (DOAJ). Gutachter*innen von Forschungsartikeln können über die Reviewer Zero Initiative (https://www.reviewerzero.net) auf Lehrmaterialien und Leitfäden zugreifen (https://osf.io/e7z5k/wiki/Resources/).\n\n\nOpen Science Praktiken hervorheben\nAus der Psychologie ist bekannt, dass Motivation über Belohnung ebenso gut wie über Bestrafung funktioniert (Balliet, Mulder, und Van Lange 2011). Im Straßenverkehr, in dem es bis vor einiger Zeit nur Bestrafungen für Verletzen von Regeln gab, äußert sich die Erkenntnis beispielsweise an Geschwindigkeitstafeln, die den Autofahrer*innen beim Einhalten des Tempolimits einen fröhlichen Smiley zurückmelden. Wissenschaftliche Zeitschriften heben Einhaltung von Empfehlungen (z.B. öffentlich verfügbare Datensätze) mit Plaketten (Badges) hervor. Die Zeitschrift Psychological Science geht dabei seit 2024 so weit, dass Badges schon wieder abgeschafft werden und Open Science dort der Standard für alle Artikel ist. Auf der Website topfactor.org sind Zeitschriften und deren Einhaltung verschiedener Standards aufgelistet und in einer Rangliste abgebildet. Zuletzt besteht in jedem System mit Belohnungen das Problem, dass die Akteure ihr Verhalten auf die Belohnungen hin ausrichten und dabei versuchen, Abkürzungen zu gehen (Klonsky 2024).\n\n\nReview Systeme\nWissenschaft zeichnet sich durch Systematik aus (Hoyningen-Huene 2013). Der wohl systematischste Weg, die wissenschaftliche Qualitätssicherung zu garantieren, wäre eine Studie, die verschiedene Systeme vergleicht. Während aktuelle Forschung ähnliches unternimmt (Soderberg u. a. 2021), werden alternative Begutachtungssysteme aktuell ausprobiert. Zur Erinnerung: Wissenschaftler*innen verfassen Artikel, die sie bei Zeitschriften einreichen. Dort ist eine Person (Editor) dafür zuständig, dass der Artikel, sofern er zur Zeitschrift passt, an Gutachtende gesendet wird.\nUm zu prüfen, ob die Urteile im Begutachtungsprozess zwischen den Urteilenden übereinstimmen, haben Etzel u. a. (2024) verschiedene Gutachtende hinsichtlich klassischer Kriterien befragt. Sie fanden heraus, dass das nicht der Fall ist und schlagen Kriterien vor, die einen klaren Wert haben, und sich gut erfassen lassen (z.B. ob Daten öffentlich verfügbar sind).\n\n\nOpen Peer Review\nSeit der Auseinandersetzung Forschender mit dem Begutachtungssystem wird darüber hinaus diskutiert, was mit Gutachten passiert: Traditionell bleiben sie unter Verschluss. In der Fachzeitschrift erscheint der finale Artikel und alle vorherigen Versionen sind nur Autor*innen, Herausgeber*in, und Gutachter*innen bekannt. Diese bleiben zudem meistens anonym, das heißt, wenn sie sich keine Mühe gegeben haben, wird es wahrscheinlich nie auffallen. Außerdem haben Forschende keinen Anreiz, Gutachten zu verfassen - höchstens erhalten sie einen Nachweis, dass sie ein Manuskript bei einer Zeitschrift begutachtet haben. Einige Zeitschriften haben inzwischen das Open Peer Review eingeführt. Der Name passt nur halb, denn veröffentlicht werden Gutachten nur dann, wenn der Artikel bei der Zeitschrift akzeptiert wird. Das birgt die Gefahr, dass schwerwiegende Probleme, welche für eine Ablehnung üblicherweise nötig sind, nicht ans Licht kommen. Forschende können den Artikel dann ohne Überarbeitung bei einer anderen Zeitschrift einreichen, wo die Probleme vielleicht nicht entdeckt werden. Dieses Vorgehen führt zu Doppelarbeit und enormen Kosten (Aczel, Szaszi, und Holcombe 2021). Zoltan Kekecs merkte dazu bei einer Diskussion im Rahmen einer Konferenz an, dass selbst Kasinos, die in Konkurrenz zueinander stehen, Listen von Betrüger*innen miteinander austauschen. Herausgeber*innen von Zeitschriften tun das noch nicht. Ein Modell, bei dem alle Bewertungen veröffentlicht werden, bietet Unjournal (https://unjournal.pubpub.org).\n\n\n\n\n\n\n\n\nGutachten bleiben unter Verschluss\nGutachten werden bei Publikation veröffentlicht\nGutachten werden bei Publikation und Ablehnung veröffentlicht\n\n\n\n\nEs ist kein Nachweis der Qualitätskontrolle möglich.\nAbgelehnte Artikel können bei anderen Zeitschriften ohne Überarbeitung eingereicht werden und führen zu Mehraufwand.\nQualitätskontrolle ist nachvollziehbar und transparent.\n\n\n\nUmstrittener als die Veröffentlichung von Gutachten ist die Anonymität der Gutachter*innen: Der Standard ist, dass Gutachten anonym sind, aber bei Wunsch unterzeichnet werden können. Promovierende, die einen Artikel eines potenziellen zukünftigen Chefs oder einer zukünftigen Chefin schlecht beurteilen, werden dadurch geschützt. Selbst Professor*innen können bei negativen Beurteilungen riskieren, dass der oder die Kolleg*in später einen Forschungsgelderantrag von ihnen begutachtet und sich für die Kritik rächt. Auf der anderen Seite kann Anonymität zur Folge haben, dass Kritik gegen die Person gerichtet ist und nicht konstruktiv ist.\nEine weitere Art, wie Peer-Review offen sein kann, ist, dass sich jede Person daran beteiligen kann. Das ist zum Beispiel bei Meta-Psychology möglich. Problematisch ist dabei jedoch die relativ geringe Beteiligung. Bei den Zeitschriften Zeitschrift Synlett und ASAPbio werden Gruppe dazu koordiniert, wie es sie zur Diskussion spannender Artikel schon in Form von Journal Clubs gibt - nur eben als “Pre-Print Review Club”.\n\n\nBegutachtung von Pre-Prints\n\n\n\n\n\n\nWas ist ein Pre-Print?\n\n\n\nEin Pre-Print nennt man ein Manuskript in dem oder vor dem Stadium der Einreichung bei einer Zeitschrift. Es wurde möglicherweise noch nicht begutachtet oder nach Begutachtung abgelehnt, auf einer Internetseite veröffentlicht, ist zitierbar, und kostenlos zugängig. In manchen Fällen mag es sinnvoll sein, ein wissenschaftlichen Beitrag nicht der Begutachtung zu unterziehen (z.B. bei Kommentaren, Positions-Artikel, oder öffentlichem Austausch). Typischerweise zählen begutachtete Beiträge für die wissenschaftliche Karriere mehr. Der Zweck von Pre-Prints ist vielfältig: Sie erhöhen die Verfügbarkeit von Wissen, erlauben eine schnellere Veröffentlichung (Beweise in der Mathematik müssen aufwändig im Laufe von bis zu mehreren Jahren nachgeprüft werden), und verhindern, dass einem andere Forschende mit einer innovativen Idee zuvorkommen. In der Medizin und den Sozialwissenschaften wurden sie aufgrund des schnelleren Austausches im Zuge der Corona-Pandemie ausgiebig verwendet (Fraser u. a. 2020). In der Epidemilogie konnte nicht nachgewiesen werden, dass Pre-Prints qualitativ schlechter sind (Nelson u. a. 2022).\n\n\nÜber Plattformen und Gemeinschaften wie PCI oder f1000research oder zukünftig auch MetaROR (https://researchonresearch.org/project/metaror/) werden Pre-Prints begutachtet. Sie werden dann nicht bei einer Zeitschrift sondern bei dem der jeweiligen Einrichtung eingereicht und dort begutachtet. Das Modell nennt sich publish-review-curate, es werden also zuerst Pre-Prints hochgeladen, diese dann begutachtet, und schließlich zu Themenbereichen zusammengefasst. Die Qualitätssicherung wird dabei von Forschenden selbst organisiert und ist unabhängig von kommerziellen Verlagen. Dabei wird empfohlen, Studierende aktiv mit in den Prozess einzubeziehen (Holford u. a. 2024). Das Modell bei PCI ist, dass Artikel, die dort ein positives Gutachten erhalten haben, ohne weiteres Gutachten bei einer der teilnehmenden (PCI-friendly) Zeitschriften veröffentlicht werden können. Auf F1000research.com fungiert wie eine Zeitschrift, bei der Artikel direkt zugängig sind, sich über die Zeit durch Peer Review verändern. Auf ähnliche Weise gibt es bei der Zeitschrift für digitale Geisteswissenschaften eine Peer-Review-Ampel: Nach Einreichung sind Artikel dort direkt verfügbar und eine Ampel gibt an, ob sie unter Begutachtung sind, und falls sie begutachtet wurden, ob es (noch) schwerwiegende Probleme gibt oder nicht.\n\n\nGedächtnis von Gutachten\nEine weitere Möglichkeit, Gutachten fest an Forschungsartikel “dranzuheften”, besteht über Plattformen, die eine schnelle Kommentierung ermöglichen. Zum Beispiel lassen sich über Pubpeer.com oder hypothes.is alle wissenschaftlichen Beiträge (z.B. auch Daten) öffentlich und wenn gewünscht anonym kommentieren. Das ist für Pre-Prints möglich, sodass diese Kommentare dann dauerhaft damit verknüpft sind. Mittels Plug-Ins für Internetbrowser werden dann Artikel, zu denen es bei Pubpeer Diskussionen gibt, markiert. Weitere Plattformen sind alphaxiv.org, Disqus, oder scirev.org.\n\n\nAufmerksamkeit zum Thema in bestehenden Zeitschriften\nAnforderungen an wissenschaftliche Artikel seitens der Zeitschriften sind 2010 maßgeblichen Änderungen untergangen. In den Sozialwissenschaften orientieren sich zahlreiche Zeitschriften an den Richtlinien zur „Transparency and Openness Promotion” (TOP) und erhalten entsprechende TOP-Faktoren (https://topfactor.org/summary). Dabei wird festgehalten, welcher Grad an Offenheit und Transparenz von Forschungsdaten und -materialien gefordert wird und ob Replikationen bei der jeweiligen Zeitschrift veröffentlicht werden. Neue Herausgeber*innen bei existierenden Zeitschriften haben große Änderungen vorgenommen. Beispielsweise haben Hardwicke und Vazire (2023) für die Zeitschrift Psychological Science ein standardmäßiges Nachrechnen aller berichteten Ergebnisse ab 2024 angekündigt, indem sie mit dem Institute for Replication zusammenarbeiten (https://i4replication.org). Vereinzelt haben Zeitschriften Spezialausgaben herausgegeben, bei denen der Fokus auf Replikationsstudien oder der Reproduzierbarkeit von Ergebnissen lag (Carriquiry, Daniels, und Reid 2023).\n\n\nZeitschriften für “nicht Innovatives”\nDurch die Selektion spannender Ergebnisse gibt es für Forschung, die nicht bahnbrechend und dennoch höchst relevant ist, keine Plattform. Schätzungen zufolge werden bis zu 40% aller durchgeführten Studien innerhalb von 4 Jahren nach Durchführung nicht veröffentlicht (Ensinck und Lakens 2023). Andere Forschende können nicht davon lernen und Ressourcen, die in die Sammlung und Auswertung der Daten geflossen sind, Zeit von Versuchspersonen, und lange Vorbereitungen der Forschung werden schließlich verschwendet. Zur Lösung dieses Problems haben sich neue Zeitschriften und Formate gebildet. In der Ökonomie hat sich auf Forderungen (Zimmermann 2015) hin beispielsweise eine Zeitschrift für Replikationen und Kommentare gebildet. Die Zeitschrift Meta-Psychology bietet ein Format für Replikationen und eines für „Schubladenberichte” an. Letzteres ist für Studien vorgesehen, die wegen wenig überraschenden Ergebnissen oder Fehlern in der Durchführung anderweitig in der Schublade landen würden aber dennoch wichtige Informationen beinhalten. Ebenfalls zur Abbildung des für den Forschungsprozess typischen Fehlschlagens wurde das Journal of Trial and Error gegründet, und bei ReScience C und Rescience X können Berichte über Reproduzierbarkeit und Replikationen veröffentlicht werden.\n\n\nPublikationsmodelle\nNoch radikalere Vorschläge als die Anpassung bisheriger Zeitschriften ist der Vorschlag, das bisherige System durch ein neues zu ersetzen. Dabei handelt es sich um ein soziales Dilemma, bei dem Millionen von Forschenden sich plötzlich anders verhalten müssen und dabei entgegen der Spielregeln des wissenschaftlichen Systems handeln müssen (Brembs u. a. 2023). Das Dilemma wurde von kommerziellen Verlagen gestaltet, welche daraus Geld verdienen. Brembs u. a. (2023) haben einen präzisen Vorschlag erarbeitet, bei welchem ein dezentrales System aufgebaut wird, das soziale Netzwerke wie Mastodon als Vorbild hat, von Wissenschaftler*innen organisiert wird, und Forschungsprodukte wie Daten oder Programme ebenso wie die traditionellen Forschungsberichte wertschätzt. Plattformen, die ein solches Mikro-Publishing-System bereits implementieren sind Research-Equals oder Octopus.ac (Hsing?). Dem System, bei dem alle Produkte begutachtet werden, der Begutachtungsprozess aber nicht die Qualität sicherstellt, stehen hier Ampel-Systeme und öffentliche Kommentierung entgegen, die signalisieren, was und ob begutachtet wurde, welche Kritikpunkte vorlagen, und wie damit umgegangen wurde.\n\n\n\nForschende\nUnabhängig von Nationalität, Wissenschaftsgebiet, und Universität haben viele Forschende ihre Arbeitsweisen im Zuge von Open Science überdacht und angepasst. Hunderte haben öffentliche Erklärungen zur Forschungstransparenz und der Forderung von Open Science Praktiken in der Rolle von Gutachter*innen unterzeichnet. Einzelne Forschende führen im Rahmen von Lehre Replikationsstudien durch (Boyce, Mathur, und Frank 2023; Jekel u. a. 2020; Korell, Reinecke, und Lott 2023a), schließen sich weltweit zusammen um gemeinsam Projekte durchzuführen, die einzelne nicht stemmen könnten (z.B. Psychological Science Accelerator, https://psysciacc.org), und entwickeln Informations-Sammlungen (Open Scholarship Knowledge Base, https://oercommons.org/hubs/OSKB), Leitfäden und Glossare (Parsons u. a. 2022), um den Zugang zu Open Science zu erleichtern. Eine noch unerfüllte Forderung ist die, Forschende das System über Zusammenschlüsse im Rahmen von Gewerkschaften zu reformieren (Rahal u. a. 2023).\n\n\nBewertungskriterien\nMit der San Francisco Erklärung zur Forschungsbewertung (Declaration on Research Assessment, SF DORA) begannen 2012 viele Institutionen und Forschende, sich öffentlich und klar dagegen zu positionieren, Forschung ausschließlich auf Basis von Zitationsmetriken (z.B. Impact Factor) zu bewerten. Im Jahr 2024 gab es bereits über 25.000 Signaturen aus 165 Ländern. Die Erklärung besteht aus einer allgemeinen und anschließend spezifischen Empfehlungen (für Föderorganisationen, Instutionen, Verlage, …). Die allgemeine Empfehlung lautet. “Verwenden Sie keine Kennzahlen auf der Ebene von Fachzeitschriften, wie den Journal Impact Factor, als Ersatz, um die Qualität einzelner Fachartikel zu bewerten, um die Beiträge einzelner Wissenschaftler zu bewerten, oder um Entscheidungen über Einstellung, Beförderung oder Finanzierung zu treffen.” (https://sfdora.org/read/read-the-declaration-deutsch/).\nIn der Psychologie werden Bewertungskriterien für Forschende systematisch und mit Methoden der Persönlichkeitspsychologie und Diagnostik entwickelt (Schönbrodt, Gärtner, Frank, Gollwitzer, Ihle, Mischkowski, Phan, Schmitt, Scheel, Schubert, Steinberg, u. a. 2022; Gärtner, Leising, und Schönbrodt 2022b). Ein Problem ist dabei, dass sich Forschende in Gruppen die Arbeit aufteilen und manche Personen davon eher profitieren als andere (z.B. weil sie sich auf Methoden spezialisieren und in der Autor*innenliste seltener an erster Stelle stehen). Analog tritt das Problem im Fußball auf: Würde man Spieler und Spielerinnen nur nach geschossenen Toren bewerten, wären Leute in der Abwehr und im Mittelfeld und sogar diejenigen, die für die Vorlage verantwortlich sind, benachteiligt. Tiokhin u. a. (2023) schlagen vor diesem Hintergrund eine schrittweise Bewertung vor: Zuerst sollten die Gruppen, in denen Forschende arbeiten, bewertet werden, und anschließend die einzelnen Mitglieder.\nAuch bei der Bewertung von einzelnen Forschungsartikeln, also vor allem im Peer Review, werden Bewertungskriterien weiterentwickelt. Elsherif, Feldman, und Yeung (2023) haben eine Vorlage zur Begutachtung quantiativer, psychologischer Studien und Replikationen entwickelt. Im Rahmen der “Peer Reviewer Openness Initiative” (PRO, https://www.opennessinitiative.org) haben sich Forschende öffentlich dazu positioniert, Forschungsartikel nur dann positiv zu beurteilen, wenn sie Zugang zu allen nötigen Materialien und Daten haben (außer, es gibt gute ethische Gründe, weshalb das nicht möglich ist).\nEin grundlegendes Problem bei der Entwicklung von Bewertungskriterien ist, dass sie oft Dinge jenseits der Normalfälle benachteiligen (Hostler 2024). Mittels Kriterien wird ein gleichförmiger Maßstab über viele verschiedene Forschenden und Forschungsdisziplinen gehalten. Wirtschaftswissenschaftler*innen, die nach dem Impact Factor bewertet werden, veröffentlichen gerne Artikel in Zeitschriften, die sich mit anderen Disziplinen überschneiden, weil dort die Zitationszahlen höher sind. Werden methodische Kriterien zur Bewertung herangezogen (z.B. ob eine zentrale Studie präregistriert ist), werden diejenigen benachteiligt, die qualitativ forschen und für die eine klassische Präregistrierung gar nicht hilfreich ist.\n\nAlternativen zum Impact Factor\nWährend die San Francisco Erklärung “DORA” klar den Journal Impact Factor verurteilt, lässt sie offen, welche Alternativen gewählt werden sollten. Aufbauend empfiehlt die Coalition for Advancing Research Assessment (coara.eu) die Verwendung qualitativer Merkmale. B. A. Nosek u. a. (2015) entwickelten die Transparency and Openness Promotion Guidelines (TOP Guidelines), die Zeitschriften auf Basis von zehn Kriterien bewerten und nach denen sich Zeitschriften einordnen lassen (topfactor.org).\n\n\n\n\n\n\nTOP Faktor versus Hirsch Index\n\n\n\nUm den TOP Faktor für eine Zeitschrift zu berechnen, muss zuerst für alle Facetten notiert werden, wie offen und transparent eine Zeitschrift ist. Beispielsweise muss bezüglich der Transparenz von Materialien geprüft werden, ob in den Richtlinien der Zeitschrift überhaupt etwas steht (0 Punkte), ob in Zeitschriften bloß notiert werden muss, ob Materialien verfügbar sind (1 Punkt), ob Materialien in einer Datenbank hochgeladen werden müssen (2 Punkte), oder ob darüber hinaus eine Person die Analysen reproduzieren (also nachrechnen) wird (3 Punkte). Die Punkte über alle Facetten werden aufsummiert - es ist also streng genommen eine TOP “Summe” und kein Faktor (so wie der Impact Factor eigentlich ein Quotient ist). Psychometrisch ist dabei bedenklich, dass die Summe der verschiedenen Facetten gebildet wird, so als würden 3 Punkte bei einer Facette einen fehlenden Punkt bei einer anderen Facette problemlos ersetzen können.\nDer Hirsch Index (oder h-index) gibt an, dass von allen Publikationen mindestens so viele davon so oft zitiert wurden. Ein Index von 4 hieße, dass mindestens 4 Artikel mindestens 4 mal zitiert wurden.\nH Indizes und TOP Faktoren sind für viele Zeitschriften öffentlich einsehbar. Die Daten lassen sich leicht herunterladen und miteinander in Zusammenhang setzen (Materialien dafür sind online verfügbar: https://osf.io/utzfs/). Die meisten Zeitschriften haben einen sehr kleinen H Index, weshalb die Daten für bessere Lesbarkeit logarithmiert wurden. Während für Zeitschriften in Nordamerika ein leichter Zusammenhang zwischen H Index und TOP Faktor sichtbar ist, liegt für westeuropäische Zeitschriften kein Zusammenhang vor. Offenheit und Transparenz als objektive Gütekriterien für Wissenschaft haben also nachweislich nichts oder nur wenig mit Zitationszahlen zu tun.\n\n\n\nUm aus Zitationszahlen Qualität schließen zu können, wurde von Peroni und Shotton (2012) ein System entwickelt, wie festgelegt werden kann, um was für eine Art Zitation es sich handelt (Citation Typing Ontology, CiTO) und das bereits von Zeitschriften implementiert wurde (Willighagen 2023). Inwiefern es die Bewertung von Forschung verbessert, bleibt abzuwarten. Nicht ganz ernstgemeint wurde für die Medizin außerdem der Free Lunch Index vorgeschlagen, der die Summe der Geschenke aus der Industrie abbildet (Scanff u. a. 2023).\n\n\n\nInfrastruktur (Open Infrastructure)\nIn der Pyramide zum Kulturwandel bildet Infrastruktur das Fundament. Sie ermöglicht, dass verschiedene Open Science Praktiken umgesetzt werden können. Beispielsweise lassen sich Forschungsdaten nicht einfach veröffentlichen, wenn es dafür keinen Ort oder keine Internetseite gibt. Diesbezüglich gab es massive Fortschritte und Forschungsmaterialien, Daten, oder Publikationen zu teilen war nie einfacher. Mittels der Richtlinien für Infrastruktur in der Forschung (Bilder, Lin, und Neylon 2020) ist festgelegt, wie Infrastruktur im Idealfall aufgebaut sein sollte: Beispielsweise sollte sie nachhaltig gestaltet und langfristig finanziert sein und über Disziplinen, Institutionen, und Orte hinaus verwendet werden. In Deutschland setzt sich zudem der Verein “Nationale Forschingsdaten Infrastruktur (NFDI) dafür ein, dass Daten als gemeinsames Gut organisiert werden. In fachspezifischen Konsortien werden Strukturen geschaffen, mittels derer sich Forschungsdaten teilen lassen. Eine interaktive Karte offener Infrastruktur ist online verfügbar (https://kumu.io/access2perspectives/open-science#disciplines/by-os-principle/open-infrastructure).\n\n\n\n\n\n\nDie Kosten eines “Offenen Buches”\n\n\n\nDieses Buch wurde vollständig mittels kostenloser Software verfasst (GNU R, RStudio, Quarto) und wird kostenlos bei Github gehostet. Ein weitere Schritt wäre die ausschließliche Verwendung von Open Source Software, also von Programemn, deren Code öffentlich gemacht wurde und welcher für eigene Zwecke verwendet werden kann. Aktuell ist die Verfügbarkeit dieses Buches davon abhängig, dass Github kostenlos bleibt.\n\n\n\nBeispiele für Open Science Infrastrukturen\n\n\n\n\n\n\n\nService\nZweck\nAnbieter\n\n\n\n\nLiteraturdatenbank\nVerwalten und Durchsuchen von Literatur\nOpenAlex\n\n\nDatenrepositorium\nVeröffentlichung von Forschungsdaten\nre3data.org, osf.io, researchbox.org, zenodo.org\n\n\nPre-Print Server\nVeröffentlichung von Forschungsartikeln\narXiv.org\n\n\nZeitschriftensystem (Editorial Manager)\nVerwaltung von wissenschaftlichen Fachzeitschriften (Einreichung, Begutachtung, Publikation, Indizierung)\nOpen Journal System\n\n\nPost-Publication Peer Review\nBegutachtung und Kommentierung von Forschung nach Veröffentlichung\nPubpeer.com\n\n\nIdentifizierung von Forschenden, Institutionen, und Forschung\n\nOpen Researcher and Contributor ID (ORCID.org), Research Organization Registry (ROR.org), Digital Object Identifier (DOI.org)\n\n\n\n\n\nOpen Access Publikationen\nUm den Zugang zu wissenschaftlichem Wissen zu erhöhen, wird mehr und mehr Forschung als “Open Access” (offener Zugang) veröffentlicht. Spezifisch kann das auf vielen verschiedenen Wegen geschehen: Personen können Artikel in nicht-kommerziellen Zeitschriften veröffentlichen (für eine Sammlung an über 20.000 Zeitschriften siehe z.B. https://doaj.org), manche kommerzielle Zeitschriften machen Artikel nach Ablauf einer bestimmten Zeit frei verfügbar, oder sie können Geld bezahlen, damit der Artikel öffentlich zugänglich in einer traditionellen Fachzeitschrift erscheint. Dabei können sich die Kosten auf Werte zwischen ein paar hunderten Euro bis zu 9.000€ bei “angesehenen Zeitschriften” belaufen. Die Gelder dafür stammen meistens aus den Mitteln von Universitäten (z.B. Open Access Funds) oder aus Projektmitteln, das heißt, dass bei der Beantragung von Geldern für das Projekt auch Gelder mit beantragt wurden, um die Kosten für Open Access Publikationen zu bezahlen. Während Open Access ursprünglich explizit nicht dieses Bezahlmodell meint, haben es Zeitschriften inzwischen “rekommerzialisiert” (Morgan und Smaldino 2024), sich also zu Nutzen gemacht. In der Open-Access-Strategie der Hochschulen des Landes Nordrhein-Westfalen (Openness 2023b) werden die Typen übersichtlich aufgelistet. Neben Open Access bei der Erstveröffentlichung haben Forschende im Sinne von “Green Open Access” üblicherweise das Recht, die von ihnen verfassten Artikeln auf ihrer eigenen Website, denen ihrer Institution, oder auf fachlichen Repositorien hochzuladen. Zumstein (2023) empfiehlt beispielsweise, bei Zeitschriften mit Abo-Modell (“Subskriptionsmodell”) nicht die Open Access Option dazuzukaufen, da es nicht nachhaltig ist und stattdessen die Zweitveröffentlichung offen zu machen.\n\nTypen von Open Access bei der Erstveröffentlichung nach Openness (2023c)\n\n\n\n\n\n\nTyp\nRegelung\n\n\n\n\nDiamond\nAlle Publikationen sind sofort und kostenlos verfügbar und es fallen keine Publikationsgebühren an\n\n\nGold\nAlle Publikationen sind sofort und kostenlos verfügbar, Autor*innen bezahlen Geld je Publikation\n\n\nHybrid\nDie Zeitschrift hat ein Abo-Modell. Einzelne Artikel werden gegen eine Gebühr öffentlich gemacht.\n\n\nMoving Wall\nDie Zeitschrift hat ein Abo-Modell. Nach Ablauf einer Frist (6-48 Monate) sind Artikel frei zugänglich.\n\n\nPromotional\nZur Bewerbung der Zeitschrift sind einzelne Artikel frei verfügbar.\n\n\n\nTypen von Open Access (siehe zenodo NRW AG Open Science Auflistung)\n\n\n\n\n\n\nWie Prestige für Offenheit blind machen kann\n\n\n\nAn manchen Orten gibt es ungeschriebene Gesetze wie “wenn du während deiner Promotion in einer hochrangigen Zeitschrift publizierst, wirst du die Bestnote kriegen”. Damit wird den prestigereichen Zeitschriften immer mehr Macht zugeschoben. Leuten wird außerordentlich dafür gratuliert, dass sie etwas in Psychological Bulletin oder sogar Nature Human Behavior veröffentlicht haben. Dass niemand, der nicht mit einer Universität affiliiert ist (also an einer arbeitet oder studiert), den Artikel im Psychological Bulletin lesen kann, oder dass für die Veröffentlichung in Nature bis zu 9.000 Euro (meistens aus Steuergeldern) bezahlt wurden, spielt dabei keine Rolle.\n\n\nNeben kommerziellen und Open Access Zeitschriften existiert noch ein weiterer Akteur bei der Zugänglichkeit von wissenschaftlichem Wissen: Mittels Schattenbibliotheken wie Sci-Hub können Personen auf Artikel, die sonst hinter einer Bezahlschranke liegen, zugreifen (“Guerilla Open Access”). Sci-Hub (https://de.wikipedia.org/wiki/Sci-Hub) als bekannteste Schattenbibliothek beinhaltet fast 70% aller 81.6 Millionen wissenschaftlichen Artikeln bis zum Jahr 2018 (Himmelstein u. a. 2018). Nutzungsstatistiken aus Deutschland hat Strecker (2019) analysiert. Die Verbreitung und das Herunterladen sind rechtlich umstritten. Andere Möglichkeiten, kostenlos an Forschungsartikel zu kommen sind 12ft.io, der Hashtag #canihazpaper in sozialen Netzwerken, oder das persönliche Anschreiben von Autor*innen per Mail oder über soziale Netzwerke für Forschende (Researchgate.net, Academia.edu).\n\nWahl der Fachzeitschrift\nAbgesehen von Prestige oder Journal Impact Factors können Forschende bei der Wahl der Zeitschrift, in der sie ihre Forschung veröffentlichen möchten, auf Gold Open Access achten (z.B. via https://doaj.org oder https://freejournals.org/current-member-journals/) und den TOP Faktor berücksichtigen (topfactor.org). Zudem sollten sie bei ihren Bibliotheken nachhaken: Bei Zeitschriften mit Abo-Modell (hybrid Open Access) hat sich nämlich ein Markt für bezahlte Open Access Publikationen entwickelt. Zeitschriften und Verlage veröffentlichen dabei extrem viele Artikel ohne strenge Begutachtung und verdienen Geld durch die Open Access Gebühren. In dem Fall sind die Zeitschriften als “Open Access Zeitschriften” vermarktet und die Kosten heißen “Author Processing Charges (APCs)”. Mittels des von der Universität Bielefeld betriebenen Dashboards wird aufgeschlüsselt, welche Zeitschriften und welche Verlage wie viel Geld von deutschen Universitäten bekommen haben (https://treemaps.openapc.net/apcdata/openapc/#publisher/). Der Verlag MDPI, mit über 2000 Artikeln auf Platz 1 bei der Artikelanzahl und mit Einnahmen von über 4 Millionen Euro auf Platz 2 hinsichtlich Profit, ist dabei besonders umstritten: Forschende (https://predatoryjournals.org/news/f/is-mdpi-a-predatory-publisher) konnten Nachweisen, dass die Bearbeitungszeiten (Dauer der Begutachtung, Revision, Veröffentlichung) unrealistisch gleichförmig sind und es pro Tag mehrere Spezialausgaben gibt (für gewöhnlich hat eine Zeitschrift 1-2 Spezialausgaben pro Jahr). Beall hat auf seiner Website (https://beallslist.net) eine umstrittene Liste veröffentlicht, die Zeitschriften als unwissenschaftlich kennzeichnet und beschreibt seine Erfahrungen in einem Artikel (Beall 2017). Ebenfalls helfen Tools zur Identifikation unseriöser Zeitschriften und Konferenzen (https://thinkchecksubmit.org, https://thinkcheckattend.org).\n\n\nMonitoring\nWie viel Forschung als Open Access veröffentlicht wird oder wie viel das Kostet, lässt sich mithilfe verschiedener Werkzeuge beobachten. OpenAPCs listet Open Access Kosten je nach Verlag, Zeitschrift, und Universität auf (https://treemaps.openapc.net/apcdata/openapc/) und der Open Access Monitor schlüsselt auf, wie viele Publikationen in Deutschland unter welchen Open Access Modellen veröffentlicht werden (https://open-access-monitor.de).\nAktuell (Stand Sommer 2024) haben 54,4% aller indizierten Fachzeitschriften kein offenes Modell und der Großteil aller Forschung wird darüber veröffentlicht. 19,2% der Fachzeitschriften laufen außerdem unter einem Transformationsvertrag. Dabei soll ein Übergang vom Abo-Modell zu einem Open Access Modell geschafft werden und alle bisher veröffentlichten Artikel sollen ebenfalls öffentlich zugänglich gemacht werden. Die wohl größte Rolle spielt dabei das DEAL Konsortium (https://deal-konsortium.de/publizierende): Dabei haben sich deutsche Wissenschaftsorganisationen zusammengeschlossen, um mit Verlagen einen bundesweiten Vertrag auszuhandeln, das allen deutschen Forschenden ermöglicht, Open Access ohne zusätzliche Kosten zu publizieren.\n\n\n\n\n\n\nOffene Lehrbücher\n\n\n\nWährend Artikel in Fachzeitschriften primär für den Austausch unter Forschenden genutzt werden, spielen Lehrbücher die besondere Rolle, dass sie die Kommunikation zwischen Expert*innen und Interessierten (z.B. Studierenden) bilden. Das Verhältnis zwischen Lehrbuch-Autor*innen und Verlagen ist weniger angespannt als das zwischen Forschenden und Zeitschriften - auch, wenn es größtenteils dieselben Verlage sind. Zwei besondere Unterschiede könnten die Ursache dafür sein: 1. Verfasser*innen von Lehrbüchern verdienen von Verkäufen und 2. können sie ihre eigenen Werke für Prüfungen relevant erklären. In der Folge müssen Studierende die Kosten tragen und nicht sie selbst. Im Vereinigten Königreich existieren bereits Pilotprojekte, die das Ziel haben, die Lehre möglichst vollständig auf offene Lehrbücher umzustellen (Farrow, Pitt, und Weller 2020).\n\n\n\n\nMassenrücktritte von Herausgeber*innen\nImmer häufiger kommt es vor, dass die Herausgeberschaft einer Zeitschrift zurücktritt. Grund dafür, dass Verlage die Publikationskosten oder die Anzahl veröffentlichter Artikel erhöhen möchte. Eine Liste solcher Rücktritte verwaltet Retractionwatch.org (“Editorial Mass Resignations”, https://retractionwatch.com/the-retraction-watch-mass-resignations-list/). Dabei hat eine Community aus Forschenden jahrelang hart gearbeitet, um die Zeitschrift zu verwalten und bekannt zu machen und wird dann damit bestraft, dass sie noch mehr Geld dafür zahlen muss, ihre Forschung miteinander auszutauschen.\nIn vielen solcher Fälle gründen die Forschenden im Anschluss an ihren Rücktritt eine neue, üblicherweise Open Access Zeitschrift. Während Universitätsbibliotheken Gründungen neuer Zeitschriften unterstützen und der Prozess technisch unproblematisch ist, gibt es legale und soziale Hürden: Verlage wie Taylor & Francis vereinbaren mit den Forschenden häufig “Non-comepete Klauseln”, verbieten ihnen also, im Anschluss an ihre Herausgebertätigkeit innerhalb eines oder mehrerer Jahre bei einer anderen Zeitschrift zu arbeiten. Eine wissenschaftliche Community, die der Kern einer Zeitschrift ist, muss außerdem einstimmig hinter der Veränderung stehen. In einem Fall hat die Herausgeberschaft klar kommuniziert, dass es sozial völlig inakzeptabel ist, die alte Zeitschrift zu unterstützen. Neu gegründete Zeitschriften haben außerdem noch keinen Impact Factor, weil noch keine zitierfähigen Artikel erschienen sind und Artikel noch nicht zitiert werden konnten. Verlage arbeiten gegen Massenrücktritte, indem sie die Zugehörigen der Herausgeberschaft häufig rotieren, sodass sie sich schlechter absprechen können. Treffen in Person sind durch die Internationalität der Forschung ebenfalls erschwert.\n\n\n\n\n\n\nOffenheit durch Klemmbausteine\n\n\n\nOffenheit der Wissenschaft kann viele Gesichter haben: Eine besondere Form des öffentlichen Zugangs verbreitet sich aktuell in der biotechnologischen Forschung aus. Damit Wissenschaftler*innen, Ingenieur*innen, aber auch die allgemeine Bevölkerung Zugang zu Konstruktionen hat, wird dort zu Klemmbausteinen wie beispielsweise solchen von dem Unternehmen LEGO® gegriffen (Boulter u. a. 2022). Das Patent für “den Lego-Stein” ist bereits ausgelaufen, sodass verschiedene Unternehmen oder Personen mit 3D-Drucker eigene Klemmbausteine herstellen können. Die Dateien für 3D-Drucker sind im Internet frei verfügbar. So konnte der Forscher David Aguilar beispielsweise einen prosthetischen Arm mit Klemmbausteinen entwerfen.\n\n\n\n\n\nPre-Prints\nWie bereits im Kapitel zur Begutachtung von Pre-Prints erläutert, sind Pre-Prints immer öffentlich und kostenlos verfügbar. Forschende können verschiedene Lizenzen vergeben, die beispielsweise eine kommerzielle Verwendung verbietet, sind dabei jedoch fast immer sehr liberal. Neben den bereits diskutierten Vorteilen von Pre-Prints, sind Änderungen bei ihnen um ein vielfaches schneller: Forschende können jederzeit auf Kritik reagieren und Fehler korrigieren. In einem Artikel zum Coronavirus fiel ein Fehler kurz nach der Veröffentlichung auf und wurde innerhalb von zwei Tagen korrigiert. Bei einem Zeitschriftenartikel kann sich dieser Prozess über viele Jahre ziehen. Gerade bei ernsthaften Problemen, die eine Retraction zur Folge haben, sind Verlage vergleichsweise langsam. Einen Pre-Print können Autor*innen jederzeit vom Netz nehmen - wobei er über Suchmaschinen dabei sicherlich noch auffindbar bleibt.\nPre-Prints können außerdem dazu beitragen, dass Ressourcen effizienter benutzt werden: Durch die beschleunigte Kommunikation zwischen Forschenden fällt schneller auf, wenn verschiedene Gruppen an derselben Fragestellung arbeiten. So können sich Kooperationen bilden oder Gruppen verlagern ihre Schwerpunkte.\n\n\n\n\n\n\nUnberechtigte Sorge vor Ideenklau\n\n\n\nIn manchen wissenschaftlichen Disziplinen sind Forschende Pre-Prints gegenüber zögerlich. Sie haben Angst, dass jemand ihre Idee klaut und schneller als sie einen Artikel bei einer Fachzeitschrift dazu veröffentlicht. Während diese Angst beim klassischen System z.B. durch böswillige Gutachtende oder Zuhörer*innen bei einer Konferenz ebenfalls besteht und selbst bei veröffentlichten Fachzeitschriftenartikeln passiert, ist der Vorteil von Pre-Prints, dass sie mit einem Datum versehen sind und für alle klar nachvollziehbar ist, wann was veröffentlicht wurde.\n\n\n\n\nAnsätze gegen die Selektion spannender Ergebnisse\nDie Ergebnisse einer Untersuchung sind das, was am wenigsten in der Hand der forschenden Person liegt (bzw. liegen sollte - immerhin interessiert uns ja die Wahrheit und nicht die Kompetenz Forschender, Daten möglichst stark zu schönen). Umso frustrierender ist es, dass Zeitschriften das Ergebnis als Kriterium zur Publikation verwenden. Unter der Vielzahl von Einreichungen werden vor allem diejenigen Artikel gewählt, die spannende Ergebnisse erzielt haben oder die ihre anfängliche Vermutung bestätigen konnten (Confirmation Bias). Die folgenden Ansätze lösen dieses Problem zum Beispiel dadurch, dass die Ergebnisse aus dem Begutachtungsprozess ausgeschlossen werden.\n\nResults-blind peer review\nDer einfachste Weg ist dabei, den Ergebnisteil einfach zu schwärzen oder wegzulassen. Verschiedene Zeitschriften bieten das als Option an. Da es sich hierbei zurzeit (2024) eher um eine Ausnahme handelt, ist den meisten Gutachtenden jedoch klar, dass vor allem diejenigen die Option zum results-blind peer review wählen, deren Ergebnisse nicht “hübsch genug” für den klassischen Weg sind.\n\n\nRegistered Report\n\n\n\nKlassischer Forschungsprozess im Vergleich zu Registered Reports mit zusätzlicher Begutachtung vor der Studiendurchführung\n\n\nEin radikalerer Ansatz als die Begutachtung ohne Ergebnisteil ist die Begutachtung des Artikels, ohne dass Ergebnisse überhaupt existieren. Dieses Format heißt Registered Report. Dabei wird das Manuskript mit der zu prüfenden Theorie, Methodik, und dem Analyseplan bei der Zeitschrift eingereicht, ohne dass überhaupt Daten erhoben wurden. Kommt es zur Akzeptanz dieses „halbfertigen” Artikels (in principle acceptance), werden die Daten gesammelt, wie geplant ausgewertet, und es folgt eine weitere Begutachtungsrunde. Hierbei ist vorgeschrieben, dass die Autor*innen nichts an den bereits verfassten Teilen verändern dürfen und die Gutachter*innen im Nachhinein keine Kritik am bereits geprüften Vorgehen üben dürfen. Es geht nur noch darum, ob der Plan eingehalten wurde und ob die Schlussfolgerungen auf den geplanten Analysen fußen. Damit soll verhindert werden, dass Artikel abgelehnt werden, weil die Ergebnisse nicht spannend genug, innovativ genug, oder den Erwartungen entsprechend sind. Erste Untersuchungen können bereits nachweisen, dass sich damit die Qualität der Forschung gegenüber dem traditionellen Vorgehen verbessert (Soderberg u. a. 2021). Eine Übersicht über Zeitschriften, die dieses Format anbieten ist online verfügbar (https://www.cos.io/initiatives/registered-reports à Participating Journals; (Chambers?).; Chambers und Tzavella (2022)).  Ebenfalls wird dadurch deutlich, dass Forschung einem massiven Publikationsbias unterliegt (d.h. es werden vor allem Studien veröffentlicht, die ihre Vermutungen bestätigen konnten und kaum Studien, in denen das nicht geschah): (Scheel2021?) zeigten, dass der Anteil erwartungskonformer Ergebnisse bei Registered Reports mit 44% deutlich unter den in der Psychologie üblichen 96% liegt.\n\n\nPre-Print basierte Modelle\nDurch die immer häufigere Veröffentlichung von Pre-Prints, also noch nicht begutachteten Manuskripten, eröffnen sich für die Begutachtung neue Wege. Sogenannte Overlay Journals (elife) wählen unter Pre-Prints solche aus, die sie an Gutachtende schicken um deren Meinungen einzuholen. Sofern die Autor*innen des Preprints einverstanden sind, erhalten sie dann Gutachten und ihr Artikel wird schließlich in der Zeitschrift veröffentlicht.\nJe nach Fach haben unterschiedlich viele Zeitschriften mit PCI Initiativen Vereinbarungen, dass sie die akzeptierten Artikel ohne eigenes Peer Review veröffentlichen. Mehr und vor allem bekannte teilnehmende Zeitschriften machen PCIs für Forschende attraktiver. Zeitschriften, die Interesse an einem PCI haben, aber die Begutachtung nicht aus der Hand geben möchten, können sich als „PCI interested Journals” listen lassen (vs. „PCI friendly journals”). Teilnehmende Zeitschriften sparen dadurch Arbeit und bleiben relevant, indem sich Ihre Funktion dahin verschiebt, dass sie thematisch relevante Forschung sammeln und disseminieren. In einem Fall hat bereits eine Zeitschrift, die als „PCI friendly” eine Vereinbarung mit PCI-RR hatte, ein Manuskript mit einer Recommendation zum erneuten Peer Review versendet und wurde sofort von den PCI Partnern entfernt. Forschende, die Gutachtenprozesse für PCIs organisieren möchten – analog zu Herausgeber*innen von klassischen Zeitschriften – können Recommender werden und müssen dazu ein Mindestmaß an Wissen haben sowie eine Schulung absolvieren (https://rr.peercommunityin.org/about/recommenders).\nTabelle 2\nZusammenfassung der verschiedenen Begutachtungsmodelle und der jeweiligen Art des Umgangs mit den Forschungsergebnissen\n\n\n\n\n\n\n\nBegutachtungsprozedur\nAusblendung der Ergebnisse\n\n\nTraditionell\nErgebnisse sind sichtbar und fließen in die Beurteilung ein\n\n\nResults-Blind Peer Review\nErgebnisse liegen vor, werden den Begutachtenden jedoch vorenthalten\n\n\nRegistered Report\nErgebnisse liegen noch nicht vor\n\n\nPeer-Community-In Registered Report (PCI-RR)\nErgebnisse liegen noch nicht vor\n\n\n\n\n\n\n\n\n\nAnekdoten: Meine schlimmsten Erfahrungen mit Peer Review\n\n\n\nPeer Review ist brutal. Das ist meine persönliche Erfahrung mit dem Prozess. Früh musste ich in meiner wissenschaftlichen Arbeit lernen, dass es in vielen Fällen ein Glücksspiel ist. Renommierte Wissenschaftler*innen erklärten mir, dass sie Artikel hätten, die sie über Jahre immer wieder bei Zeitschriften immer wieder eingereicht hätten, und die schließlich positiv aufgenommen worden wären, ohne, dass sie sie stark verändert hätten. Jenseits von der Akzeptanz eines Artikels zur Publikation geht es auch um die Gründe für die Ablehnung: Häufig lesen Gutachtende Artikel nicht aufmerksam und Kritiken sind nicht konstruktiv. Hierzu meine schlimmsten Erfahrungsberichte.\nEinreichung bei Collabra: Es handelte sich um einen Artikel, der zwischen den Disziplinen steht. Es geht nicht nur um Erwartungen, nicht nur um Produktbewertungen, nicht nur um die Methode, Daten direkt aus dem Internet herunterzuladen. Der “bunte-Vogel-Artikel” war bereits bei drei Zeitschriften abgelehnt worden. Bei keiner wurde er an die Reviewer weitergegeben, weil er nie zur Zeitschrift passte. Die Zeitschrift Collabra, bei der wir ihn schließlich einreichten, war zu dem Zeitpunkt noch wenige Jahre alt und war breit aufgestellt. Nach der Einreichung im Mai 2020 haben wir über ein halbes Jahr lang auf das Gutachten gewartet. Länger zu warten ist erstmal ein gutes Zeichen: Der Artikel wurde wohl an Reviewer rausgeschickt. In dem Fall wurden wir jedoch bitter enttäuscht: Im November 2020 erfuhren wir auf Nachfrage hin, dass 14 Gutachter*innen angefragt wurden, sodass die Herausgeberin auf Basis eines Gutachtens entschied, das Manuskript abzulehnen. Grund dafür war, dass die Methode aufgrund der inzidentellen Daten nicht geeignet für die Fragestellung war. In der Psychologie sowie den Wirtschaftswissenschaften ist das Problem mit “echten Daten” seit mehreren Jahrzehnten bekannt und wir hatten es bereits ausgiebig im Artikel diskutiert.\nEinreichung bei Journal of Experimental Social Psychology: Wir hatten eine Replikation einer dort erschienen Studie durchgeführt - der Befund ließ sich nicht replizieren. Meine Überlegung war, der Zeitschrift, die den nicht robusten Befund ursprünglich publizierte, selbst die Chance zur Selbstkorrektur zu geben. Die Reviews waren fair und positiv, es gab ein paar Punkte zu diskutieren, aber uns war klar, dass es sich hier um Verständnisprobleme und keine inhaltlichen Aspekte handelte. Nicht so der Editor: Er erklärte, dass der Befund nicht neu genug wäre und klar wäre, dass es nicht replizierbar ist. Ich erklärte ihm, dass noch niemand den Befund zu replizieren versucht hatte und wir selbst sogar vor der Analyse der Ergebnisse eine Abstimmung darüber gemacht hatten, welches Ergebnis wir erwarteten: Es war sehr ausgewogen 50-50. Darüber hinaus, wurde ein weiterer Artikel publiziert, der entgegengesetzt Ergebnisse hatte. Wir stellten klar, dass wir alle aufgezeigten Probleme einfach lösen können und gerne die Chance zur Revidierung hätten. Der Editor hätte dabei kaum Arbeit außer den Artikel anschließend nochmal an Gutachtende zu schicken. Auf unsere Mail erhielten wir nur eine kurze Antwort: Hi. I know my decision is disappointing, but I’m going to stick with my decision on this one. Hier befand ich mich an einem Scheideweg: Warum ist ein Forscher nicht bereit, Gründe für eine wissenschaftliche Entscheidung zu erörtern? Wir entschieden uns, einen anderen Editor direkt zu kontaktieren. Nach kurzer Zeit erhielten wir die Einladung zu einer Revision. Der Artikel wurde schließlich in der revidierten Fassung veröffentlicht.\nEinreichung bei European Journal of Personality: Die zentrale Aussage dieses Artikel war, dass verschiedene Messwerte einer angeblichen Eigenschaft nicht miteinander zusammenhängen. Der Befund stellte die Annahme infrage, dass es sich dabei überhaupt um eine Eigenschaft handelte. Die Ablehnungsgründe zweier Gutachtenden und der Herausgeberin machten deutlich: Niemand hatte den Artikel überhaupt gelesen. Ein Gutachter merkte an, dass etwas mit den Werten nicht stimme, weil sie laut einer der Tabellen nicht miteinander zusammenhängen. Genau das war ja unsere Aussage. Wir zeigten, dass es nich an unseren Daten lag, sondern sich in anderen Datensätzen so verhielt. Hätte er die Überschrift der Tabelle gelesen, den Absatz davor, oder den danach, wäre das klar geworden. Hat er aber nicht…\nDas sind nur kurze Auszüge aus dutzenden Einreichungen und Ablehnungen. Darüber hinaus kann fast jede*r Forschende*r von substanzlosen persönliche Beleidigungen berichten. Meiner Erfahrung nach ist anonymes versus öffentliches Peer Review wie ein Vergleich von anonymen Kommentarspalten mit nicht anonymen: Bei Anonymität beherrschen persönliche Beleidigungen und Unwahrheiten den Dialog.\n\n\n\n\n\nReplikationsforschung\nHäufig ist die Rede von einer Replikationskrise, also einer Krise von zu geringer Replizierbarkeit. Wenig überraschend hat sich das auf die Rolle von Replikationen in den Sozialwissenschaften und darüber hinaus ausgewirkt. Zahlreiche Wege wurden eingeschlagen, die das Ansehen von Replikationsstudien und damit ihre Häufigkeit in der Forschung erhöhen. Dabei müssen die Wissenschaften nachholen, was sie zu Beginn ihrer Existenz hätten tun sollen, nämlich festzulegen, welche Ansprüche an Replizierbarkeit bestehen und wie diese geprüft werden sollen. Mit Replizierbarkeit meine ich dabei, dass eine Hypothese sich auch mit anderen Daten als denen der Originalstudie bestätigen lässt. Es geht also um ein minimales Maß an Verallgemeinerbarkeit und nicht primär um ein tieferes Verständnis von Theorien, auch, wenn letzteres dennoch manchmal kritisiert wird, obwohl niemand behauptet hat, Replikationen sollten das Theorie-Problem ebenfalls lösen (Feest 2019).\n\n\n\n\n\n\nUnschärfe von Replikationsstudien\n\n\n\nEin noch ungelöstes Problem ist die Unschärfe von Original- und Replikationsstudien. Ting und Greenland (2024) kritisieren, dass die Ungenauigkeit von Replikationsstudien oft missachtet wird. Ihnen entgeht dabei jedoch, dass Replikationsstudien in fast allen Fällen eine weitaus höhere Schärfe im Studiendesign haben und höhere methodische Standards einhalten, als alle vorherigen Studien. Schönigung von Ergebnissen im Sinne von p-hacking (Simmons, Nelson, und Simonsohn 2011) sind prinzipiell auch bei Replikationen möglich (Protzko 2018), durch die höheren Standards jedoch schwieriger. Zwar berücksichtigen Forschende Replikationsbefunde in ihren Urteilen angemessen (McDiarmid u. a. 2021), Untersuchungen dazu, wie Anfällig Replikationen für Datenfälschung im Vergleich zu Originalstudien sind, gibt es bisher noch keine.\n\n\n\nWas soll sich replizieren lassen?\nVor dem Hintergrund der Robustheit wird klar, wann eine erfolgreiche Replikation zu erwarten ist und wann nicht: Wird eine Hypothese in einer Originalstudie als allgemeingültig formuliert (z.B. kurz nach der Geburt wiegen männliche Babys im Mittel mehr als weibliche Babys), sollte sie sich auch wiederholbar nachweisen lassen. Dem stehen Fälle gegenüber, wenn eine Hypothese nicht allgemeingültig formuliert ist (z.B. dass etwas auf einen spezifischen Kontext bezogen ist wie in qualitativer Forschung). Es gibt ganze Disziplinen, für die Replizierbarkeit irrelevant ist, beispielsweise wird in der Archäologie bei Ausgrabungen ein Objekt aus seinem Kontext gerissen und damit das Original “zerstört”. Dieser Prozess ist nicht wiederholbar und es hat auch niemand den Anspruch daran. Ebenfalls ist es möglich Artefakte (also Befunde, die nur durch Methoden entstanden sind) zu wiederholen, wenn deren Ursprung auf einer allgemeinen Gesetzgültigkeit basiert (Devezer u. a. 2021). Zum Beispiel kann es vorkommen, dass ein statistisches Modell replizierbar “anschlägt”, also Befundmuster identifiziert, auch wenn diese nicht auf Grund der eigentlichen Erklärung entstehen sondern nur, weil sie schlecht kalibriert sind oder ihre Voraussetzungen verletzt wurden. Beispielsweise schien es lange Zeit so, dass Linkshänder*innen früher sterben als Rechtshänder*innen. Dieser Befund konnte eine Zeit lang für verschiedene deutsche Stichproben nachgewiesen werden und es wurden bereits Überlegungen angestellt, woran das liegen könnte. Anhand heutiger Daten ist die Replikation nicht mehr möglich, weil es sich um ein Artefakt handelte: Bis vor einigen Jahrzehnten wurden alle Kinder dazu erzogen, mit rechts zu schreiben und zu schneiden. Schließlich stoppte die Umerziehung. Um in den darauffolgenden Jahren als linkhändrige Person in den Sterbestatistiken aufzutauchen, musste man ziemlich jung gestorben sein - denn ältere Personen, die mit links schreiben, gab es ja aufgrund der Umerziehung nicht. Das hatte zur Folge, dass linkshändrige Tote im Mittel ganze 10 Jahre jünger waren als rechtshändrige Tote. Replizierbarkeit ist also nicht hinreichend für Validität oder Wahrheit, aber um die Gültigkeit einer Hypothese zu unterstreichen oder ihren Wahrheitsanspruch zu verteidigen ist Replizierbarkeit notwendig.\nFletcher (2021) listet die Bedingungen dafür auf, dass sich etwas replizieren lässt:\n\nEs liegen keine Fehler in der Datenanalyse vor.\nDer Befund ist nicht auf statistische Unschärfe zurückzuführen.\nDer Befund hängt nicht von vernachlässigten Hintergrundfaktoren ab.\nEs lag kein Betrug oder anderes wissenschaftliches Fehlverhalten vor.\nDer Befund lässt sich auf eine Grundgesamtheit verallgemeinern, die größer als die Stichprobe der Originalstudie ist.\nDie Hypothese ist auch dann noch gültig, wenn sie auf eine völlig andere Weise geprüft wird. \n\n\n\n\n\n\n\nNotwendige und hinreichende Bedingung\n\n\n\nReplizierbarkeit ist eine notwendige aber keine hinreichende Bedingung für Allgemeingültigkeit, was ist damit gemeint? Die Begriffe “notwendig” und “hinreichend” sind vielen Personen wahrscheinlich aus der mathematischen Kurvendiskussion bekannt. Ihre genaue Bedeutung ist vor allem in der Logik relevant:\n\nNotwendig heißt “es geht nicht ohne”. Kakaopulver zu haben, ist notwendig dafür, dass ich einen Kakao zubereiten kann heißt, ich kann keinen Kakao zubereiten, wenn ich kein Kakaopulver habe. Gleichzeitig ist es nicht ausreichend oder hinreichend: Nur, weil ich Kakaopulver habe, heißt das nicht, dass ich Kakao zubereiten kann. Vielleicht fehlt noch Milch, Wasser, eine Tasse, oder eine Mikrowelle.\nHinreichend heißt: Wenn es da ist, dann reicht das schon und keine weiteren Bedingungen müssen erfüllt sein. Wenn ich ein Flugzeug am Himmel höre, dann ist das hinreichend dafür, dass am Himmel ein Flugzeug entlang fliegt. Es ist aber auch möglich, dass ein Flugzeug am Himmel entlang fliegt, ohne dass ich es höre (zum Beispiel, weil es sehr hoch fliegt, die Umgebungsgeräusche sehr laut sind, oder es ein leiser Segelflieger ist).\n\nAnders als in manchen Beispielen, muss die Bedingung nicht immer vor dem Ereignis auftreten. Es geht also um keinen Kausalzusammenhang, bei dem eines zum anderen führt. Eine Besonderheit bei Bedingungen ist, dass wenn A notwendig für B ist, dann ist B hinreichend für A. Dass ich einen Kakao zubereitet habe, ist also hinreichend dafür, dass ich Kakaopulver habe. Und, dass ein Flugzeug am Himmel entlang fliegt ist notwendig dafür, dass ich es hören kann.\n\n\n\n\nWer repliziert?\nTrotz starker Bemühungen sind Replikationsstudien immer noch relativ selten. Schätzungen reichen von 5% bis unter 0.1% je nach Forschungsdisziplin. Eigenen Angaben zufolge, haben die meisten Forschenden zwar mal eine Replikation durchgeführt, konnten aber die Originalbefunde nicht bestätigen (Baker 2016). Ein Großteil der Befunde aus der folgenden Tabelle stammen aus einem Tweet von Gilad Feldman.\n\nÜberblick darüber, wo wieviel repliziert wird\n\n\n\n\n\n\n\nDisziplin\nQuelle\nBefund\n\n\n\n\nWirtschaftswissenschaften\nAnkel-Peters, Fiala, und Neubauer (2023)\nJe nach Zeitschrift ist der Anteil an Replikationen und Reproduktionen zwischen 0 und 11%.\n\n\nWirtschaftswissenschaften\nMueller-Langer u. a. (2019)\n0,1% der Publizierten Studien sind Replikationen.\n\n\nExperimentelle Linguistik\nKobrock und Roettger (2023)\nWeniger als 0,001% der Studien beinhalten Replikationen.\n\n\nPsychologie (1900 - ca. 2012)\nMakel, Plucker, und Hegarty (2012)\nCirca 1.07% aller Artikel seit 1900 beinhalten Replikationen, der Anteil ist zuletzt gestiegen.\n\n\nPsychologie (2014 - 2017)\nHardwicke u. a. (2022)\nReplikationen werden in 3-8% der Studien berichtet.\n\n\nPsychologie (2010 - 2021, High Impact Zeitschriften)\nClarke u. a. (2023)\n0,2% (169 von 84.834) Artikel beinhalteten direkte Replikationen.\n\n\nÖkologie und Evolution\nKelly (2019)\nWeniger als 1% der Studien werden als Replikationen bezeichnet.\n\n\nZweitsprachenforschung\nMarsden u. a. (2018)\n0,25% der Studien sind Replikationen.\n\n\nErziehungswissenschaften\nMakel und Plucker (2014)\n0,13% der Artikel beinhalteten Replikationen.\n\n\nSonderpädagogik\nMakel u. a. (2016)\n0,5% der Artikel beinhalteten Replikationsstudien.\n\n\nKriminologie\nMcNeeley und Warner (2015)\nEtwas über 2% der Artikel beinhalten Replikationsstudien.\n\n\nVerhaltensökologie\nKelly (2006)\nReplikationen werden selten durchgeführt.\n\n\n\n\n\nAnsehen von Replikationsstudien\nReplikationen von Bem’s berüchtigter Studie zum Erfühlen der Zukunft wurden bei der Zeitschrift, bei der sie veröffentlicht wurde, ohne Begutachtungsprozess abgelehnt (“Desk Reject”). Der Editor erklärte, die Zeitschrift veröffentlicht keine Replikationsstudien, egal was dabei herauskam, da sie nicht die Zeitschrift der Bem Replikationen sein möchten (Daniel Lakens 2023). Unter den 3185 Zeitschriften mit TOP Faktor gaben im Jahr 2024 341 Zeitschriften (10.7%) an, dass sie Replikationen akzeptieren. Sie werden als Konfrontation der aktuellen theoretischen Erwartungen mit aktuellen Daten definiert und ihre wichtige Rolle im wissenschaftlichen Prozess mehr und mehr anerkannt (Brian A. Nosek und Errington 2020). Ein Großteil dieser Entwicklung ist von der Sozialpsychologie getragen, andere Felder verändern sich kaum oder nur langsam (Torka, Mazei, und Hüffmeier 2023), doch selbst dort sind Diskussionen von Replikationsbefunden zum Teil noch hart und die Replikationen werden falsch dargestellt und ungerechtfertigt kritisiert (Chandrashekar und Feldman, o. J.).\nIn den Wirtschaftswissenschaften hat sich das Insitute for Replications (I4R, https://i4replication.org/reports.html) gebildet, welches sogenannte Replication Games organisiert, bei denen Studien reproduziert und repliziert wurden. Anknüpfend an eine Forderung (Zimmermann 2015) wurde das Journal of Comments and Replications in Economics (JCRe) gegründet. Ab 2024 konzentriert sich das I4R darauf, Replikationen zu spezifischen Zeitschriften durchzuführen. Ironischerweise tut es das nicht für Open Access Zeitschriften, sondern beispielsweise für Nature (Brodeur u. a. 2024). Das Journal of Applied Econometrics veröffentlicht ebenfalls Replikationen zu Studien aus zahlreichen ökonomischen Zeitschriften.\n\n\nArten von Replikationsprojekten\nWährend ein Großteil der Replikationsstudien klassischen Studien entspricht, haben sich Spezialmodelle entwickelt. Beispielsweise können Forschende mit StudySwap Gruppen suchen, die ihre Ergebnisse vor der Veröffentlichung replizieren [Daniel Lakens (2023); p. 11]. Im Rahmen von Registered Replication Reports konzentrieren sich große Teams an mehreren Orten auf eine zuvor abgestimmte Studie und führen sie gleichzeitig überall durch. Einer der fruchtbarsten Ansätze ist die Nutzung von Abschlussarbeiten für Replikationen. Im Rahmen eines Studiums müssen alle Studierende eine Abschlussarbeit (z.B. Bachelorarbeit oder Masterarbeit) ablegen. Anhand von Replikationen können sie lernen, eine wichtige Studie nachzubilden und prüfen gleichzeitig die Robustheit der Originalbefunde. Während Quintana (2021) vorschlug, Abschlussarbeiten so zu verwenden, ist es bereits gängige Praxis in den vergleichenden Politikwissenschaften (Korell, Reinecke, und Lott 2023b), in der Psychologie (Jekel u. a. 2020; Feldman 2021; Boyce, Mathur, und Frank 2023; Wagge u. a. 2019), und in den Wirtschaftswissenschaften (https://home.uni-leipzig.de/lerep/). Am Wissenschaftszentrum Berlin für Sozialforschung (https://www.wzb.eu/de/forschung/markt-und-entscheidung/verhalten-auf-maerkten/labsquare) und beim Sports Science Replication Center (https://ssreplicationcentre.com) starteten 2024 ebenfalls Replikationsreihen.\n\nTypen von Replikationen\n\n\n\n\n\n\n\nTyp\nErklärung\nBeispiel\n\n\n\n\nRegistered Replication Report\nForschende an vielen verschiedenen Orten führen dieselbe Replikationsstudie durch (Simons, Holcombe, und Spellman 2014).\nCheung u. a. (2016)\n\n\nInternal Replication\nEine Forschungsgruppe berichtet in einem Forschungsartikel eine eigene Replikation einer ihrer Studien.\nOngchoco, Walter-Terrill, und Scholl (2023)\n\n\nClose Replication\nEine Orginalstudie soll möglichst ähnlich von anderen Forschenden durchgeführt werden.\nXiao u. a. (2021)\n\n\nConceptual Replication\nAndere Forschende prüfen dieselbe Hypothese erneut, verwenden dabei aber absichtlich andere Methoden.\nSobkow u. a. (2021)\n\n\n\n\n\nWer veröffentlicht Replikationen?\nSrivastava (2012) schlug eine “Pottery Barn” Regel für wissenschaftliche Zeitschriften vor. Unter dieser Regel versteht man das in Töpfereien übliche Regel “wer es kaputt macht, muss es kaufen”. Für Replikationen hieße das, dass eine Zeitschrift alle Replikationen zu einer Studie, die sie veröffentlicht hat, ebenfalls veröffentlichen muss. Tatsächlich tut das keine Zeitschrift - möglicherweise aus Furcht, ihren Impact Factor zu reduzieren oder der unzureichenden wissenschaftlichen Qualitätssicherung beschuldigt zu werden. Abgesehen von den Replikationsszeitschriften JCRe (https://www.jcr-econ.org) und Rescience X (rescience.org/x) gibt es vereinzelte Zeitschriften, die Replikationsstudien veröffentlichen. In der Psychologie sind das beispielsweise Meta-Psychology und das Journal of Trial and Error.\nUm die Auffindbarkeit von Replikationen dennoch zu erhöhen, sind Replikationendatenbanken entstanden. Dabei sammeln Forschende Replikationsstudien und bereiten sie nutzerfreundlich auf. Für Wirtschaftswissenschaften hat Jan Höffler mit dem Replication Wiki (https://replication.uni-goettingen.de/wiki/) grundlegende Arbeit geleistet (Höffler 2017). In der Psychologie stellte LeBel curatescience.org vor. Das Projekt starb schnell wieder aus, die Idee wurde von FORRT mit “Replications and Reversals” (forrt.org/reversals/) aufgegriffen und 2023 mit der “Replication Database” zur “FORRT Replication Database” vereinigt. Mit meta-analytischen Funktionen und der Möglichkeit, Quellenverzeichniss auf Replikationsstudien zu prüfen ist die FORRT Replication Database aktuell das umfangreichste Projekt (forrt.org/replication-hub). Der Großteil dieser Projekte wird von großen Communities gestemmt, bei welcher die Teilnehmenden mühselig per Hand Daten beitragen. Automatisierte Methoden sind in Entwicklung, allerdings noch unausgereift (Ruiter 2023).\n\n\nWas ist eine gute Replikation?\nÜber viele Fächer hinweg werden Replikationsmethoden entwickelt. Vorangehend mit der Psychologie (Brandt u. a. 2014; Hüffmeier, Mazei, und Schultze 2016) gibt es Richtlinien für Replikationen in quantiativer Soziologie (Freese und Peterson 2017), Geisteswissenschaften (Schöch 2023), und Marketing (Urminsky und Dietvorst 2024). Methoden zur Stichprobenplanung (Simonsohn 2015; Pawel, Consonni, und Held 2023; Bourazas, Consonni, und Deldossi 2024) und zur Auswahl (Isager u. a. 2024; Howard und Maxwell 2023), Bewertung (Yeung und Feldman 2023), und Kommunikation (Janz und Freese 2021) von Replikationsstudien wurden entwickelt. Unten werden Empfehlungen in Form eines Replikationsleitfadens strukturiert.\n\n\n\n\n\n\nKurzleitfaden für die Durchführung von Replikationsstudien\n\n\n\n\nWahl der Studie\nDie zu replizierende Studie sollte relevant und anzweifelbar sein. Relevanz kann sich durch viele Zitationen äußern oder dadurch, dass viel Forschung auf dem Befund aufbaut. Existieren bereits viele Replikationen und ist klar, dass Replizierbarkeit gegeben oder nicht gegeben ist, ist eine Replikationsstudie wenig informativ. Aufgrund der Replikationskrise ist die Anzweifelbarkeit meistens gegeben (z.B. durch P-Werte nah an der 5% Schwelle). Mittels meta-analytischer Auffälligkeiten (Adler, Röseler, und Schöniger 2023) lässt sich die Anzweifelbarkeit ebenfalls prüfen.\nBei der Auswahl empfiehlt es sich zudem, bisherige Diskussionen (Kommentare in Zeitschriften oder auf Pubpeer.com) zu berücksichtigen, falls es sie gibt. Im Rahmen von Abschlussarbeiten ist außerdem die Machbarkeit zu berücksichtigen: Eine Längsschnittstudie, die 10 Jahre dauert, ist nicht gut machbar. Ebenfalls sollte es für den entsprechenden Bereich etablierte Replikationsstandards geben. Für Daten von Gehirnscannern ist das beispielsweise noch nicht der Fall, da dort hunderte bis tausende von Korrelationen verglichen werden und die Originalwerte häufig nicht verfügbar sind.\nDurchführung\nEine Bestandsaufnahme aller öffentlicher Materialien ist empfehlenswert. Wenn möglich, sollten Originalergebnisse reproduziert und geprüft werden. Auch ohne Daten lassen sich zum Beispiel via statcheck.io (Nuijten u. a. 2016) Werte auf Korrektheit prüfen. Bei jüngeren Studien können die Originalautor*innen Daten und Materialien zur Verfügung stellen.\nEine besondere Herausforderung ist in der Forschung die Planung des Stichprobenumfanges (statistische Power). Zwar ist das Problem für Replikationsstudien deshalb am geringsten, weil es dort schon einen Befund zur Orientierung gibt, allerdings ist der meistens zu unscharf, um den für die Berechnungen zu verwenden. Der Small Telescopes Approach (Simonsohn 2015) hilft hierbei aus und ggf. muss auf Äquivalenztests zurückgegriffen werden (Daniël Lakens 2017).\nUm die Anpassung der Originalmethode kommen Replizierende nur selten: Materialien sind veraltet, müssen in eine andere Sprache übersetzt werden, oder an eine besondere Stichprobe von Personen angepasst werden. Dabei können ähnliche Studien helfen, die mittels Replikationsdatenbank aufgespürt werden können (Röseler, Kaiser, u. a. 2024). Häufig bieten sich auch Erweiterungen (Extensions) an, die den Informationsgehalt der Studie erhöhen.\nAnalyse\nÄhnlich wie bei den Methoden ist es häufig sinnvoll, die Originalanalyse nachzubilden (konfirmatorisch) und anschließend weitere Tests durchzuführen (exploratorisch). Ein Vergleich der Ergebnisse - auch im Hinblick auf methodische Unterschiede - erlaubt dann ein umfassendes Bild.\nDiskussion\nUnter welchen Bedingungen die Replikation als erfolgreich interpretiert wird, sollte zuvor in einer Präregistrierung festgelegt werden. Vorschläge für Kriterien machen Brandt u. a. (2014), LeBel u. a. (2019), und Anderson, Kelley, und Maxwell (2017). Abweichungen von der Präregistrierung und der Originalstudie sollten dabei ausgiebig diskutiert werden. Ob ein Kommentar der Autor*innen der Originalstudie hilfreich ist, ist umstritten, obgleich Kommentare für die Veröffentlichung bei manchen Zeitschriften nötig ist.\nBericht\nUmfangreiche Berichte von Feldman und Kolleg*innen (Ziano, Mok, und Feldman 2021) bieten gute Orientierung für eigene Manuskripte. Für kurze Berichte ist ebenfalls ein standardisiertes Formular verfügbar, mittels welchem Befunde in die Replikationendatenbank eingetragen werden können. Brandt u. a. (2014) empfehlen außerdem eine Registrierung der Ergebnisse, welche die Weiterverwendung ermöglicht (Röseler u. a. 2022). Zuletzt ist eine Veröffentlichung des Pre-Prints empfehlenswert. Für Sozialwissenschaften und Medizin befindet sich zur Zeit eine interdisziplinäre Replikationszeitschrift in der Vorbereitungsphase, bei der die Studie zur Veröffentlichung eingereicht werden kann.\n\n\n\n\n\n\nModellierung von Replizierbarkeit\nAlle Studien in einem Wissenschaftsbereich zu replizieren ist aktuell unrealistisch und würde enorme Ressourcen benötigen. Daher befinden sich verschiedene Methoden in Entwicklung, um auf Basis von Eigenschaften einer Studie vorherzusagen, welche Replikationsergebnis zu erwarten ist. Im repliCATS Projekt, das Teil des SCORE Projektes zur Messung wissenschaftlicher Qualität ist, werden Einschätzungen von Forschenden, Reproduktionsversuche, und Replikationsversuche kombiniert. Ähnliche Projekte verwenden komplexe statistische Modelle (z.B. Large-Language-Models) oder meta-analytische Methoden wie p-curve oder Z-curve zur Vorhersage von Replikationsraten. Solche Modelle benötigen üblicherweise sehr viele Beobachtungen und Vorhersagen sind nur auf der Ebene von hunderten Studien sinnvoll. Beispielsweise rechneten Boyce, Mathur, und Frank (2023) und Hagen Cumulative Science project (Jekel u. a. 2020) Moderationsanalysen, prüften also welche Eigenschaften von Studien sich auf das Replikationsergebnis auswirken (z.B. Wechsel von Studie im Labor zu Online-Studie). Mittels umfassender Datenbanken wie der FORRT Replication Database (Röseler, Kaiser, u. a. 2024) können in Zukunft präzisere Modelle erstellt werden. Aktuelle Ergebnisse sind als vorläufig und mit Vorsicht zu interpretieren, da sie auf nicht-zufälligen Stichproben basieren, die Studieneigenschaften nicht systematisch und zufällig variiert wurden, und Replikationen solcher meta-analytischen Befunde schwierig sind.\n\n\nVeröffentlichung aller Ergebnisse\nUm das lange bekannte File-Drawer-Problem (Sterling 1959; Rosenthal 1979) zu lösen gibt es intensive Bemühungen, statistische Modelle zu entwickeln, die die Verzerrung “herausrechnen” können. Keines der aktuellen Modelle funktioniert für alle Daten (Carter u. a. 2019), weshalb Forschende aktuell nicht daran vorbei kommen, alle Ergebnisse zu veröffentlichen.\nIn der Medizin ist der besondere Fall, dass Studien am Menschen öffentlich registriert werden müssen, bevor sie durchgeführt werden. Es existiert also eine öffentlich einsehbare Datenbank, mittels derer nachvollzogen werden kann, wer zu welchem Zeitpunkt eine Studie durchgeführt hat. Gleichzeitig müssen die Registrierungsnummern bei der Veröffentlichung von Artikeln angegeben werden. Werden beide Daten kombiniert, können wir sehen, wie viele Studien innerhalb eines bestimmten Zeitraums nach der Registrierung veröffentlicht werden - und das sogar aufgeteilt nach Personen und Institutionen (Quest Dashboard; https://quest-cttd.bihealth.org/). Im Open Trials Projekt (https://opentrials.net/about/) wird zudem daran gearbeitet, Informationen zu Registrierungen und Forschungsartikeln aus verschiedenen Datenbanken zu verknüpfen.\nIn anderen Fächern besteht keine Pflicht zur Durchführung von Studien, sodass völlig unklar ist, welche und wie viele Studien “in der Schublade landen”. Die Zeitschrift Meta-Psychology bietet ein Artikel-Format für psychologische File-Drawer-Reports (Schubladenberichte) an, in denen Forschende alle Studien zu einem bestimmten Thema veröffentlichen können und dabei auch fehlgeschlagene oder fehlerhafte Studien berichten. Das Journal of Negative Results (https://www.jnr-eeb.org/index.php/jnr/about) und das Journal of Articles in Support of the Null Hypothesis (https://www.jasnh.com) sind spezialisiert auf Ergebnisse, die die nicht den Erwartungen entsprechen. Im Rahmen des Projektes PsychFileDrawer existierte eine Zeit lang eine online Datenbank für psychologische Studien, die nicht veröffentlicht wurden, und “All Results Journals” veröffentlichten Ergebnisse aus den Naturwissenschaften (http://www.arjournals.com). Von letzteren ist allerdings nur noch die Biologie Zeitschrift aktiv, während Physik und Chemie seit langer Zeit nichts publiziert haben.\nThemenspezifisch haben sich vor allem in der Psychologie Datenbanken etabliert, die alle Studien zu einem Thema zusammenfassen, mittels derer sich Studien durchsuchen und manchmal sogar statistisch zusammenfassen lassen. Solche Zusammenfassungen sind häufig Meta-Analysen (Studien über Studien bzw. Analysen von vielen Studienergebnissen) und mithilfe der Datenbanken sind sie dynamisch, es lassen sich also Studien filtern, Datenbanken wachsen über die Zeit, und Benutzer*innen können die Analysen selbst bestimmen. Durch die zentrale Rolle von statistischen Ergebnissen sind sie vielversprechend in Bezug auf kumulative Wissenschaft, das heißt, sie erleichtern es Forschenden, aufeinander aufzubauen. Neben den Themen und Funktionen unterscheiden sich die Datenbanken außerdem darin, wie sie Ergebnisse sammeln. Manche stammen von einzelnen Forschenden und andere sind durch “Crowd Sourcing” entstanden, das heißt, eine Gruppe stellt die Infrastruktur der Datenbank bereit und andere Forschende senden ihre Daten dort hin. Das hat den Vorteil, dass die Arbeit geteilt wird und die Beitragenden ihre Daten durch die Veröffentlichung in der Datenbank sichtbarer machen. In der folgenden Tabelle sind einige themenspezifische Datenbanken aufgelistet.\n\n\n\nName\nLink\nTopics\n\n\nMetaLab\nlangcog.github.io/metalab\nSprachentwicklung, kognitive Entwicklung\n\n\nmetaBUS\nmetabus.org\nSozialwissenschaften\n\n\nSOLES\ncamarades.shinyapps.io/AD-SOLES/\nTiermodelle von Alzheimer\n\n\nOpAQ\nt1p.de/openanchoring\nAnker Effekte\n\n\nFReD\nt1p.de/ReD\nReplikationsstudien\n\n\nPower Posing\nmetaanalyses.shinyapps.io/bodypositions\nEffekte der Körperhaltung\n\n\nMetadataset\nmetadataset.com\nLandwirtschaft\n\n\nMETAPSY\nhttps://www.metapsy.org\nPsychotherapie\n\n\n\nInzwischen werden auch Werkzeuge entwickelt, die Forschenden helfen, solche Datenbanken zu erstellen. MetaUI und Dynameta erleichtern das erstellen einer Website, auf welche Daten interaktiv analysiert und heruntergeladen werden können (metaUI, https://github.com/lukaswallrich/metaui; Dynameta, https://github.com/gls21/Dynameta). Weitere Materialien erarbeitet das Project PsychOpen CAMA (https://cama.psychopen.eu).\n\n\nUmgang mit Fehlern\nWissenschaftliche Fehler können massive Folgen haben. Beispielsweise hatte der Wakefield Skandal, bei welchem Eltern bezahlt wurden, falsche Aussagen über die Entwicklung ihrer Kinder nach einer Impfung zu tätigen, eine Impfskepsis zur Folge. Das bedeutet, dass Eltern ihre Kinder aus Sorge vor den Folgen nicht impfen lassen und die Kinder dadurch erkranken und sterben können, obwohl der Ursprung der Sorge falsch war und längst und vielfach widerlegt wurde (DeStefano und Shimabukuro 2019). In vielen Wissenschaften herrscht eine angespannte Fehlerkultur: Sobald jemand einen Fehler herausstellt, wird es als persönlicher Angriff verstanden und Kritiker werden beleidigt (Baumeister und Vohs 2016). Durch die strenge Hierarchien kann es für Forschende, die noch keine Festanstellung haben, fatal sein, Professor*innen zu kritisieren, da sie ihre Artikel begutachten und über ihre Arbeitsverträge entscheiden können. Im Fall Wakefield dauert es selbst nachdem der Fehler klar war, viele Jahre, bis die Artikel öffentlich von der Zeitschrift zurückgezogen wurden (Eggertson 2010). Und zuletzt merken Forschende gar nicht, wenn Artikel zurückgezogen werden, außer sie suchen aktiv danach.\nMit verschiedenen Ansätzen soll die Fehlerkultur offener gestaltet werden und diese Probleme gelöst werden. Eigentlich sollte allen klar sein, dass Fehler immer wieder passieren und alle davon profitieren, wenn die Fehler korrigiert werden. In der Wissenschaftspraxis profitieren nur leider nicht immer alle, sondern die Person, die den Fehler begangen hat, hat dank dieses Fehlers vielleicht einen Nobelpreis oder eine Professur bekommen. Um den Diskurs darüber anzukurbeln, haben Psycholog*innen aus der Schweiz ein Kopfgeld-Programm gestartet (https://error.reviews), bei welchem sich Personen bewerben und anschließend mit der Findung von Fehlern anderer Geld verdienen, oder in der Rolle der Begutachtenden Geld verdienen, wenn sie keine oder nur kleine Fehler begangen haben. Plattformen wie Pubpeer.com erlauben außerdem die anonyme Kommentierung von Forschung. Von besonderem Interesse ist dort Forschung von renommierten Personen, wie zum Beispiel dem Nobelpreisträger Thomas Südhof, auf Basis dessen Forschungsartikeln Diskussionen mit teilweise über 50 Kommentaren geführt werden. Kommentare bei Pubper haben außerdem zu zahlreichen Retractions (also “Zurückziehungen” veröffentlichter Artikel) geführt. Retractions werden in der Retractiondatabase (retractiondatabase.org) gesammelt. Mittels eines Browser-Plugins können Forschende sich Artikel, zu denen es Pubpeer-Diskussionen gibt, hervorheben lassen. Um Retractions sichtbarer zu machen, wird aktuell (Sommer 2024) eine Studie durchgeführt, bei der Personen, die in Pre-Prints zurückgezogene Artikel zitieren, eine E-Mail-Benachrichtigung erhalten (RetractoBot, https://www.retracted.net).\nIn den Fällen, in denen es zu Retractions kommt, wird ein kurzer Text veröffentlicht, in dem erklärt wird, weshalb ein Artikel zurückgezogen wird. Solche Texte sind kaum standardisiert und oft intransparent. Das Committee of Publishing Ethics (https://publicationethics.org) hat Empfehlungen erarbeitet, unter welchen Bedingungen Artikel zurückgezogen werden und Ivory und Elson (2024) empfehlen standardisierte Texte, um die Gründe für die Retraction darzulegen.\nVereinzelt prüfen Wissenschaftler*innen große Mengen an Artikeln. Am berühmtesten ist darunter Elisabeth Bik. Sie hat zu Retractions von fast 600 Artikeln und Korrekturen von fast 500 Artikeln beigetragen (https://www.buzzfeednews.com/article/stephaniemlee/elisabeth-bik-didier-raoult-hydroxychloroquine-study). Dabei prüft sie Abbildungen in Forschungsartikeln aus der Biologie. Beispielsweise werden dabei sogenannte Western Blots beim Überführen von Artikeln in das Zeitschriftenformat, beim Erstellen von Abbildungen durch die Forschenden, oder sogar boswillig kopiert.\n\n\n\n\n\n\nErfahrungsbericht: Vom Fehler zur Korrektur\n\n\n\nIm Jahr 2021 habe ich einen Forschungsartikel zu einem Datensatz geschrieben, der durch die Zusammenarbeit von 99 Forschenden erstellt werden konnte (Röseler, Weber, und Schütz 2021). 2022 wurde der Artikel beim Journal of Open Psychology Data veröffentlicht. Es war mir jedoch ein Fehler unterlaufen. Wie der erkannt und behoben wurde, beschreibe ich in der folgenden Historie:\n\nOktober 2022: Während der Revision (Überarbeitung) des Artikels kamen Daten von weiteren Forschenden hinzu. An dem Verfassen des Artikels waren bis dahin 64 Leute beteiligt, die Liste stieg dann auf 74 Personen. Ich sendete das überarbeitete Manuskript mit einer Tabelle der 74 Leute an die Zeitschrift. In dem System konnten Autor*innen zusätzlich auch in dafür vorgesehene Felder eingetragen werden. In den Feldern standen also die 64 Personen und im Artikel die 74. Der eingereichte Artikel wurde dann akzeptiert und seitens der Zeitschrift in das entsprechende Format überführt. Auch die Tabelle wurde überführt, nur löschte dabei jemand gezielt die zehn in die Tabelle (alphabetisch) einsortierten Personen unter Abgleich mit den Personen, die bei der ersten Einreichung in das System eingetragen wurden. Ich bekam den fertigen Artikel und hatte eine Woche Zeit, dazu Rückmeldung zugeben. Mir fielen die fehlenden Personen nicht auf.\n2023 kontaktierte mich einer der fehlenden Autoren: Seine Doktorandin wollte die Quellenangabe auf ihren Lebenslauf schreiben, fand sich aber unter den Beteiligten nicht wieder. Dabei fiel auf, dass das Team aus 6 Personen komplett fehlte. Ich schrieb dem Herausgeber der Zeitschrift, entschuldigte mich für den Fehler, und fragte, ob eine Änderung noch möglich sei. Das war nicht der Fall, aber eine Korrektur war möglich.\nIch entschied mich für die Korrektur. Die Zeitschrift hatte in den vergangenen Monaten die Software für die Verwaltung von Einreichungen verändert und ich musste für die neue Einreichung alle 74 Personen erneut eintragen. Dies dauert inklusive Prüfungen ungefähr einen vollständigen Arbeitstag, allerdings forderte das System die Nationalitäten aller Beteiligten - und die wusste ich nicht. Ungefähr die Hälfte von ihnen waren Studierende, viele weitere hatten die Institution gewechselt, es war mir also nicht mehr möglich, die Nationalitäten aller Beteiligten in Erfahrung zu bringen. Ich wies auf das Problem hin und wartete darauf, dass bei der Korrektur eine Ausnahme gemacht werden konnte. Gleichzeitig begann meine 6-monatige Elternzeit, inklusive Umzug und Jobwechsel. Nach meiner Rückkehr kontaktierte ich einen der Herausgeber erneut. Es folgten weitere sieben Monate der Stille - mir wurde trotz mehrmaligem Nachfragen nicht geantwortet.\nIm Januar 2024 erstellte ich einen Pubpeer-Kommentar. Ich verlor die Hoffnung, dass der Prozess jemals ein Ende finden würde, wollte aber auf den Fehler aufmerksam machen. Dort war die korrekte Liste der Beteiligten, die auch der Zeitschrift vorlag.\nIm Juli 2024 schrieb ich mehreren Herausgebern erneut. Diesmal wurde mir geantwortet. Der Zeitschrift lagen plötzlich Nationalitäten der Beteiligten vor, die jemand bei der Original-Veröffentlichung händisch eingetragen haben musste. Ich konnte nun alle Autor*innen erneut und mit Nationalität eintragen. Mir fiel dabei auf, dass außer den sechs fehlenden noch vier weitere fehlten, die sich nicht bei mir gemeldet hatten. Ich berichtete allen Betroffenen über die Vorgänge und entschuldigte mich nocheinmal. Seitens der Zeitschrift wurde dann sehr schnell der formatierte Artikel vorbereitet. Wieder hatte ich eine Woche Zeit für Korrekturvorschläge. Gemeinsam mit Kollegen und Hilfskräften merkten wir circa 10 weitere Fehler bei Namen und Institutionen an und baten darum, die korrigierte Version erneut an uns zu senden, damit wir uns nicht wieder jahrelang um eine Korrektur bemühen müssen.\nIm August 2024 war dsa Copy-Editing abgeschlossen und die Korrektur ging zur Veröffentlichung (Röseler, Weber, u. a. 2024).\n\n\n\n\n\nOffene Lehre / Open Teaching / Open Educational Resources\nUnter dem Stichwort Open Educational Resources werden jegliche Materialien verstanden, die zur Lehre verwendet werden können. Das kann Bücher - so wie dieses - Forschungsartikel, Präsentationsfolien, Vorlesungsskripte (z.B. zum Thema Mikroökonomie) aufgezeichnete Lehrveranstaltungen (z.B. zum Thema Wissenschaftstheorie), oder Erklärvideos betreffen. Open bedeutet dabei meistens, dass sie sich gratis aus dem Internet herunterladen lassen. Diese Ressourcen zu teilen trifft den Kern von Open Science: Es soll niemand ausgeschlossen werden, nicht durch mangelnde Verfügbarkeit an einem Ort, hohe Preise, oder die Sprache. Häufig laden Open Science Verfechter*innen ihre Materialien in Portalen hoch (z.B. https://www.oerbw.de, https://www.twillo.de/oer/web/, https://portal.hoou.de, https://oercommons.org). Plattformen wie Zenodo (https://zenodo.org) oder das Open Science Framework (osf.io) erlauben eine kostenlose Langzeitarchivierung - also garantierte Verfügbarkeit von mindestens 20 bzw. 50 Jahren).\nUnter den verschiedenen Akteuren im Bereich Open Educational Resources ist besonders FORRT hervorzuheben: In Educational Nexus werden spezifisch zu Open Science viele verschiedene Kurse und Unterlagen bereitgestellt (https://forrt.org/syllabus/). Es werden online Seminare durchgeführt, ein mehrsprachiges Glossar und eine Wissensdatenbank verwaltet (Parsons u. a. 2022). Weitere nennenswerte Wissensbanken sind die ROSiE Knowledge Hub und der Open Economics Guide.\n\nWeiterführende Informationen\n\nHenderson und Chambers (2022) beschreibt zehn einfache Regeln zum Schreiben eines Registered Reports\nDaniël Lakens u. a. (2024) diskutieren die Vorteile von Registered Reports und Präregistrierungen.\nEin Erklärvideo zu PCIs ist online verfügbar (https://www.youtube.com/watch?v=4PZhpnc8wwo).\nBerichte, wie Replikationen in verschiedenen Disziplinen aussehen, berichtet Arts und Sciences (2018).\nMittels einem Wiki und einem Webinar können Studierende über das ReplicationWiki über Replikationen lernen.\nJohanna Gereke und Anne-Sophie Waag haben in einem Vortrag Open Science in der universitären Lehre diskutiert.\nLevendis (2018) nutzt in seinem Lehrbuch zu Zeitreihenanalysen ausschließlich Reproduktionen von Analysen aus tatsächlichen Publikationen.\nEinen weltweiten Überblick über Open Educational Resources (Offene Lehrmaterialien) bietet die OER World Map: https://oerworldmap.org\n\n\n\n\nLiteratur\n\n\n\n\nAczel, Balazs, Barnabas Szaszi, und Alex O Holcombe. 2021. „A billion-dollar donation: estimating the cost of researchers’ time spent on peer review“. Research Integrity and Peer Review 6: 1–8.\n\n\nAdler, Susanne Jana, Lukas Röseler, und Martina Katharina Schöniger. 2023. „A toolbox to evaluate the trustworthiness of published findings“. Journal of Business Research 167: 114189. https://doi.org/10.1016/j.jbusres.2023.114189.\n\n\nAnderson, Samantha F., Ken Kelley, und Scott E. Maxwell. 2017. „Sample-Size Planning for More Accurate Statistical Power: A Method Adjusting Sample Effect Sizes for Publication Bias and Uncertainty“. Psychological Science 28 (11): 1547–62. https://doi.org/10.1177/0956797617723724.\n\n\nAnkel-Peters, Jörg, Nathan Fiala, und Florian Neubauer. 2023. „Do economists replicate?“ Journal of Economic Behavior & Organization 212: 219–32.\n\n\nArts, Royal Netherlands Academy of, und Sciences. 2018. „Replication studies—Improving reproducibility in the empirical sciences“. In. KNAW Amsterdam.\n\n\nAzevedo, Flavio, Sam Parsons, Leticia Micheli, Julia Feld Strand, Eike Mark Rinke, Samuel Guay, Mahmoud Medhat Elsherif, u. a. 2019. „Introducing a Framework for open and Reproducible Research Training (FORRT)“.\n\n\nBaker, Monya. 2016. „1,500 scientists lift the lid on reproducibility“. Nature 533 (7604): 452–54.\n\n\nBalliet, Daniel, Laetitia B Mulder, und Paul AM Van Lange. 2011. „Reward, punishment, and cooperation: a meta-analysis.“ Psychological bulletin 137 (4): 594.\n\n\nBaumeister, Roy F., und Kathleen D. Vohs. 2016. „Misguided Effort With Elusive Implications“. Perspectives on psychological science : a journal of the Association for Psychological Science 11 (4): 574–75. https://doi.org/10.1177/1745691616652878.\n\n\nBeall, Jeffrey. 2017. „What I learned from predatory publishers“. Biochem. Med. (Zagreb) 27 (2): 273–78.\n\n\nBilder, Geoffrey, Jennifer Lin, und Cameron Neylon. 2020. „The principles of open scholarly infrastructure“. The Principles of Open Scholarly Infrastructure.\n\n\nBoulter, Etienne, Julien Colombelli, Ricardo Henriques, und Chloé C Féral. 2022. „The LEGO brick road to open science and biotechnology“. Trends Biotechnol. 40 (9): 1073–87.\n\n\nBourazas, Konstantinos, Guido Consonni, und Laura Deldossi. 2024. „Bayesian sample size determination for detecting heterogeneity in multi-site replication studies“. TEST, 1–20.\n\n\nBoyce, Veronica, Maya Mathur, und Michael C. Frank. 2023. „Eleven years of student replication projects provide evidence on the correlates of replicability in psychology“. Royal Society Open Science 10 (11): 231240. https://doi.org/10.1098/rsos.231240.\n\n\nBrandt, Mark J., Hans IJzerman, Ap Dijksterhuis, Frank J. Farach, Jason Geller, Roger Giner-Sorolla, James A. Grange, Marco Perugini, Jeffrey R. Spies, und Anna van ’t Veer. 2014. „The Replication Recipe: What makes for a convincing replication?“ Journal of Experimental Social Psychology 50: 217–24. https://doi.org/10.1016/j.jesp.2013.10.005.\n\n\nBrembs, Björn, Philippe Huneman, Felix Schönbrodt, Gustav Nilsonne, Toma Susi, Renke Siems, Pandelis Perakakis, Varvara Trachana, Lai Ma, und Sara Rodriguez-Cuadrado. 2023. „Replacing academic journals“. Royal Society Open Science 10 (7). https://doi.org/10.1098/rsos.230206.\n\n\nBrodeur, Abel, Anna Dreber, Fernando Hoces de la Guardia, und Edward Miguel. 2024. „Reproduction and replication at scale“. Nature Human Behaviour 8 (1): 2–3.\n\n\nCarriquiry, Alicia L, Michael J Daniels, und Nancy Reid. 2023. „Editorial: Special Issue on Reproducibility and Replicability“. Stat. Sci. 38 (4).\n\n\nCarter, Evan C., Felix D. Schönbrodt, Will M. Gervais, und Joseph Hilgard. 2019. „Correcting for Bias in Psychology: A Comparison of Meta-Analytic Methods“. Advances in Methods and Practices in Psychological Science 2 (2): 115–44. https://doi.org/10.1177/2515245919847196.\n\n\nChambers, Christopher D, und Loukia Tzavella. 2022. „The past, present and future of Registered Reports“. Nature human behaviour 6 (1): 29–42.\n\n\nChandrashekar, Subramanya Prasad, und Gilad Feldman. o. J. „On the process and value of direct close replications: Reply to Shafir and Cheek (2024) commentary on Chandrashekar et al.(2021)“.\n\n\nCheung, Irene, Lorne Campbell, Etienne P. LeBel, Robert A. Ackerman, Bülent Aykutog˘lu, Štěpán Bahník, Jeffrey D. Bowen, u. a. 2016. „Registered Replication Report: Study 1 From Finkel, Rusbult, Kumashiro, & Hannon (2002)“. Perspectives on psychological science : a journal of the Association for Psychological Science 11 (5): 750–64. https://doi.org/10.1177/1745691616664694.\n\n\nClarke, Beth, Pui Yu Lee, Sarah R Schiavone, Mijke Rhemtulla, und Simine Vazire. 2023. „The prevalence of direct replication articles in top-ranking psychology journals“. PsyArXiv.\n\n\nDeStefano, Frank, und Tom T Shimabukuro. 2019. „The MMR vaccine and autism“. Annual review of virology 6 (1): 585–600.\n\n\nDeutsche Forschungsgemeinschaft. 2022. „Open Science als Teil der Wissenschaftskultur. Positionierung der Deutschen Forschungsgemeinschaft“. Zenodo.\n\n\nDevezer, Berna, Danielle J Navarro, Joachim Vandekerckhove, und Erkan Ozge Buzbas. 2021. „The case for formal methodology in scientific reform“. Royal Society open science 8 (3): 200805.\n\n\nEggertson, Laura. 2010. „Lancet retracts 12-year-old article linking autism to MMR vaccines.“ Canadian Medical Association Journal (CMAJ) 182.\n\n\nElsherif, Mahmoud, Gilad Feldman, und Siu Kit Yeung. 2023. „Quantitative manuscript peer review template“. OSF.\n\n\nEnsinck, Eline Noëlle Fleur, und Daniel Lakens. 2023. „An inception cohort study quantifying how many registered studies are published“. PsyArXiv.\n\n\nEtzel, Franka, Anna Seyffert-Müller, Felix D Schönbrodt, Lucie Kreuzer, Anne Gärtner, Paula Knischewski, und Daniel Leising. 2024. „Inter-Rater Reliability in Assessing the Methodological Quality of Research Papers in Psychology“.\n\n\nFarrow, Robert, Rebecca Pitt, und Martin Weller. 2020. „Open Textbooks as an innovation route for open science pedagogy“. Educ. Inf. 36 (3): 227–45.\n\n\nFeest, Uljana. 2019. „Why replication is overrated“. Philosophy of Science 86 (5): 895–905.\n\n\nFeldman, Gilad. 2021. „Replications and extensions of classic findings in Judgment and Decision Making“. https://doi.org/10.17605/OSF.IO/5Z4A8.\n\n\nFletcher, Samuel C. 2021. „The role of replication in psychological science“. European Journal for Philosophy of Science 11 (1): 23.\n\n\nFrank, Michael C. 2019. „N-best evaluation for academic hiring and promotion“. PsyArXiv.\n\n\nFraser, Nicholas, Liam Brierley, Gautam Dey, Jessica K Polka, Máté Pálfy, Federico Nanni, und Jonathon Alexis Coates. 2020. „Preprinting the COVID-19 pandemic“. bioRxiv. bioRxiv.\n\n\nFreese, Jeremy, und David Peterson. 2017. „Replication in social science“. Annual Review of Sociology 43 (1): 147–65.\n\n\nGärtner, Anne, Daniel Leising, und Felix Schönbrodt. 2022a. „Responsible Research Assessment II: A specific proposal for hiring and promotion in psychology“.\n\n\nGärtner, Anne, Daniel Leising, und Felix D Schönbrodt. 2022b. „Responsible Research Assessment II: A specific proposal for hiring and promotion in psychology“. PsyArXiv.\n\n\nHardwicke, Tom E, Robert T Thibault, Jessica E Kosie, Joshua D Wallach, Mallory C Kidwell, und John PA Ioannidis. 2022. „Estimating the prevalence of transparency and reproducibility-related research practices in psychology (2014–2017)“. Perspectives on Psychological Science 17 (1): 239–51.\n\n\nHardwicke, Tom E, und Simine Vazire. 2023. „Transparency is now the default at psychological science“. Psychol. Sci., Dezember, 9567976231221573.\n\n\nHenderson, Emma L, und Christopher D Chambers. 2022. „Ten simple rules for writing a Registered Report“. PLoS Comput. Biol. 18 (10): e1010571.\n\n\nHimmelstein, Daniel S, Ariel Rodriguez Romero, Jacob G Levernier, Thomas Anthony Munro, Stephen Reid McLaughlin, Bastian Greshake Tzovaras, und Casey S Greene. 2018. „Sci-Hub provides access to nearly all scholarly literature“. Elife 7 (März).\n\n\nHöffler, Jan H. 2017. „ReplicationWiki: improving transparency in social sciences research“. D-Lib Magazine 23 (3): 1.\n\n\nHolford, Dawn, Janet McLean, Alex O Holcombe, Iratxe Puebla, und Vera Kempe. 2024. „Engaging undergraduate students in preprint peer review“. Active Learning in Higher Education, 14697874241264495.\n\n\nHostler, Tom. 2024. „Research assessment using a narrow definition of ‚research quality‘ is an act of gatekeeping: A comment on Gärtner et al. (2022)“. Meta-Psychology 8 (März).\n\n\nHoward, George S, und Scott E Maxwell. 2023. „ORMA: A strategy to reduce Psychology’s replication problems“. New Ideas in Psychology 68: 100991.\n\n\nHoyningen-Huene, Paul. 2013. Systematicity: The nature of science. Oxford studies in philosophy of science. Oxford: Oxford Univ. Press.\n\n\nHüffmeier, Joachim, Jens Mazei, und Thomas Schultze. 2016. „Reconceptualizing replication as a sequence of different studies: A replication typology“. Journal of Experimental Social Psychology 66: 81–92. https://doi.org/10.1016/j.jesp.2015.09.009.\n\n\nIsager, Peder M, Daniël Lakens, Thed van Leeuwen, und Anna E van’t Veer. 2024. „Exploring a formal approach to selecting studies for replication: A feasibility study in social neuroscience“. Cortex 171: 330–46.\n\n\nIvory, James D, und Malte Elson. 2024. „A tale of three retractions: a call for standardized categorization and criteria in retraction statements“. Current Psychology 43 (17): 16023–29.\n\n\nJanz, Nicole, und Jeremy Freese. 2021. „Replicate others as you would like to be replicated yourself“. PS Polit. Sci. Polit. 54 (2): 305–8.\n\n\nJekel, Marc, Susann Fiedler, Ramona Allstadt Torras, Dorothee Mischkowski, Angela Rachael Dorrough, und Andreas Glöckner. 2020. „How to Teach Open Science Principles in the Undergraduate Curriculum—The Hagen Cumulative Science Project“. Psychology Learning & Teaching 19 (1): 91–106. https://doi.org/10.1177/1475725719868149.\n\n\nKelly, Clint D. 2006. „Replicating empirical research in behavioral ecology: how and why it should be done but rarely ever is“. Q. Rev. Biol. 81 (3): 221–36.\n\n\n———. 2019. „Rate and success of study replication in ecology and evolution“. PeerJ 7: e7654.\n\n\nKletke, Olaf, Inga Larres, Kathrin Höhner, Wibke Kleina, Julia Stapels, Bernd Zey, und Niels Kasties. 2024. „Bestands-und Bedarfserhebung im Forschungsdatenmanagement: Ergebnisse des Projektes FDM-TUDO“. o-bib. Das offene Bibliotheksjournal/Herausgeber VDB 11 (2): 1–18.\n\n\nKlonsky, E David. 2024. „Campbell’s Law explains the replication crisis: Pre-registration badges are history repeating“. Assessment, Mai, 10731911241253430.\n\n\nKobrock, Kristina, und Timo Roettger. 2023. „Assessing the replication landscape in experimental linguistics.“ Glossa Psycholinguistics 2 (1): 1–28.\n\n\nKorbmacher, Max, Flavio Azevedo, Charlotte R. Pennington, Helena Hartmann, Madeleine Pownall, Kathleen Schmidt, Mahmoud Elsherif, u. a. 2023. „The replication crisis has led to positive structural, procedural, and community changes“. Communications Psychology 1 (1). https://doi.org/10.1038/s44271-023-00003-2.\n\n\nKorell, Daniel, Niklas Reinecke, und Lars Lott. 2023a. „Student-led replication studies in comparative politics: new findings by fresh eyes?“ Zeitschrift für Vergleichende Politikwissenschaft 17 (3): 261–73. https://doi.org/10.1007/s12286-023-00578-4.\n\n\n———. 2023b. „Student-led replication studies in comparative politics: new findings by fresh eyes?“ Zeitschrift für Vergleichende Politikwissenschaft 17 (3): 261–73.\n\n\nLakens, Daniel. 2023. „Concerns about Replicability, Theorizing, Applicability, Generalizability, and Methodology across Two Crises in Social Psychology“. https://doi.org/10.31234/osf.io/dtvs7.\n\n\nLakens, Daniël. 2017. „Equivalence Tests: A Practical Primer for t Tests, Correlations, and Meta-Analyses“. Social Psychological and Personality Science 8 (4): 355–62. https://doi.org/10.1177/1948550617697177.\n\n\nLakens, Daniël, Cristian Mesquida, Sajedeh Rasti, und Massimiliano Ditroilo. 2024. „The benefits of preregistration and Registered Reports“. Evidence-Based Toxicology 2 (1): 2376046.\n\n\nLeBel, Etienne P., Wolf Vanpaemel, Irene Cheung, und Lorne Campbell. 2019. „A Brief Guide to Evaluate Replications“. Meta-Psychology 3. https://doi.org/10.15626/MP.2018.843.\n\n\nLevendis, John D. 2018. Time series econometrics. Springer.\n\n\nMakel, Matthew C, und Jonathan A Plucker. 2014. „Facts are more important than novelty“. Educ. Res. 43 (6): 304–16.\n\n\nMakel, Matthew C, Jonathan A Plucker, Jennifer Freeman, Allison Lombardi, Brandi Simonsen, und Michael Coyne. 2016. „Replication of special education research: Necessary but far too rare“. Remedial and Special Education 37 (4): 205–12.\n\n\nMakel, Matthew C, Jonathan A Plucker, und Boyd Hegarty. 2012. „Replications in psychology research: How often do they really occur?“ Perspect. Psychol. Sci. 7 (6): 537–42.\n\n\nMarsden, Emma, Kara Morgan-Short, Sophie Thompson, und David Abugaber. 2018. „Replication in second language research: Narrative and systematic reviews and recommendations for the field“. Language Learning 68 (2): 321–91.\n\n\nMcDiarmid, Alex D, Alexa M Tullett, Cassie M Whitt, Simine Vazire, Paul E Smaldino, und Jeremy E Stephens. 2021. „Psychologists update their beliefs about effect sizes after replication studies“. Nature human behaviour 5 (12): 1663–73.\n\n\nMcNeeley, Susan, und Jessica J Warner. 2015. „Replication in criminology: A necessary practice“. Eur. J. Criminol. 12 (5): 581–97.\n\n\nMerton, Robert K. 1973. The sociology of science: Theoretical and empirical investigations. University of Chicago press.\n\n\nMorgan, Thomas J H, und Paul E Smaldino. 2024. „Author-paid publication fees corrupt science and should be abandoned“.\n\n\nMueller-Langer, Frank, Benedikt Fecher, Dietmar Harhoff, und Gert G Wagner. 2019. „Replication studies in economics—How many and which papers are chosen for replication, and why?“ Research Policy 48 (1): 62–83.\n\n\nNelson, Lindsay, Honghan Ye, Anna Schwenn, Shinhyo Lee, Salsabil Arabi, und B Ian Hutchins. 2022. „Robustness of evidence reported in preprints during peer review“. Lancet Glob. Health 10 (11): e1684–87.\n\n\nNosek, B A, G Alter, G C Banks, D Borsboom, S D Bowman, S J Breckler, S Buck, u. a. 2015. „Promoting an open research culture“. Science 348 (6242): 1422–25.\n\n\nNosek, Brian A, und Timothy M Errington. 2020. „What is replication?“ PLoS biology 18 (3): e3000691.\n\n\nNuijten, Michèle B, Chris HJ Hartgerink, Marcel ALM Van Assen, Sacha Epskamp, und Jelte M Wicherts. 2016. „The prevalence of statistical reporting errors in psychology (1985–2013)“. Behavior research methods 48: 1205–26.\n\n\nOngchoco, Joan Danielle K, Robert Walter-Terrill, und Brian J Scholl. 2023. „Visual event boundaries restrict anchoring effects in decision-making“. Proceedings of the National Academy of Sciences 120 (44): e2303883120.\n\n\nOpenness, D H Nrw |. 2023c. „Open-Access-Strategie der Hochschulen des Landes NRW“. Zenodo.\n\n\n———. 2023b. „Open-Access-Strategie der Hochschulen des Landes NRW“. Zenodo.\n\n\n———. 2023a. „Open-Access-Strategie der Hochschulen des Landes NRW“. Zenodo.\n\n\nParsons, Sam, Flávio Azevedo, Mahmoud M. Elsherif, Samuel Guay, Owen N. Shahim, Gisela H. Govaart, Emma Norris, u. a. 2022. „A community-sourced glossary of open scholarship terms“. Nature Human Behaviour 6 (3): 312–18. https://doi.org/10.1038/s41562-021-01269-4.\n\n\nPawel, Samuel, Guido Consonni, und Leonhard Held. 2023. „Bayesian approaches to designing replication studies“. Psychol. Methods, August.\n\n\nPennington, Charlotte Rebecca, und Madeleine Pownall. 2024. „What have we learned from the replication crisis? Integrating open research into social psychology teaching“. PsyArXiv.\n\n\nPeroni, Silvio, und David Shotton. 2012. „FaBiO and CiTO: Ontologies for describing bibliographic resources and citations“. Web Semant. 17 (Dezember): 33–43.\n\n\nPriem, Jason, Heather Piwowar, und Richard Orr. 2022. „OpenAlex: A fully-open index of scholarly works, authors, venues, institutions, and concepts“.\n\n\nProtzko, John. 2018. „Null-hacking, a lurking problem“.\n\n\nQuan, Joshua. 2021. „Toward reproducibility: Academic libraries and open science“. Cambridge University Press.\n\n\nQuintana, Daniel S. 2021. „Replication studies for undergraduate theses to improve science and education“. Nat. Hum. Behav. 5 (9): 1117–18.\n\n\nRahal, Rima-Maria, Susann Fiedler, Adeyemi Adetula, Ronnie P-A Berntsson, Ulrich Dirnagl, Gordon B Feld, Christian J Fiebach, u. a. 2023. „Quality research needs good working conditions“. Nature human behaviour 7 (2): 164–67.\n\n\nRöseler, Lukas, Taisia Gendlina, Josefine Krapp, Noemi Labusch, und Astrid Schütz. 2022. „Successes and Failures of Replications: A Meta-Analysis of Independent Replication Studies Based on the OSF Registries“. https://doi.org/10.31222/osf.io/8psw2.\n\n\nRöseler, Lukas, Leonard Kaiser, Christopher Albert Doetsch, Noah Klett, Christian Seida, Astrid Schütz, Balazs Aczel, u. a. 2024. „The Replication Database: Documenting the Replicability of Psychological Science“. https://doi.org/10.31222/osf.io/me2ub.\n\n\nRöseler, Lukas, Lucia Weber, Katharina Helgerth, Elena Stich, Miriam Günther, Paulina Tegethoff, Felix Stefan Wagner, u. a. 2024. „Correction: The Open Anchoring Quest Dataset: Anchored Estimates from 96 Studies on Anchoring Effects“. Journal of Open Psychology Data 12 (1).\n\n\nRöseler, Lukas, Lucia Weber, und Astrid Schütz. 2021. „OpAQ: Open Anchoring Quest“. https://osf.io/ygnvb.\n\n\nRosenthal, Robert. 1979. „The file drawer problem and tolerance for null results“. Psychological Bulletin 86 (3): 638–41. https://doi.org/10.1037/0033-2909.86.3.638.\n\n\nRuiter, Bob de. 2023. „Automatically Finding and Categorizing Replication Studies“. arXiv e-prints, arXiv–2311.\n\n\nScanff, Alexandre, Nicolas Mauhe, Marion Taburet, Pierre-Etienne Savourat, Thomas Clément, Benjamin Bastian, Ioana Cristea, Alain Braillon, Nicolas Carayol, und Florian Naudet. 2023. „The ‚Free lunches‘ index for assessing academics: a not entirely serious proposal“. Scientometrics, November.\n\n\nSchmidt, Birgit, Andrea Chiarelli, Lucia Loffreda, und Jeroen Sondervan. 2024. „Emerging roles and responsibilities of libraries in support of reproducible research“. LIBER Q. 33 (1): 1–21.\n\n\nSchöch, Christof. 2023. „Repetitive research: a conceptual space and terminology of replication, reproduction, revision, reanalysis, reinvestigation and reuse in digital humanities“. International Journal of Digital Humanities 5 (2-3): 373–403. https://doi.org/10.1007/s42803-023-00073-y.\n\n\nSchönbrodt, Felix, Anne Gärtner, Maximilian Frank, Mario Gollwitzer, Malika Ihle, Dorothee Mischkowski, Le Vy Phan, Manfred Schmitt, Anne M. Scheel, Anna-Lena Schubert, Ulf Steinberg, u. a. 2022. „Responsible Research Assessment I: Implementing DORA for hiring and promotion in psychology“. https://doi.org/10.23668/psycharchives.8162.\n\n\nSchönbrodt, Felix, Anne Gärtner, Maximilian Frank, Mario Gollwitzer, Malika Ihle, Dorothee Mischkowski, Le Vy Phan, Manfred Schmitt, Anne M Scheel, Anna-Lena Schubert, und others. 2022. „Responsible Research Assessment I: Implementing DORA for hiring and promotion in psychology“.\n\n\nSiems, Renke. 2024. „Subprime Impact Crisis. Bibliotheken, Politik und digitale Souveränität“. BIBL. Forsch. Prax. 0 (0).\n\n\nSilverstein, Priya, Colin Elman, Amanda Kay Montoya, Barbara McGillivray, Charlotte Rebecca Pennington, Chase H Harrison, Crystal Nicole Steltenpohl, u. a. 2023. „A guide for social science journal editors on easing into open science“.\n\n\nSimmons, Joseph P., Leif D. Nelson, und Uri Simonsohn. 2011. „False-positive psychology: undisclosed flexibility in data collection and analysis allows presenting anything as significant“. Psychological Science 22 (11): 1359–66. https://doi.org/10.1177/0956797611417632.\n\n\nSimons, Daniel J., Alex O. Holcombe, und Barbara A. Spellman. 2014. „An Introduction to Registered Replication Reports at Perspectives on Psychological Science“. Perspectives on psychological science : a journal of the Association for Psychological Science 9 (5): 552–55. https://doi.org/10.1177/1745691614543974.\n\n\nSimonsohn, Uri. 2015. „Small telescopes: detectability and the evaluation of replication results“. Psychological Science 26 (5): 559–69. https://doi.org/10.1177/0956797614567341.\n\n\nSobkow, Agata, Marcin Surowski, Angelika Olszewska, Nina Antoniewska, Urszula Bartkiewicz, Agnieszka Brzeska, Adrianna Brzozowska, u. a. 2021. Conceptual replication study of fifteen JDM effects: Insights from the Polish sample. https://doi.org/10.31234/osf.io/5fc6z.\n\n\nSoderberg, Courtney K., Timothy M. Errington, Sarah R. Schiavone, Julia Bottesini, Felix Singleton Thorn, Simine Vazire, Kevin M. Esterling, und Brian A. Nosek. 2021. „Initial evidence of research quality of registered reports compared with the standard publishing model“. Nature Human Behaviour. https://doi.org/10.1038/s41562-021-01142-4.\n\n\nSrivastava, Sanjay. 2012. „A Pottery Barn rule for scientific journals“. The Hardest Science 27.\n\n\nSterling, T. D. 1959. „Publication decisions and their possible effects on inferences drawn from tests of significance—or vice versa“. Journal of the American Statistical Association 54 (285): 30–34. http://www.jstor.com/stable/2282137.\n\n\nStrecker, Dorothea. 2019. „Nutzung der Schattenbibliothek Sci-Hub in Deutschland“.\n\n\nTing, Carol, und Sander Greenland. 2024. „Forcing a deterministic frame on probabilistic phenomena: A communication blind spot in media coverage of the ‚replication crisis‘“. Sci. Commun., April.\n\n\nTiokhin, Leo, Karthik Panchanathan, Paul E Smaldino, und Daniël Lakens. 2023. „Shifting the level of selection in science“. Perspect. Psychol. Sci., August, 17456916231182568.\n\n\nTorka, Ann-Kathrin, Jens Mazei, und Joachim Hüffmeier. 2023. „Are replications mainstream now? A comparison of support for replications expressed in the policies of social psychology journals in 2015 and 2022“. Social Psychological Bulletin 18: 1–22.\n\n\nUNESCO. 2020. „First draft of the UNESCO Recommendation on Open Science“.\n\n\nUrminsky, Oleg, und Berkeley J Dietvorst. 2024. „Taking the Full Measure: Integrating Replication into Research Practice to Assess Generalizability“. Journal of Consumer Research 51 (1): 157–68.\n\n\nWagge, Jordan R, Mark J Brandt, Ljiljana B Lazarevic, Nicole Legate, Cody Christopherson, Brady Wiggins, und Jon E Grahe. 2019. „Publishing research with undergraduate students via replication work: The collaborative replications and education project“. Frontiers in psychology 10: 247.\n\n\nWillighagen, Egon. 2023. „Two years of explicit CiTO annotations“. J. Cheminform. 15 (1): 14.\n\n\nXiao, Qinyu, Choi Shan Lam, Muhrajan Piara, und Gilad Feldman. 2021. „Revisiting status quo bias“. Meta-Psychology 5. https://doi.org/10.15626/MP.2020.2470.\n\n\nYeung, Siu Kit, und Gilad Feldman. 2023. „RRR assessment manuscript template“. OSF.\n\n\nZiano, Ignazio, Pui Yan Mok, und Gilad Feldman. 2021. „Replication and Extension of Alicke (1985) Better-Than-Average Effect for Desirable and Controllable Traits“. Social Psychological and Personality Science 12 (6): 1005–17. https://doi.org/10.1177/1948550620948973.\n\n\nZimmermann, Christian. 2015. „On the need for a replication journal“. wp 2015 (016).\n\n\nZumstein, Philipp. 2023. „Open Access und ein Blick auf das wissenschaftliche Publikationswesen“. PsychArchives.",
    "crumbs": [
      "Lösungen",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Das System</span>"
    ]
  },
  {
    "objectID": "lösungen_methoden.html",
    "href": "lösungen_methoden.html",
    "title": "Methoden",
    "section": "",
    "text": "In der Wissenschaft gibt es nicht die eine Methode (Feyerabend 1975/2002), sondern Methoden werden für Probleme entwickelt, Probleme gelöst und Methoden weiterentwickelt oder fallen gelassen. Methoden sind also wie Werkzeuge, und nicht alles lässt sich mit einem Schraubendreher zusammenbauen. Mit Open Science Reformen kommen unzählige Methodische Neuerungen, Verbesserungen, und Vorschläge, die für Forschende häufig überwältigend sind: Forschungsprojekte voranbringen, Seminare und Vorlesungen halten, Drittmittel einwerben, und jetzt auch noch Open Science? Während viele Probleme auf eine unzureichende Methodenausbildung zurückgeführt werden (Daniel Lakens 2021), die Forschende zu ihrer eigenen Last nachholen müssen, erleichtern einige Methoden die Arbeit. Für Promovierende wurde in der biologischen Psychologie beispielsweise der Wegweise ARIADNE entwickelt (https://igor-biodgps.github.io/ARIADNE/graph/graph.html). Dieses Kapitel bietet einen Überblick über methodische Entwicklungen und Diskussionen in den Sozialwissenschaften und Disziplinen, die vorrangig mit statistischen Methoden arbeiten.\n\nMeta-Analysen\nUnter Meta-Analysen werden Studien über Studien verstanden. Dabei extrahieren Forschende üblicherweise Ergebnisse aus bereits veröffentlichten Studien, schreiben andere Forschende aus einem Feld an und fragen nach unveröffentlichten Studien, und analysieren mit statistischen Methoden die Gemeinsamkeiten und Unterschiede zwischen den Ergebnissen. Fletcher (2022) argumentiert, dass nur mithilfe von Meta-Analysen die Allgemeingültigkeit von (statistischen) Phänomenen nachgewiesen werden kann. Im Idealfall könnten alle so weitermachen wie bisher und meta-analytische Modelle würden die Probleme korrigieren. Angesichts verheerender Publikationsbiases ist das allerdings aktuell nicht möglich. Wie Meta-Analysen dennoch informativ sein können, empfehlen Carlsson u. a. (2024) allgemein und liste ich im Folgenden für spezifische Probleme auf.\n\n\nPublikationsbias und P-Hacking einschätzen\nBei Meta-Analysen gilt: “Garbage in, garbage out”. Wer viele schlecht durchgeführte Studien in einer Meta-Analyse zusammenfasst, erhält eine schlechte Zusammenfassung. Das hatte beispielsweise zur Folge, dass Hagger u. a. (2010) ihrer Meta-Analyse einen deutlichen Effekt für ein Modell über die Willensstärke finden konnten, nachfolgende, groß angelegte Replikationsversuche und Analysen jedoch alle scheiterten, einen ebenso großen Effekt zu finden (Hagger u. a. 2016; Friese u. a. 2018; Dang u. a. 2020; Vohs u. a. 2021). Was trotzdem möglich ist und auch in jeder Meta-Analyse getan werden sollte, ist eine Einschätzung der Datenqualität, beispielsweise der Stärke des Publikationsbiases. Dabei gibt es Methoden, die prüfen, ob es nicht veröffentlichte Studien gibt, und Methoden, die für die potenziell fehlenden Studien korrigieren. Teilweise funktionieren diese erst bei über 200 Studien, manche lassen sich jedoch auch schon bei einem Dutzend Studien anwenden.\n\n\nFunnel Plot\nEine der ältesten Methoden ist der Funnel Plot (Trichter-Diagramm) (Light und Pillemer 1984). Dabei werden Präzision und Effektstärke der einzelnen Studien in einem Diagramm dargestellt. Im Idealfall sollten die Punkte das Bild eines Trichters bilden: Je präziser eine Studie ist (zum Beispiel durch eine große Stichprobe), desto eher sollte der dort gemessene Zusammenhang im Mittel liegen. Unpräzisere Studien verschätzen sich unsystematisch, also sie liegen mal drüber und mal drunter. Dadurch, dass nicht signifikante Ergebnisse selten veröffentlicht werden, bildet sich in dem Trichter-Diagramm fast nie ein Trichter, sondern nicht-signifikante Ergebnisse fehlen einfach.\nIn der folgenden Abbildung ist ein Trichter-Diagramm für eine Studie zum Zusammenhang zwischen Angst vor Mathematik und Leistung in Mathematik. Das Muster ist fast symmetrisch, es liegt also nur ein schwacher Publikationsbias vor. Die kleinen Zusammenhänge am unteren Rand sind etwas nach rechts verzerrt, und die präzisen Effekte ganz oben sind nicht alle gleich groß, sondern variieren stark.\n\n\nCode\n# Funnel plot example\nlibrary(psymetadata)\nds &lt;- psymetadata::barroso2021\n\nmodel &lt;- metafor::rma.mv(yi = ds$yi, V = ds$vi, random = ~1 | study_id, data = ds)\nmetafor::funnel(model, xlab = \"Beobachtete Effektstärke\", ylab = \"Präzision (Standardfehler)\")\n\n\n\n\n\n\n\n\n\n\n\nP-Curve\nBeim P-Hacking werden Daten auf verschiedene Weisen ausgewertet und es wird diejenige berichtet, die einen niedrigen und damit signifikanten P-Wert zur Folge hat. Ergebnisse werden also nicht signifikant, weil die Hypothesen korrekt sind, sondern weil die Daten so lange ausgewertet wurden, bis sie signifikant wurden. Im Kapitel P-Hacking konnten wir sehen, wie P-Werte verteilt sind, je nachdem ob die Hypothese korrekt ist oder nicht. Diese Tatsache macht sich die P-Curve (Simonsohn, Nelson, und Simmons (2014b); Simonsohn, Nelson, und Simmons (2014a); Simonsohn, Simmons, und Nelson (2015)) zu Nutzen. P-Werte eines Sets and Studien werden in einem Diagramm abgebildet und ihre Verteilung wird geprüft. Liegt kein P-Hacking vor sind die Werte entweder gleich verteilt (alle P-Werte kommen gleich häufig vor) oder sammeln sich bei 0 (kleinere P-Werte kommen häufiger vor). P-Hacking hat jedoch zur Folge, dass sich die Werte an der 5% Grenze tummeln, denn weiter als bis dort müssen die Daten nicht “gehackt” werden. Die Methode gewann vor allem deshalb an Bekanntheit, weil Simmons und Simonsohn (2017) die Methode auf eines der berühmtesten psychologischen Phänomene, dem Power Posing, angewandt haben und herausfanden, dass dort wahrscheinlich P-Hacking vorlag. Kritiker stellten später weitere Möglichkeiten vor, wie auch ohne P-Hacking eine suspekte P-Curve entstehen kann und das Verfahren wird inzwischen kaum mehr verwendet (Erdfelder und Heck 2019).\n\n\nZ-Curve\nStatt P-Werte zu nehmen, können meta-analytische Befunde auch in sogenannte Z-Werte umgerechnet werden. Sie sind normalverteilt und mittels zusätzlicher Algorithmen lässt sich auf Basis von beobachteten Effekten schätzen, wie viele weitere Effekte es geben müsste. Dieses Verfahren namen Z-Curve (Bartoš und Schimmack 2022) kann also für den File-Drawer Effekt und für P-Hacking korrigieren. Das Ergebnis daraus ist auch eine Schätzung, wie hoch die Replikationsrate wäre, wenn alle analysierten Studien erneut durchgeführt würden. Aktuellen Studien zufolge funktionieren diese Schätzungen ziemlich gut, obgleich sie eine große Menge an Daten benötigen (Sotola und Credé 2022; Sotola 2023; Röseler 2023).\n\n\nSensitivitätsanalyse\nEin Ansatz, welcher nicht nur bei Meta-Analysen sondern bei fast allen statistischen Auswertungen funktioniert, sind sogenannte Sensitivitäts- oder Robustheits-Analysen. Es werden dabei verschiedene Auswertungswege durchgegangen und dabei geprüft, wie stark sie sich auf die Ergebnisse auswerten. Bei Meta-Analysen können zum Beispiel viele mögliche Verfahren gleichzeitig gerechnet werden. Einen solchen Schrotschuss-Ansatz hat Kepes, Bushman, und Anderson (2017) geprägt, woraufhin er in anderen Studien übernommen wurde (Körner u. a. 2022). Einen Überblick über verschiedene Verfahren und unter welchen Bedingungen sie für Daten geeignet sind, bieten Carter u. a. (2019).\n\n\nDaumenregeln für Beurteilung einzelner Artikel\nEine Meta-Analyse ist aufwändig und kann mehrere Jahre dauern. Selbst Forschende, die keine Mittel für Studentische Hilfskräfte haben, die ihnen beim Codieren und Prüfen von hunderten bis tausenden Studien helfen, haben hier kaum eine Chance, eine ordentliche Analyse durchzuführen. Die folgenden Daumenregeln - und mehr als das sollen sie auch nicht sein - bieten Abkürzungen zur Beurteilung wissenschaftlicher Qualität.\n\n\nViele Signifikante Studien\nSeit einer Vertrauenskrise in der Sozialpsychologie in den 1960er Jahren (Daniel Lakens 2023) werden in Forschungsartikeln seitens vieler Zeitschriften mehrere Studien gefordert. Das hatte zur Folge, dass die Ressourcen statt in eine ordentliche Studie in mehrere kleinere Studien investiert wurden. Die meisten dieser “Multi-Study-Paper” haben dann ausschließlich signifikante Ergebnisse über bis zu 10 Studien hinweg. Während viele Studien mit durchweg signifikanten Ergebnissen auf den ersten Blick beeindruckend aussehen, lösen Sie beim näheren Hinsehen jedoch Skepsis aus: Einzelne Studien haben üblicherweise eine Wahrscheinlichkeit von 80-95%, dass dabei signifikante Ergebnisse bei der zentralen Analyse herauskommen. Diese Wahrscheinlichkeit (Statistische Teststärke oder Power) nimmt ab, wenn man mehrere Studien nacheinander durchführt. Es ist vergleichbar mit einem Schützen, der in 99% der Fälle mit einem Gewehr eine Glasflasche trifft. Die Wahrscheinlichkeit, dass er bei einem Schuss eine Glasflasche trifft ist also 99%. Die Wahrscheinlichkeit, dass er mit 50 Schüssen 50 Glasflaschen trifft ist weniger, nämlich 99%^50 (hoch fünfzig) = 60,5%. Bei wissenschaftlichen Studien kommt es auf ähnliche Weise zu einer “Power-Deflation”. Die Wahrscheinlichkeit, 4 signifikante Studien mit jeweils 80% Power durchzuführen, ist 40,96%. Dann eine genau solche Studie zu veröffentlichen ist extrem unwahrscheinlich (Daniël Lakens und Etz 2017).\n\n\nEffektstärken “gerade so signifikant”\nAngelehnt an die Logik der P-Curve ist es unwahrscheinlich, dass P-Werte zwischen 1 und 5% liegen. Aufgrund von p-hacking kommt es allerdings häufig vor. Ein P-Wert nahe 5% geht außerdem mit einem Konfidenzintervall der Effektstärke nahe 0 einher (z.B. (jane2024?)). Angenommen jemand führt zwei Studien zu einem Thema durch und beide haben P-Werte nahe 5% und ungefähr gleich große Versuchspersonen-Anzahlen, dann kommt die Frage auf, weshalb die Stichprobengröße für die spätere Studie nicht erhöht wurde: Auf Basis eines gerade so signifikanten Ergebnisses ist klar, dass man “Glück” hatte, da die statistische Power nicht sonderlich hoch war. Plant man also die Stichprobe für die nächste Studie, sollte man die erste Studie dabei zugrund legen und den Plan anpassen (z.B. Daniël Lakens (2021a)).\n\n\nCode\nrs &lt;- c(seq(.01, .9, by = .01)) # Korrelationen, für die die Funktion ausgeführt wird\nrsci &lt;- sapply(rs, FUN = function(rs) {psychometric::CIr(r = rs, n = 250)}) # füre Funktion mehrfach aus\n\nrci &lt;- data.frame(t(rsci))\nnames(rci) &lt;- c(\"lcl\", \"ucl\")\nrci$r &lt;- rs\n\nplot(rci$r, rci$r, type = \"l\", xlab = \"Korrelation\"\n     , ylab = \"Korrelation mit Konfidenzintervall\"\n     , main = \"Breite von Konfidenzintervallen je nach Korrelationsgröße\\nfür N = 250\")\nlines(rci$r, rci$ucl, lty = 2)\nlines(rci$r, rci$lcl, lty = 2)\n\n\n\n\n\n\n\n\n\n\n\nPrüfung auf Reproduzierbarkeit\nWird ein Befund mit denselben Daten und idealerweise demselben Programm bzw. Analysecode erneut getestet und geprüft, ob dieselben Zahlen dabei rauskommen - also nicht nur, ob die Hypothese erneut bestätigt wird - dann handelt es sich um eine Reproduktion der Ergebnisse. Im Gegensatz zu einer Replikation werden also keine neuen Daten erhoben. Dass Ergebnisse reproduzierbar sind, sollte das absolute Minimum für wissenschaftliche Berichte sein, ist es jedoch noch längst nicht. In nur seltenen Fällen gibt es Reproduktionen, die mittlere Rate über Felder hinweg (Wirtschaftswissenschaften, Erziehungswissenscahften, Biomedizin, Gesundheitswissenscahften) liegt bei circa 50% C. Chang und Li (2022) und nicht einmal einfache Analysen mit Programmen, die explizit Protokolle zur Reproduzierbarkeit ausgeben ist extrem nierig (Thibault u. a. 2024).\n\n\n\n\n\n\nBegriffs-Wirrwarr\n\n\n\nWährend der Begriff Replikation in den Wirtschaftswissenschaften sowohl die Prüfung einer vorliegenden Studie mit neuen Daten, als auch die erneute Prüfung mit denselben Daten meint, wird für letzteres in der Psychologie Reproduktion verwendet. In der biologischen Forschung über Fortpflanzung, der Reproduktionsforschung, wird außerdem auf Reproduzierbarkeit ausgewichen. In wieder anderen Fällen wie der Open Science Collaboration (2015) wird bei Replikationen (neue Daten) von “Reproducibility” gesprochen und wiederholte Tests mit denselben Daten werden “komputationale Reproduzierbarkeit” (computational reproducibility) genannt. Zuletzt verschwimmen in manchen Bereichen die Grenzen, wenn zum Beispiel bei einer Replikation der Befunde der Pisa Studie teilweise dieselben Daten und teilweise neue verwendet werden oder wenn die Daten computergeneriert sind und dasselbe Programm fähig ist mittels Pseudozufallszahlengenerator andere Daten zu generieren, die aber dieselbe Struktur haben.\n\n\nSeit wenigen Jahren führt die Zeitschrift Meta-Psychology als eine der ersten für alle veröffentlichten Artikel Reproduzierbarkeits-Prüfungen durch. Diese werden durch Forschende freiwillig oder im Rahmen ihrer Tätigkeit bei der Zeitschrift durchgeführt. Während diese Praxis bereits für andere Zeitschriften gefordert wurde (Lindsay 2023), ist es jedoch noch immer die Ausnahme. Reproduktionschecks aller möglicher Disziplinen können bei Rescience veröffentlicht werden (http://rescience.github.io). Für das Jahr 2024 hat das Institute for Replications angekündigt, Studien aus der Zeitschrift Nature Human Behavior zu reproduzieren („Promoting reproduction and replication at scale“ 2024). Nature Human Behavior ist eine der angesehensten Zeitschriften bei der Erforschung menschlichen Verhaltens, wobei angesehen nicht mit wissenschaftlicher Qualität gleichzusetzen ist. Sie wird vom Springer Verlag verwaltet und fordert mit Publikationskosten in Höhe von circa 9000€ pro Artikel die höchste Gebühr. Die strategische Entscheidung, sich auf die dortigen Artikel zu konzentrieren hat den Vorteil, dass Personen, die die Reproduzierbarkeits-Checks durchführen, diese eventuell dort veröffentlichen können und dass Reproduktionen große Aufmerksamkeit erfahren. Angesichts des Qualitätsanspruches solcher Zeitschriften an ihre Qualität und der Tatsache, dass kostenlose Zeitschriften wie Meta-Psychology die Prozedur ohne externe Hilfe durch das Institute for Replication durchführen können, bildet sich hier wieder das bekannte Bild ab, bei dem Verlage ihr Prestige dafür missbrauchen, kostenlose und profit-generierende Arbeit aus der Wissenschaft zu ziehen. Am Ende ist es wieder nicht die Zeitschrift selbst, die zur wissenschaftlichen Qualitätssicherung beiträgt, sondern das Institute for Replication.\nEine Abkürzung bei der Prüfung von Korrektheit, welche bei vielen Zeitschriften verwendet wird, ist das Programm statcheck. Es erkennt automatisch klassische statistische Tests und prüft auf Basis der berichteten Werte, ob diese konsistent sind. Hartgerink (2016) hat Ergebnisse aus über 50.000 Artikeln mit dem Programm geprüft und die Artikel mittels Pubpeer kommentieren lassen. Weil der Algorithmus in seltenen Fällen - wie in den Kommentaren offen dargelegt - fälschlicherweise Werte als fehlerhaft markiert und die Autor*innen der Artikel zuvor nicht vor den Kommentaren gewarnt wurden, hat die DGPs das Vorgehen verurteilt. Die Antworten der Statcheck-Gruppe und von Christ Hartgerink sind nicht mehr verfügbar.\n\n\n\n\n\n\nReproduktion auf Knopfdruck\n\n\n\nMit sogenannten Push-Button-Replications ist gemeint, dass Ergebnisse ohne großen Aufwand und von allen Forschenden nachgerechnet werden können - auf Knopfdruck eben. Während sozialwissenschaftliche Zeitschriften mehr und mehr fordern, Daten und Analysecode so zu veröffentlichen, dass die Ergebnisse nachgerechnet werden, verkörpert die Zeitschrift Image Processing Online (IPOL, https://www.ipol.im) das Ideal dieses Vorgehen: Zu jedem dort veröffentlichten Artikel ist eine Demo verfügbar, bei der nach Auswahl eines Bildes, der in dem Artikel veröffentlichte Algorithmus live durchgeführt wird.\n\n\n\nGroßangelegte Reproduktionsprojekte\nIn verschiedenen Forschungsdisziplinen gibt es großangelegte Projekte, Reproduzierbarkeit für vollständige Disziplinen zu schätzen. Ein Pionier auf dem Gebiet war das ReplicationWiki von Höffler (https://replication.uni-goettingen.de/). Nachfolgende Projekte wie das Replication Network (replicationnetwork3.wordpress.com) stützten sich weitesgehend auf die dort zusammengefassten Daten. Für die Wirtschaftswissenschaften berichteten Brodeur, Mikola, und Cook (2024) eine Reproduzierbarkeitsrate von 70% und in den Management Sciences bei 55% (Fišar u. a. 2024). Mit dem Insitute for Replication (I4R) überschneidet sich außerdem die Social Science Reproduction Platform des Berkley Initiative for Transparency in the Social Sciences (BITSS); https://www.bitss.org/resources/social-science-reproduction-platform/). Während das I4R voraussichtlich 2024 eine Datenbank mit allen Ergebnissen veröffentlicht, ist die Plattform der BITSS bereits verfügbar.\n\n\nOpen Code\nÖffentlich verfügbare Daten und Code sind notwendig für Reproduktions- und Robustheits-Checks. Zeitschriften stehen hier zwischen der Entscheidung, Einreichungen schwieriger und sich selbst damit weniger attraktiv zu machen, indem sie höhere Anforderungen stellen, und die wissenschaftliche Qualitätssicherung zu fördern. Ein ähnliches Problem herrscht auch bei Betreibern von Panels, in denen regelmäßig große Befragungen oder Leistungstest, wie zum Beispiel die PISA Studie oder das Sozio-Ökonomische-Panel (SOEP). Bei Analysen der SOEP-Daten wird der Code nur in 20% der Fälle geteilt (https://www.wifa.uni-leipzig.de/fileadmin/Fakultät_Wifa/Institut_für_Theoretische_Volkswirtschaftslehre/Professur_Makroökonomik/Economics_Research_Seminar/ERS-Paper_Marcus.pdf)\n\n\n\nRobustheits-Analysen\nÄhnlich wie die Sensitivitäts- oder Robustheitsanalysen lassen sich auch bei einzelnen Studien weitere Wege im “Garden of Forking Paths” gehen. Zur Erinnerung: Der Weg von Daten zu Ergebnissen ist lang und beinhaltet viele verschiedene Entscheidungen. Um zu zeigen, dass das Ergebnis eben nicht von diesen Entscheidungen abhängt, kann gezeigt werden, wie die Ergebnisse aussehen, wenn andere Entscheidungen getroffen werden würden. Der Extremfall dieser Robustheits-Analysen ist die Multiversum-Analyse. Hier wird versucht, alle möglichen Entscheidungen gleichzeitig zu treffen. Die daraus resultierenden Ergebnisse werden dann wieder in irgendeiner Form analysiert (z.B. gemittelt) oder dargestellt (Jacobsen u. a. 2024). Eine weitere Möglichkeit ist die der Multi-Analyst-Study. Dabei geht es um die Abhängigkeit der Ergebnisse von den Entscheidungen verschiedener Forschenden und viele Personen analysieren die Daten unabhängig voneinander. Es wird schließlich geprüft, wie stark die Ergebnisse zwischen den Forschenden übereinstimmen.\nIn der folgenden Abbildung wurde für einen festen Datensatz (Fantasiedaten) verschiedene Analysemethoden verwendet. Dabei wurden verschiedene Typen von Korrelationen, verschiedene Stichprobenumfänge, und verschiedene Hypothesen verwendet. Das Ergebnis ändert sich dabei jedes mal ein bisschen, sodass der Wert zwischen 0,20 und 0,35 liegt, die positive (und signifikante) Korrelation bleibt aber erhalten.\n\n\nCode\nlibrary(MASS) # Paket laden\n\nset.seed(1) # Zufallszahl festlegen, damit Ergebnisse immer identisch sind \nr_det &lt;- .4 # einprogrammierte Korrelation\nds &lt;- MASS::mvrnorm(n = 197, mu = c(0,0), Sigma = matrix(c(1, r_det, r_det, 1), ncol = 2, byrow = TRUE))\nds &lt;- as.data.frame(ds)\nnames(ds) &lt;- c(\"x\", \"y\")\n\n\nr &lt;- data.frame(\"p\", \"estimate\")\n\nfor (i in c(nrow(ds), 150)) {\n  for (j in c(\"two.sided\", \"greater\")) {\n    for (k in c(\"pearson\", \"spearman\", \"kendall\")) {\n      for (l in F) {\n        \n        r &lt;- rbind(r, unlist(cor.test(ds$x[1:i], ds$y[1:i], alternative = j, method = k, continuity = l)[3:4]))\n        \n      }\n    }\n  }\n}\n\nr &lt;- r[-1, ]\nnames(r) &lt;- c(\"p\", \"estimate\")\nplot(1:nrow(r), r$estimate\n     , pch = 20\n     # , col = ifelse(as.numeric(r$p) &lt; .05, \"red\", \"blue\")\n     , ylim = c(0, r_det+.2)\n     , xaxt = 'n'\n     , ylab = \"Korrelation\"\n     , xlab = \"\"\n     , bty = \"l\"\n     )\nabline(0,0)\n\n\n\n\n\n\n\n\n\n\nReproduzierbare Manuskripte\nIn der online-Version dieses Buches gibt es die Möglichkeit, vor Abbildungen einen Code anzeigen zu lassen. Mit diesem Code lässt sich die Abbildungen ganz einfach rekonstruieren bzw. reproduzieren. Auch Forschungsartikel können auf diese Weise geschrieben werden. Text, Programmiercode, sowie Ergebnisse als Zahlen, Tabellen, und Diagramme werden in einem einzelnen Programm geschrieben und Forschende ersparen sich das Kopieren oder Abtippen von Zahlen. Das Nachrechnen von Ergebnissen wird außerdem stark vereinfacht. Programmiersprachen basieren auf der sogenannten Markdown Sprache und in Programmen können unzählige weitere Programmiersprachen eingebettet sein.\nWährend Forschende oft nicht die Expertise oder die Zeit haben, ihre Manuskripte reproduzierbar zu gestalten, gibt es bereits Pilotprojekte (Baker u. a. 2023) und Zeitschriften (Carlsson u. a. 2017), die Forschende unterstützen. In Kombination mit Multiversum-Analysen ist es in Forschungsartikeln im Internet außerdem möglich, den Text so zu erstellen, dass er interaktiv auf alternative Analyse-Entscheidungen reagiert. Leser*innen können also im Manuskript Entscheidungen treffen und direkt sehen, wie sich die Ergebnisse ändern.\n\n\n\nStatistik\nEs sind wahrscheinlich alle Forschungsdisziplinen, die Statistik verwenden, von der Replikationskrise betroffen. Ganz im Sinne von “post hoc ergo propter hoc” (danach, also deswegen), wird diese Tatsache häufig so interpretiert, dass das Verwenden statistischer Methoden die Ursache für die Replikationsprobleme ist. Während dagegen argumentiert wird, dass die Methoden nur falsch verwendet werden (Daniël Lakens 2021b), schlagen manche Forschende auch Veränderungen oder Alternativen vor. Eine Gruppe 72 von Psycholog*innen hat beispielsweise gefordert, die Signifikanzgrenze für neue Befunde von 5% auf 0,5% herunterzusetzen (Benjamin u. a. 2018), sodass p-hacking erschwert wird. Andere schlagen vor, das Null Hypothesis Significance Testing (NHST, Null-Hypothesen-Signifikanztesten) komplett zu verbannen und andere Methoden zu verwenden: Wagenmakers (2007) plädiert für Bayesianische Statistik, und die Zeitschfit Basic and Applied Psychology verbietet die Verwendung von Signifikanztests, was das Problem von falsch-positiven Befunden möglicherweise noch vergrößert hat (Fricker Jr u. a. 2019).\n\n\nOpen Data und Open Materials\nDurch die häufigen befristeten Verträge und viele Wechsel zwischen Universitäten aber auch durch Abbrüche von Promotionen oder Ausscheiden aus der Wissenschaft durch das Wissenschaftszeitvertragsgesetz müssen Projekte an andere Forschende übergeben werden. Wenn die Forschungsmaterialien und -daten nicht dokumentiert und aufbereitet sind, gehen dadurch Zeit und Arbeit verloren. Im Extremfall wurden in einem Labor Tiere aufgezogen und operiert und die Untersuchung kann nicht weitergeführt werden. Open Data und Materials (Offene [Forschungs]Daten und offene [Forschungs]Materialien) haben den Zweck, das zu verhindern, gemeinsames Arbeiten zu ermöglichen, und Fehler korrigierbar zu machen. In extremeren Fällen versuchen Forschende, Artikel mit gefälschten Daten zu veröffentlichen. Nur an den Stellen, an denen offen zugängliche Daten verfügbar sind, kann dieser Betrug auffallen (Carlisle 2021).\nZahlreiche Untersuchungen konnten bereits zeigen, dass Daten auch auf Anfrage häufig nicht geteilt werden, und dass sich das im Zuge der Replikationskrise nicht verändert hat(Vanpaemel u. a. 2015). Ob Daten geteilt werden gibt darüber hinaus keinen Aufschluss darüber, ob darin Fehler sind (Claesen u. a. 2023). Mehr und mehr Zeitschriften fodern die Veröffentlichung von Daten (z.B. https://topfactor.org/journals?factor=Data+Transparency), Drittmittelgeber fordern Datenmanagementpläne, Werkzeuge zur automatischen Datenaufbereitung befinden sich in der Entwicklung (https://leibniz-psychology.org/das-institut/drittmittelprojekte/datawiz-ii), und viele Forschungsdaten-Repositorien - also Websites, auf denen Daten hochladbar und auffindbar sind - sind entstanden.\nDie wichtigsten Voraussetzungen dafür, dass Forschende Daten teilen können, sind die Zustimmung der Versuchspersonen (falls vorhanden), Anonymisierung falls nötig (z.B. bei Daten über Gesundheit oder politischer Einstellung), und die Rechte an den Daten. Die Zustimmung von Versuchspersonen werden standardmäßig vor Beginn von Untersuchungen erfragt, Anonymisierung geschieht bei der Erhebung oder im Nachhinein (z.B. bei qualitativen Daten mittels AMNESIA), und die Rechte liegen üblicherweise nur dann nicht vor, wenn die Daten für ein Unternehmen erhoben wurden. Der wohl schwierigste Teil ist die Anonymisierung. Campbell u. a. (2023) berichten beispielsweise, wie sie Berichte von Überlebenden sexueller Übergriffe mittels eines mehrstufigen Prozesses anonymisiert haben, bei denen Namen, Daten, Orte, Trauma-Historien, und weitere sensible Daten zensiert wurden.\n\n\n\n\n\n\nWo werden Forschungsdaten hochgeladen?\n\n\n\nJe nach Fach und Institution werden Forschungsdaten an unterschiedlichen Stellen archiviert. Universitäten haben häufig eigene Services, für die Auffindbarkeit bieten sich aber üblicherweise fachspezifische Repositorien an: Psycholog*innen nutzen häufig das Open Science Framework (OSF), PsychArchives vom Leibniz-Institut für Psychologie, oder Researchbox.org. Für die Wirtschaftswissenschaften bietet das Leibniz-Institut für Sozialwissenschaften mit GESIS verschiedene Hilfen. Mit LISTER wird in der Chemie an Software gearbeitet, mittels derer Daten auf Basis von elektronischen Laborbüchern halbautomatisiert beschrieben werden können. Re3data bietet eine Übersicht über Forschungsdatenrepositorien (z.B. nach Fächern).\n\nRepositorien für Forschungsdaten\n\n\nFach/Disziplin\nRepositorium\n\n\n\n\nPsychologie\nosf.io\n\n\nSozialwissenschaften\ndata.gesis.org\n\n\nPolitik- und Sozialwissenschaften\nicpsr.umich.edu\n\n\nLebenswissenschaften\nPangaea.de\n\n\nKunst- und Geistewissenschaften\nde.Dariah.eu\n\n\nLinguistik\nClarin.eu\n\n\nBiologie\ngfbio.org\n\n\nMaterialwissenschaften\nnomad-lab.eu\n\n\nQualitative Daten\nqdr.syr.edu\n\n\n\n\n\n\n\n\n\n\n\nWo werden Forschungsdaten veröffentlicht?\n\n\n\nÜber Repositorien können Forschungsdaten per Mausklick veröffentlicht werden. Wenn weitere Funktionen wie interaktive Analysen oder ein Peer Review gewünscht ist, nutzen Forschende darauf abgestimmte Werkzeuge und Fachzeitschriften. Psychologische Datensätze können beispielsweise im Journal of Open Psychology Data veröffentlicht werden, das R-Paket PsyMetaData (Rodriguez und Williams 2022) enthält Daten aus psychologischen Meta-Analysen, und die Zeitschrift Inggrid veröffentlicht Daten aus den Ingenieurswissenschaften.\nAuf dafür erstellten Websites können Forschende bei MOCODA auf Daten von Chatverläufen zugreifen, die freiwillig und anonymisiert von Personen geteilt wurden, bei CODA Daten zur menschlichen Kooperation herunterladen, oder bei OpAQ Schätzurteile analysieren.\n\n\n\nKriterien für die Aufbereitung Forschungsdaten\nDas bloße Hochladen von Forschungsdaten auf eine Website reicht nicht aus, um Forschung transparenter zu machen. Üblicherweise werden die Daten in dem Forschungsartikel, in dem sie verwendet wurden, verlinkt und es wird ein Codebook bereitgestellt, in dem steht, welche Werte was bedeuten.\nDie Tabelle zeigt einen Ausschnitt aus einem Fantasie-Datensatz mit drei Variablen. Üblicherweise enthalten Datensätze viele weitere Daten (z.B. die Abiturnote aufgeteilt in verschiedene Fächer, Demografische Daten wie Alter und Geschlecht, Datum der Befragung, ggf. Variablen mit kryptischen Namen wie “V1_Z01”, “V0815”). Hier sind die drei Variablen (also Spalten) “ID”, welche eine fortlaufende Zahl ist um verschiedene Versuchspersonen zu kennzeichnen, IQ, mit welchem ein gemessener IQ von einem bestimmten Intelligenztest angegeben wird, und die Abiturnote, die Personen selbst berichten sollten. Das Codebuch enthält diese Informationen.\n\nBeispiel für einen Datensatz\n\n\nID\nIQ\nAbiturnote\n\n\n\n\n1\n103\n2,6\n\n\n2\n86\n2,4\n\n\n3\n112\n1,8\n\n\n\n\n\nFAIR und CARE\nInterdisziplinär wurden die FAIR Prinzipien für Forschungsdaten entwickelt. Sie empfehlen, Daten auffindbar (Findable), zugänglich (Accessible), von verschiedenen Computern lesbar (Interoperable), und wiederverwendbar (Resusable) zu archivieren. De Waard (2016) strukturiert Anforderungen pyramidenförmig mit der Speicherung als Fundament, dem Teilen darüber, und der Qualitätssicherung als Spitze. Laut einem EU-Bericht belaufen sich die jährlichen Kosten dafür, dass Daten nicht den FAIR Prinzipien entsprechen, auf 10,2 Milliarden Euro. Disziplin-spezifische Vorlagen, um geteilte Daten FAIR zu machen, werden derzeit vom Center for Open Science entwickelt (www.cos.io/blog/cedar-embeddable-editor).\nEinen weiteren Schritt gehen die CARE-Prinzipien. Sie wurden für Datenerhebung zu einheimischen Völkern entworfen. Aufbauen auf FAIR fordern sie einen kollektiven Nutzen (collective benefit) der Daten, beispielsweise durch Verwendbarkeit durch die Gesellschaft wie es bei Hochwassergefahrenkarten der Fall ist oder durch Bürgerbeteiligung wie bei Münsters “Coolem Stadtplan”, der Orte, an denen es an heißen Tagen kühl ist, eintragen lässt. Den Personen, die mit den Daten abgebildet werden, muss eine Autorität zur Kontrolle (authority to control) gegeben werden, das heißt, sie sollen Mitbestimmungsrecht über das Aussehen der Daten haben. Zur Wahrung ihrer Selbstbestimmung sollten Daten außerdem verantwortlich (responsible) geteilt werden und ihre Rechte und ihr Wohlergehen sollten bei der Forschung im Zentrum stehen (ethics). Wenn Nicht-Wissenschaftler*innen aktiv an der Datenerhebung oder -aufbereitung beteiligt sind, ist die Rede von Citizen Science (Bürger*innen Wisssenschaft). Beispielsweise können Personen ihre gesammelten Liebesbriefe an das Liebesbriefarchiv senden, bei welchem die deutsche Sprache, Umgangsformen, und Kulturwandel umfangreicher als nur mithilfe von einzelnen berühmten Gelehrten erforscht werden kann oder Personen können von ihrem privaten Rechner über das Internet Weltraumteleskope bedienen (http://www.aim-muenster.de). Dabei ist nicht gemeint, dass Laien Forschung durchführen, auf deren Basis sie “klassische” Forschung widerlegen (Levy 2022), sondern, dass sie Einblicke in den Forschungsprozess erhalten und zum Erkenntnisgewinn unter Anleitung mitwirken können.\n\n\n\n\n\n\nForschungsdaten bei Regierungen anfragen\n\n\n\nWenn Regierungen Unternehmen beauftragen, Fragestellungen zu beantworten, können Bürger*innen die Daten dafür anfragen. Für das Vereinigte Königreich gibt es beispielsweise die Platform WhatDoTheyKnow, in Deutschland ist das Analog FragDenStaat. In einer Studie haben Maier u. a. (2024) solche Daten verwendet, um zu prüfen, ob dort Empfehlungen zu Open Science Praktiken berücksichtigt wurden.\n\n\n\n\nSorgen zu Offenen Daten\n\n\nDaten-Polizei\nDer Diskurs um Open Science ist hin und wieder sehr aufgeladen: Forschende, die nach Daten fragen oder diese Nutzen um Fehler zu identifizieren, werden als Datenparasiten, Datenpolizei, oder sogar Datenterroristen bezeichnet. Die Aussage, “wer nichts zu verbergen hat, hat auch nichts zu befürchten”, hat einen dystopischen Beigeschmack und Forschende machen sich in politisch aufgebrachten Zeiten und im Angesicht von Plagiatsjägern ungern durchsichtig. Es ist in diesem Kontext aber zu Berücksichtigen, dass es bei Wissenschaft nicht um ein eigenbrötlerisches Hobby handelt, sondern um einen Beruf mit gesellschaftlicher Verantwortung. Wer hier etwas verbirgt, sollte zurecht unter Verdacht stehen, keine ordentliche Wissenschaft zu machen. An der Universitäts- und Landesbibliothek Münster stehen passend dazu an der Außenwand die großen roten Buchstaben: “Gehorche keinem”. Bilde dir stattdessen deine eigene Meinung, geh in die Werke und die Daten der Forschenden und überzeuge dich selbst von der Wahrheit. Wenn Forschende ihre Daten nicht teilen und Artikel hinter Bezahlschranken veröffentlichen, bedeutet das eine unnötige Erschwerung der unabhängigen Meinungsbildung.\n\n\nDaten-Klau\nDaten zu teilen steht oft das Ziel entgegen, anderen Forschenden gegenüber einen Wettbewerbsvorteil zu haben. Das bedeutet, dass Forschende ihre Daten zurückhalten, möglichst viele Artikel auf deren Basis veröffentlichen, und erst wenn dort “nichts mehr zu holen ist”, die Daten teilen. Die Sorge ist, dass andere Forschende schneller darin sind, Forschungsartikel auf Basis der Daten zu veröffentlichen. Faktisch werden die Daten schließlich gar nicht veröffentlicht. Bei dieser Sorge ist wichtig, nachzuvollziehen, dass Daten teilen nicht Daten verschenken bedeutet. Beim Teilen müssen Forschende eine Lizenz angeben, die beispielsweise das Zitieren der Daten vorgibt. Halten sich andere Forschende nicht daran, riskieren sie ihre Karriere. Darüber hinaus ist es einfacher nachzuweisen, wer die Daten ursprünglich erhoben hat, wenn die Person sie frühzeitig veröffentlicht.\n\n\nZitationszahlen\nGelegentlich wird für Open Data damit geworben, dass Forschung dadurch mehr Zitate erhält. Das motiviert einige Wissenschaftler*innen mehr als die gute wissenschaftliche Praxis, ist aber wahrscheinlich nicht (Colavizza u. a. 2020).\n\n\n\n\n\n\nWas bedeutet Langzeitarchivierung?\n\n\n\nDrittmittelgeber fordern gelegentlich Langzeitarchivierung. Das bedeutet je nach Kontext, dass die Daten 20 oder 50 Jahre lange gespeichert werden und abrufbar sein müssen. Eine etwas extreme Variante der Langzeitarchivierung wurde mit Daten auf Github.io durchgeführt: Dort wurden alle Daten, die am 02.02.2020 hochgeladen waren, gespeichert und in einer alten Kohlemine in Norwegen gebracht. Dort sollen sie bis zu 1000 Jahre verbleiben können.\n\n\n\n\n\nReplizierbarkeit erhöhen\nDie bisherigen Ansätze wie Meta-Analysen oder Reproduzierbarkeits-Prüfungen können häufig für bestehende Projekte durchgeführt werden. Bei den folgenden gestaltet sich das allerdings schwieriger - sie sind vor allem für neue Forschung geeignet und haben das Ziel, die Replizierbarkeit von neu veröffentlichten Studien zu erhöhen. Wie im Fazit des Buches erörtert, ist eine allgemeine, fächerübergreifende Aussage darüber, ob diese Vorschläge sich tatsächlich auf Replizierbarkeit auswirken aufgrund von seltenen Replikationsstudien schwierig. Selbst in der Sozialpsychologie sind Replikationen aktuell noch immer die am schwächsten umgesetzte Maßnahme unter allen Empfehlungen zu Open Science (Glöckner u. a. 2024). So oder so ist der Sinn der Maßnahmen, die allgemein die Transparenz von Forschung erhöhen, im Hinblick auf Replikationsschwierigkeiten und P-Hacking deutlich.\n\n\n\n\n\n\nTransparente Berichte\n\n\n\nAczel u. a. (2020) haben eine Transparency Checklist entworfen, die Forschende Punkt für Punkt durchgehen können, um zu prüfen, ob ihr Forschungsbericht transparent ist. In der Online-App (https://www.shinyapps.org/apps/TransparencyChecklist/) kann anschließend ein Bericht daraus generiert werden, der an den Forschungsartikel angehängt werden kann. Die Checkliste ist in die Themen Präregistrierung, Methoden, Ergebnisse, und Daten/Code/Materialien eingeteilt. Beispielsweise wird in Bezug auf die Ergebnisse gefragt, ob die Anzahl an Beobachtungen für alle Gruppen angegeben wurde. Die Checkliste ist aktuell in ca. 30 Sprachen verfügbar, darunter auch Deutsch. Die Liste von Aczel u. a. (2020) ist allerdings primär für quantitative Studien geeignet. Für qualitative und gemischte Studien haben Symonds und Tang (2024) ein Bewertungsschema entworfen.\nEine weitere und kürzere Variante ist die 21-Worte-Lösung. Dabei wird eine vorgeschlagene Erklärung (Simmons, Nelson, und Simonsohn 2012) in den Bericht aufgenommen und versichert, dass keine Studien(ergebnisse) vorenthalten wurden. Sie ist bei weitem nicht so sicher und umfangreich wie die Transparenz-Checkliste, fördert aber niedrigschwellig die Auseinandersetzung mit Transparenz von Forschungsberichten.\nFür Tierstudien wurden die ARRIVE Richtlinien entwickelt. Darauf aufbauend zielen die LAG-R Standards darauf ab, die genetische Ausstatung (genetic make-up) der untersuchten Tiere klar zu berichten (Teboul u. a. 2024).\n\n\n\nPräregistration (Preregistration)\nForschung ist entweder konfirmatorisch (d.h. Forschende haben sich im Vorhinein klare Gedanken dazu gemacht, welche Daten sie erheben, welche Analysen sie rechnen, und was dabei die möglichen Ergebnisse sein könnten), oder exploratorisch (d.h. Forschende versuchen, unvoreingenommen an eine Sache heranzugehen und dabei Fragen für zukünftige, möglicherweise konfirmatorische Forschung, zu generieren). Wann immer Forschung konfirmatorisch ist, sollte sie präregistriert sein. Das bedeutet, dass alle Gedanken, die sich Forschende im Vorhinein gemacht haben sollten, vor der Datenerhebung niedergeschrieben werden sollten. Präregistrierungen sollen damit verhindern, dass sich Forschende in einem Garden of Forking Paths befinden, bzw. helfen sie, den Pfad im Vorhinein festzulegen, damit es im Nachhinein nicht mehr möglich ist, den Pfad zu ändern, um die gewünschten Ergebnisse zu generieren.\n\nEine gute Päregistrierung ist dadurch gekennzeichnet, dass Forschende sich damit aller Freiheitsgrade berauben (Wicherts u. a. 2016) und alle Entscheidungen, die sie auf dem Weg von den Daten zu den Ergebnissen fällen, vorweg nehmen. Darüber hinaus sollten Präregistrierungen gut strukturiert sein, damit andere Personen leicht nachprüfen können, was vorher festgelegt wurde (Simmons, Nelson, und Simonsohn 2020). Um alle nötigen Entscheidungen (z.B. Umgang mit fehlenden Werten oder geplante Anzahl an befragten Personen) strukturiert festzuhalten, gibt es Präregistrierungs-Vorlagen (Templates). Diese existieren für alle möglichen Bereiche, wie sozialpsychologische Experimente (van ’t Veer und Giner-Sorolla 2016) oder Replikationsstudien (Brandt u. a. 2014). Eine Sammlung von über 20 Vorlagen mit Verwendungszwecken ist hier verfügbar. Forschende, die noch wenig Erfahrung in bestimmten Bereichen haben, werden durch das Template unterstützt. Beispielsweise enthalten Preregistration-Templates für Meta-Analysen Übersichten von Literaturdatenbanken und beinhalten bereits Goldstandards zu transparenten Berichten [PRISMA; (DavidMoher?).]. Darüber hinaus können die Unterlagen der Präregistrierung für das Manuskript weiterverwendet werden.\n\n\nPräregistrierung immer mit Analyse-Plan und Analyse-Code\nBezüglich der Verbreitung von Präregistrierungen existiert der Konflikt, ob sie möglichst einfach und ressourcenschonend eingeführt werden sollten, oder ob umfangreiche und aufwändig erstellte Präregistrierungen gefordert werden sollten. Beispielsweise besteht das häufig genutzte Template von aspredicted.org aus nur 11 Fragen und beinhaltet nicht die Möglichkeit, den Analysecode oder andere Dateien anzuhängen. Da der Zweck von Präregistrierungen das Verhindern von P-Hacking ist, sind Kompromisse beim Umfang von Präregistrierungen unsinnig.\nDer Analyse-Code basiert auf einem bestimmten Programm und einer entsprechenden Programmiersprache und führt die Aufbereitung der Daten (z.B. Berechnung von Scores, Ausschluss von Beobachtungen) und die Analysen durch. Traditionell wird er nach Erhebung der Daten geschrieben. Dadurch kann es passieren, dass Studien wertlos sind, weil erst nach der Datenerhebung auffällt, dass die geplanten Analysen nicht möglich sind, und dass Studien so ausgewertet werden, dass dabei die gewünschten Ergebnisse herauskommen (p-hacking). Akker u. a. (2023) konnten für psychologische Forschungsartikel nicht nachweisen, dass Präregistrierungen gegen P-Hacking wirken, haben aber nicht zwischen Präregistrierungen mit und ohne Analyseplan differenziert. Für Registered Reports, bei denen üblicherweise ein ausgearbeiteter Analyseplan gefordert wird, konnten Scheel, Schijen, und Lakens (2021) zeigen, dass der Anteil an hypothesenkonformen Ergebnissen stark absinkt. Brodeur u. a. (2024) konnten mit tausenden statistischen Tests aus wirtschaftswissenschaftlichen Forschungsartikeln nachweisen, dass Präregistrierung nur dann gegen P-Hacking wirkt, wenn ein Analyse-Plan vorliegt. Vom Analyse-Plan unterscheidet sich der Analyse-Code dadurch, dass er unmissverständlich ist. Im Plan könnte beispielsweise stehen “wir berechnen die Korrelation zwischen Variable X und Variable Y und prüfen, ob sie signifikant ist”. Dabei wird nicht spezifiziert, welches Signifikanzniveau verwendet wird, ob ein einseitiger oder zweiseitiger Signifikanztest durchgeführt wird, welche Art der Korrelation berechnet wird (Produkt-Moment, Spearman, Kendall), wie mit fehlenden Werten umgegangen wird, usw. Die Wahrscheinlichkeit, ein signifikantes Ergebnis zu erhalten, wenn die wahre Korrelation 0 ist, steigt dann von dem festgelegten Signifikanzniveau von beispielsweise 5% auf 10%. Das kann mit Programmiercode (z.B. “cor.test(x, y)”) nicht passieren, da dort klar definiert ist, welche Standardeinstellungen verwendet werden, wenn etwas nicht spezifiziert ist. Gutachter*innen von Forschung sollten also klar darauf achten, was alles zur Präregistrierung gehört (Thibault, Pennington, und Munafò 2023).\n\n\nAbweichungen von Präregistrierungen\n„Je planmässiger Menschen vorgehen, desto wirksamer trifft sie der Zufall” heißt es in Friedrich Dürrenmatts 21 Punkten zu “Die Physiker” (durrenmatt2012p?). Auch bei Präregistrierungen gilt, dass Forschende nicht alle Eventualitäten antizipieren können. Es ist also damit zu rechnen, dass aufgrund von Programmierfehlern, Denkfehlern, oder zuvor unberücksichtigten Argumenten anders vorgegangen werden muss, als es in der Präregistrierung festgelegt wurde. Abweichungen von Präregistrierungen kommen sehr häufig vor (Heirene u. a. 2021), beispielsweise ist die tatsächliche Stichprobengröße in mehr als 50% der Fälle nicht genau die Zielstichprobe. Den richtigen Umgang zeichnet dann wieder einmal die Transparenz aus: Jede Abweichung sollte klar kommuniziert werden und es sollte diskutiert werden, weshalb abgewichen wurde, und wie sich die Abweichung auf die Ergebnisse auswirkt (z.B. indem geprüft wird, ob sich Ergebnisse ändern, wenn die Zielstichprobe statt die Gesamtstichprobe mit mehr Beobachtungen als geplant verwendet wird) (Heirene u. a. 2021; siehe auch Daniël Lakens 2024). Aktuell prüfen Gutachter*innen keine Präregistrierungen und keine Abweichungen (Syed 2023; Collaborators u. a. 2021).\n\n\nWo werden Studien präregistriert?\nFür die Präregistrierung von Studien existiert bereits eine breite Infrastruktur. Forschende können Studien beispielsweise über das Open Science Framework (osf.io) präregistrieren und dort ebenfalls Daten, Materialien, Analysen, und Manuskripte hochladen sowie später Pre-Prints veröffentlichen. Projekte und Präregistrierungen können dabei auch privat oder anonymisiert bleiben, wobei für Präregistrierungen ein mehrjähriges Embargo festgelegt werden kann, also eine Zeit, in der die Präregistrierung noch nicht öffentlich ist und nur mit einem speziellen Link abgerufen werden kann. Alle öffentlichen Präregistrierungen werden automatisch von Google Scholar indiziert und sind danach auch auffindbar, zitierbar, und auch in der OSF-Suchmaschine sichtbar. Im OSF können offene Vorlagen aber auch spezifische existierende Vorlagen wie von Brandt u. a. (2014) für die Präregistrierung verwendet werden (weitere Templates werden zukünftig eingearbeitet).\nEin weiterer Anbieter von Präregistrierungen ist aspredicted.org, wobei dort keine Dateien angehängt werden können und die 11 Fragen vor allem für klassische psychologische Untersuchungen passend sind. Medizinische Studien werden - meistens ohne die für eine eigentliche Präregistrierung wichtigen Angaben - via https://clinicaltrials.gov und Meta-Analysen via PROSPERO (http://www.crd.york.ac.uk/PROSPERO/) registriert.\n\n\n\n\n\n\nCheckliste zum Präregistrieren\n\n\n\nVor der Studiendurchführung\n\nMethodik und Analyseplan der Studie wurden vollständig und strukturiert (idealerweise mittels Template) beschrieben\nAnalyseskript wurde mithilfe von Testdaten oder Zufallszahlen geschrieben und funktioniert\nPräregistrierung wurde vor der Studie mit einem Zeitstempel archiviert (z.B. via OSF.io)\n\nNach der Studiendurchführung\n\nLink zur Präregistrierung ist im Manuskript hinterlegt (z.B. beim Methodenteil oder in einem Absatz über Transparent und Offenheit der Studie, idealerweise gepaart mit Links zu Daten und Materialien)\nAlle Abweichungen von der Präregistrierung sind aufgelistet und begründet\n\n\n\n\n\nWas, wenn nicht die gewünschten Ergebnisse herauskommen?\nEine Frage, die mich Forschende häufig im Gespräch über Präregistrierung stellen, demonstriert erneut die Diskrepanz zwischen Forschung, die gut für die Wissenschaft ist, und Forschung, die gut für die Karriere ist. Eine Präregistrierung verhindert, dass Ergebnisse geschönigt werden können (P-Hacking). Dadurch steigt das Risiko, Ergebnisse zu erhalten, die sich nicht gut veröffentlichen lassen, beispielsweise weil sie nicht bahnbrechend oder wie erwartet sind. Während eine Präregistrierung also Forschende auszeichnet, die an guter Wissenschaft interessiert sind, kann es aktuell und in verschiedenen Forschungsdisziplinen für die Karriere negative Folgen haben, Daten nicht zu schönigen.\n\n\nStichprobenumfang planen\nEine weitere Methode, die vorwiegend für konfirmatorische Forschung mit statistischen Methoden zu empfehlen ist, ist die Planung des Stichprobenumfanges. Diese Planung kann auf Ressourcen oder auf Basis anderer Rahmenbedingungen passieren, wird in den Sozialwissenschaften und der Medizin aber häufig mittels Power-Analysen gemacht. Statistische Power ist dabei die Wahrscheinlichkeit, einen Zusammenhang einer gewissen Größe zu finden, wenn tatsächlich ein Zusammenhang vorliegt. Die Logik ist dabei, dass ein Zusammenhang nicht deshalb nicht gefunden werden soll, weil die Studie nicht dafür geeignet war.\nPoweranalysen sind schon lange bekannt und es wird zumindest in der Psychologie regelmäßig kritisiert, dass sie zu selten durchgeführt werden (Cohen 1988, 1992, 2013). Während sie häufig noch nicht oder unzureichend in die Methodenausbildung von Studierenden eingebunden sind, gibt es inzwischen umfangreiche Anleitungen (Daniël Lakens 2021a), Programme (Zhang und Mai 2022; Champely 2020; Faul u. a. 2007, 2009), und Video-Tutorials. Inzwischen wurden Methoden auch für alternative Signifikanztests [Äquivalenztests; Daniël Lakens (2017); Daniël Lakens, Scheel, und Isager (2018)] und Replikationen (Simonsohn 2015) entwickelt. Eine besondere Art der Stichprobenplanung basiert auf Bayesianischer Statistik und wird vor allem in ethisch bedenklichen Situationen wie der Verhaltensforschung bei Tieren empfohlen: Richter (2024) schlägt dazu vor, den sogenannten Bayes-Faktor, der im Gegensatz zu P-Werten der Signifikanztests auch bei fehlenden Zusammenhängen konvergiert, nach jeder Beobachtung oder jeder Versuchsdurchführung zu berechnen und die Studie abzuschließen, sobald ein bestimmter Wert überschritten wurde. Wichtig ist dabei, dass sich dieses Vorgehen explizit nur für Bayesianische Ansätze eignet.\n\n\nEffektstärken statt P-Werte\nEs ist inzwischen klar, dass statistische Methoden häufig missverstanden und falsch gebraucht werden (Perezgonzalez 2014; Daniel Lakens 2021; Gigerenzer 2004). Aktuell verlagert sich die Diskussion von P-Werten (Uygun Tunç, Tunç, und Lakens 2023) hin zu Effektstärken und der Frage, welche Rolle verschiedene Werte spielen. Beispielsweise konnten Vohs u. a. (2021) nachweisen, dass es den umstrittenen Effekt der verschwindenden Selbstkontrolle im Ego Depletion Paradigma gibt (er also nicht gleich 0 ist), allerdings war er in ihrer Untersuchung so klein, dass man alleine für einen erfolgreichen Nachweise über 6.700 Versuchsperson benötigen würde - mehr als jedes Experiment, das jemals zu dem Thema durchgeführt wurde und fast doppelt so viel wie die 3.524 Versuchspersonen von Vohs u. a. (2021). Dabei ist umstritten, ob Forschende sich für leicht nachweisbare Phänomene, sogenannte “niedrig hängende Äpfel” (Baumeister 2020) konzentrieren sollten, ob es eine Grenze gibt, ab der Phänomene praktisch nicht erforschbar und damit nicht von Interesse sind (Primbs u. a. 2023), und welche praktische Relevanz sie haben (Anvari u. a. 2021).\n\n\nCode\nd &lt;- 0.08 # Cohen's d = 0.08, 95% CI [0.01, 0.15], p. 45 hier: https://www.psychologie.uni-wuerzburg.de/fileadmin/06020231/pubs/eder/Vohs_et_al_-paradigmatic_depletion_MS_final.pdf\nn_per_cell &lt;- power.t.test(n = NULL, delta = d, power = .95, type = \"two.sample\", alternative = \"one.sided\")$n\nn_minimum &lt;- ceiling(n_per_cell*2)\n\n\n\n\n\nQualitative Forschung\nBei qualitativer Forschung steht im Vorhinein häufig keine klare Fragestellung fest, sondern sie wird im Forschungsprozess entwickelt. Neben Anonymisierung (Campbell u. a. 2023) nimmt daher auch die Präregistrierung eine besondere Rolle ein. Trotzdem gibt es typische Abläufe, bei denen beispielsweise Entscheidungen über die Kodierung von Antworten getroffen werden - und die lassen sich mit dafür vorgesehenen Templates präregistrieren.\n\n\nOpen Source Software\nForschung ohne Programme zur Literaturrecherche, -verwaltung, Datenanalyse, und Schreiben von wissenschaftlichen Texten ist in den meisten Disziplinen undenkbar. Im Bereich der Software heißt der relevante Bereich “Research Software Engineering” (Technik der Forschungs-Software) und die Herausforderung besteht darin, dass Forschende, obwohl sie bisher kaum in Kontakt mit dem Schreiben von Software (z.B. Analysecode) in Berührung gekommen sind, Expertise in diesem Bereich sammeln. Das beginnt bei allgemeinen Empfehlungen im “Sprachgebrauch” von Programmiersprachen (z.B. Zen of Python) bis hin zum gemeinsamen Zusammenarbeiten über Systeme wie Github oder Gitlab.\nOpen Source bedeutet dabei, dass der Quelltext eines Programmes lesbar und kostenlos weiterverwendbar ist. Somit ist nachvollziehbar, was genau in dem Programm geschieht. Während beispielsweise die Statistiksoftware IBM SPSS keinen solchen Code hat, lässt sich in GNU R jeder Schritt bei der Berechnung statistischer Modelle nachvollziehen. Fehler in der Software können nur bei Open Source Programmen entdeckt werden und gefährden die Wissenschaft (Soergel 2014). Beispielsweise wurde Software zur Auswertung von fMRI-Daten (also einer Art des “Gehirn-Scans”) erst 15 Jahre nach deren Einführung verwendet und deren Qualität war unzureichend (Eklund, Nichols, und Knutsson 2016). Darüber hinaus besteht bei proprietärer Software, also Software, die einem Unternehmen oder einer Person “gehört”, die Gefahr des Lock-In: Personen verlassen sich auf das Programm und stimmen ihre Arbeitsprozesse stark darauf ab. Irgendwann sind sie davon abhängig, können beispielsweise ihre Daten nur noch mit dem einen Programm verwalten oder ein Wechsel würde enorme Kosten nach sich ziehen (Brembs u. a. 2023). Eine Lizenz, anhand der sich Open Source Software erkennen lässt, ist die GNU Lizenz. Weit verbreitete proprietäre Software über die Forschung hinaus ist beispielsweise Microsoft Windows und die Office Pakete, Adobe Acrobat zum Lesen von PDF Dateien, oder Zoom für Videotelefonate. Während die Regierung der Legislaturperiode 2021-2025 sich zum Ziel gemacht hat, Software Projekte als Open-Source-Projekte auszuschreiben, hat sie sich nicht daran gehalten.\nKurz: Für die Forschung, bei welcher die Nachvollziehbarkeit eine vorrangige Rolle spielt, sollte wenn möglich nur auf Open Source Software zurückgegriffen werden. Für manche Methoden ist das aktuell nicht möglich, dort ist es Aufgabe der Hochschulbibliotheken als Verwalterinnen des Wissens (Quan 2021) und Fachgesellschaften als Anlaufstelle der Disziplinen, die nötige Infrastruktur zu stellen oder entwickeln zu lassen.\n\nKommerzielle und Non-Kommerzielle Software\n\n\n\n\n\n\n\nInfrastruktur\nKommerzielle Software\nNon-Kommerzielle Software\n\n\n\n\nLiteraturdatenbank\nSCOPUS\nOpenAlex (Priem, Piwowar, und Orr 2022)\n\n\nLiteraturverwaltung\nCitavi, Mendeley\nZotero (Puckett 2011)\n\n\nDatenerhebung\nUnipark, Millisecond Inquisit\nPsychoPy (Peirce, Hirst, und MacAskill 2022)\n\n\nStatistische Datenanalyse\nIBM SPSS, Stata\nGNU R (R Core Team 2018), PSPP (Yagnik 2014), JASP (Love u. a. 2019)\n\n\nBegutachtung und Veröffentlichung\nEditorial Manager\nOpen Journal System (Willinsky 2005)\n\n\n\n\n\nSlow Science\nViele der Open Science Entwicklungen lassen sich als „langsame Wissenschaft” (oder engl. Slow Science) zusammenfassen. Der traditionellen Massenproduktion von qualitativ minderwertigen Forschungsartikeln steht die achtsame Auseinandersetzung und gründliche Prüfung entgegen. Dass viele der vorgeschlagenen Lösungen im Wettbewerb um veröffentlichte Forschungsartikel nachteilig sind, kritisiert beispielsweise Hyman (2024). Er schlägt stattdessen vor, die Probleme mittels künstlicher Intelligenz zu lösen. Verlage wie Elsevier benutzen diese beispielsweise bereits für Peer Review, wenngleich Modelle wie ChatGPT 4.0 nicht dafür geeignet sind (Thelwall 2024).\n\n\nBig Team Science\nGroßangelegte Forschungsprojekte wie ManyBabies (Byers-Heinlein u. a. 2020), Replikationsprojekte (Open Science Collaboration 2015), Initiativen wie FORRT (Azevedo u. a. 2019), und Software-Entwicklung wie GNU R (R Core Team 2018) haben gezeigt, wozu große Gruppen Forschender fähig sind und dass sich viele der aktuellen Probleme nicht durch einzelne Forschende lösen lassen. In der Physik wird der Rekord an der größten Anzahl an Autor*innen von einer Studie am CERN zum Higgs Boson gehalten. Ungefähr 15 der 33 Seiten bestehen aus den Namen der Beteiligten - mit weiteren Seiten für die Institutionen (Aad u. a. 2015). Dabei wandelt sich das Konzept von Autoren und Autorinnen hin zu Mitwirkenden [Contributors; A. Holcombe (2019)].\nUm klar anzugeben, wer was getan hat, können Forschende die Contributor Roles Taxonomy [CRediT; A. O. Holcombe (2019)] verwenden, die aus standardisierten Rollenbeschreibungen besteht. Mittels Apps können dann einfache Tabellen oder Listen erstellt werden, aus denen die Beiträge aller Mitwirkenden ersichtlich werden (A. O. Holcombe u. a. 2020).\n\n\nWeiterführende Informationen\n\nKepes, Wang, und Cortina (2023) haben einen Anfänger-Leitfaden für die Einschätzung von Publikationsbias entwickelt.\nFür Präklinische Studien verwaltet CAMARADES Berlin ein Wiki zu systematischen Reviews: https://www.camarades.de\nHarrer u. a. (2021) haben ein kostenlos verfügbares Buch zur Durchführung von Meta-Analysen geschrieben: https://bookdown.org/MathiasHarrer/Doing_Meta_Analysis_in_R/\nAdler, Röseler, und Schöniger (2023) haben verschiedene Tools zur heuristischen Beurteilung von Vertrauenswürdigkeit wissenschaftlicher Befunde gesammelt. Auf https://mktg.shinyapps.io/Toolbox_App/ können Forschende Daten eingeben und die verschiedenen Verfahren durchführen.\nWilson u. a. (2017) sammeln Empfehlungen, denen alle Forschenden für das Sicherstellen von Reproduzierbarkeit nachkommen sollten.\nKepes, Wang, und Cortina (2023) haben einen Anfänger-Leitfaden für die Einschätzung von Publikationsbias entwickelt.\nTechnische Hürden beim Teilen von menschlichen Neuroimaging Daten diskutieren Giehl u. a. (2024).\nDie Empfehlungen der Deutschen Gesellschaft für Psychologie (DGPs) zum Umgang mit Forschungsdaten (Schönbrodt, Gollwitzer, und Abele-Brehm 2017) sind online verfügbar: https://www.uni-muenster.de/imperia/md/content/fb7/ethikkommission/dgps_datenmanagement_deu_9.11.16.pdf\nhttps://www.datalad.org ist ein kostenloses Programm, das automatisch dokumentiert, wie aus Daten Wissen gewonnen wird.\nTypische Mythen offener Daten werden in einem Blog Post von Elina Takola diskutiert: https://www.sortee.org/blog/2024/04/12/2024_open_data_myths/\nMit dem FAIR-Aware Quiz (https://fairaware.dans.knaw.nl) können Forschende ihr Wissen zum Teilen von Forschungsdaten testen.\nOffene Lehrmaterialien zu FAIR Data werden von Jürgen Schneider und von der Universitätsbibliothek Mannheim bereitgestellt\nBeispiele für Citizen Science und Möglichkeiten zum Mitmachen sind auf mitforschen.org und Wissenschaft-im-Dialog.de.\nEinen Überblick und Übersetzungen der Contributor Roles Taxonomy gibt es online: https://github.com/contributorshipcollaboration/credit-translation\n\n\n\nLiteratur\n\n\n\n\nAad, Georges, Brad Abbott, Jalal Abdallah, Ovsat Abdinov, Rosemarie Aben, Maris Abolins, OS AbouZeid, u. a. 2015. „Combined Measurement of the Higgs Boson Mass in pp Collisions at s= 7 and 8 TeV with the ATLAS and CMS Experiments“. Physical review letters 114 (19): 191803.\n\n\nAczel, Balazs, Barnabas Szaszi, Alexandra Sarafoglou, Zoltan Kekecs, Šimon Kucharský, Daniel Benjamin, Christopher D. Chambers, u. a. 2020. „A consensus-based transparency checklist“. Nature human behaviour 4 (1): 4–6. https://doi.org/10.1038/s41562-019-0772-6.\n\n\nAdler, Susanne Jana, Lukas Röseler, und Martina Katharina Schöniger. 2023. „A toolbox to evaluate the trustworthiness of published findings“. Journal of Business Research 167: 114189. https://doi.org/10.1016/j.jbusres.2023.114189.\n\n\nAkker, Olmo R van den, Marcel ALM van Assen, Marjan Bakker, Mahmoud Elsherif, Tsz Keung Wong, und Jelte M Wicherts. 2023. „Preregistration in practice: A comparison of preregistered and non-preregistered studies in psychology“. Behavior Research Methods, 1–10.\n\n\nAnvari, Farid, Rogier Kievit, Daniel Lakens, Andrew K Przybylski, Leo Tiokhin, Brenton M Wiernik, und Amy Orben. 2021. „Evaluating the practical relevance of observed effect sizes in psychological research“. Preprint. doi 10.\n\n\nAzevedo, Flavio, Sam Parsons, Leticia Micheli, Julia Feld Strand, Eike Mark Rinke, Samuel Guay, Mahmoud Medhat Elsherif, u. a. 2019. „Introducing a Framework for open and Reproducible Research Training (FORRT)“.\n\n\nBaker, Daniel, Mareike Berg, Kirralise Hansford, Bartholomew Quinn, Federico Gabriele Segala, und Erin English. 2023. „ReproduceMe: lessons from a pilot project on computational reproducibility“.\n\n\nBartoš, František, und Ulrich Schimmack. 2022. „Z-curve 2.0: Estimating Replication Rates and Discovery Rates“. Meta-Psychology 6. https://doi.org/10.15626/MP.2021.2720.\n\n\nBaumeister, Roy. 2020. „Do Effect Sizes in Psychology Laboratory Experiments Mean Anything in Reality?“ https://doi.org/10.31234/osf.io/mpw4t.\n\n\nBenjamin, Daniel J., James O. Berger, Magnus Johannesson, Brian A. Nosek, Eric-Jan Wagenmakers, Richard Berk, Kenneth A. Bollen, u. a. 2018. „Redefine statistical significance“. Nature human behaviour 2 (1): 6–10. https://doi.org/10.1038/s41562-017-0189-z.\n\n\nBrandt, Mark J., Hans IJzerman, Ap Dijksterhuis, Frank J. Farach, Jason Geller, Roger Giner-Sorolla, James A. Grange, Marco Perugini, Jeffrey R. Spies, und Anna van ’t Veer. 2014. „The Replication Recipe: What makes for a convincing replication?“ Journal of Experimental Social Psychology 50: 217–24. https://doi.org/10.1016/j.jesp.2013.10.005.\n\n\nBrembs, Björn, Philippe Huneman, Felix Schönbrodt, Gustav Nilsonne, Toma Susi, Renke Siems, Pandelis Perakakis, Varvara Trachana, Lai Ma, und Sara Rodriguez-Cuadrado. 2023. „Replacing academic journals“. Royal Society Open Science 10 (7). https://doi.org/10.1098/rsos.230206.\n\n\nBrodeur, Abel, Nikolai M Cook, Jonathan S Hartley, und Anthony Heyes. 2024. „Do Preregistration and Preanalysis Plans Reduce p-Hacking and Publication Bias? Evidence from 15,992 Test Statistics and Suggestions for Improvement“. Journal of Political Economy Microeconomics 2 (3): 000–000.\n\n\nBrodeur, Abel, Derek Mikola, und Nikolai Cook. 2024. „Mass Reproducibility and Replicability: A New Hope“.\n\n\nByers-Heinlein, Krista, Christina Bergmann, Catherine Davies, Michael C Frank, J Kiley Hamlin, Melissa Kline, Jonathan F Kominsky, u. a. 2020. „Building a collaborative psychological science: Lessons learned from ManyBabies 1.“ Canadian Psychology/Psychologie Canadienne 61 (4): 349.\n\n\nC. Chang, Andrew, und Phillip Li. 2022. „Is economics research replicable? Sixty published papers from thirteen journals say ‚often not‘“. Crit. Fin. Rev. 11 (1): 185–206.\n\n\nCampbell, Rebecca, McKenzie Javorka, Jasmine Engleton, Kathryn Fishwick, Katie Gregory, und Rachael Goodman-Williams. 2023. „Open-Science guidance for qualitative research: An empirically validated approach for de-identifying sensitive narrative data“. Advances in Methods and Practices in Psychological Science 6 (4): 25152459231205832.\n\n\nCarlisle, JB. 2021. „False individual patient data and zombie randomised controlled trials submitted to Anaesthesia“. Anaesthesia 76 (4): 472–79.\n\n\nCarlsson, Rickard, Lucija Batinović, Natalie Hyltse, André Kalmendal, Thomas Nordström, und Marta Topor. 2024. „A Beginner’s Guide to Open and Reproducible Systematic Reviews in Psychology“.\n\n\nCarlsson, Rickard, Henrik Danielsson, Moritz Heene, Åse Innes-Ker, Daniël Lakens, Ulrich Schimmack, Felix D. Schönbrodt, Marcel van Asssen, und Yana Weinstein. 2017. „Inaugural Editorial of Meta-Psychology“. Meta-Psychology 1: a1001. https://doi.org/10.15626/MP2017.1001.\n\n\nCarter, Evan C., Felix D. Schönbrodt, Will M. Gervais, und Joseph Hilgard. 2019. „Correcting for Bias in Psychology: A Comparison of Meta-Analytic Methods“. Advances in Methods and Practices in Psychological Science 2 (2): 115–44. https://doi.org/10.1177/2515245919847196.\n\n\nChampely, Stephane. 2020. „pwr: Basic Functions for Power Analysis. R package version 1.3-0“. https://CRAN.R-project.org/package=pwr.\n\n\nClaesen, Aline, Wolf Vanpaemel, Anne-Sofie Maerten, Thomas Verliefde, Francis Tuerlinckx, und Tom Heyman. 2023. „Data sharing upon request and statistical consistency errors in psychology: A replication of Wicherts, Bakker and Molenaar (2011)“. Plos one 18 (4): e0284243.\n\n\nCobey, Kelly D, Christophe A Fehlmann, Marina Christ Franco, Ana Patricia Ayala, Lindsey Sikora, Danielle B Rice, Chenchen Xu, u. a. 2023. „Epidemiological characteristics and prevalence rates of research reproducibility across disciplines: a scoping review of articles published in 2018-2019“. Elife 12: e78518.\n\n\nCohen, Jacob. 1988. Statistical Power Analysis for the Behavioral Sciences. Routledge. https://doi.org/10.4324/9780203771587.\n\n\n———. 1992. „A power primer“. Psychological Bulletin 112 (3): 409–10. https://doi.org/10.1037//0033-2909.112.3.409.\n\n\n———. 2013. Statistical Power Analysis for the Behavioral Sciences. 2nd ed. Hoboken: Taylor and Francis.\n\n\nColavizza, Giovanni, Iain Hrynaszkiewicz, Isla Staden, Kirstie Whitaker, und Barbara McGillivray. 2020. „The citation advantage of linking publications to research data“. PloS one 15 (4): e0230416.\n\n\nCollaborators, TARG Meta-Research Group &, Robert T Thibault, Robbie Clark, Hugo Pedder, Olmo van den Akker, Samuel Westwood, Jacqueline Thompson, und Marcus Munafo. 2021. „Estimating the prevalence of discrepancies between study registrations and publications: A systematic review and meta-analyses“. MedRxiv, 2021–07.\n\n\nDang, Junhua, Paul Barker, Anna Baumert, Margriet Bentvelzen, Elliot Berkman, Nita Buchholz, Jacek Buczny, u. a. 2020. „A Multilab Replication of the Ego Depletion Effect“. Social Psychological and Personality Science, 194855061988770. https://doi.org/10.1177/1948550619887702.\n\n\nDe Waard, Anita. 2016. „Research data management at Elsevier: Supporting networks of data and workflows“. Information Services & Use 36 (1-2): 49–55.\n\n\nEklund, Anders, Thomas E Nichols, und Hans Knutsson. 2016. „Cluster failure: Why fMRI inferences for spatial extent have inflated false-positive rates“. Proceedings of the national academy of sciences 113 (28): 7900–7905.\n\n\nErdfelder, Edgar, und Daniel W. Heck. 2019. „Detecting Evidential Value and p -Hacking With the p -Curve Tool“. Zeitschrift für Psychologie 227 (4): 249–60. https://doi.org/10.1027/2151-2604/a000383.\n\n\nFaul, Franz, Edgar Erdfelder, Axel Buchner, und Albert-Georg Lang. 2009. „Statistical power analyses using G*Power 3.1: tests for correlation and regression analyses“. Behavior research methods 41 (4): 1149–60. https://doi.org/10.3758/BRM.41.4.1149.\n\n\nFaul, Franz, Edgar Erdfelder, Albert-Georg Lang, und Axel Buchner. 2007. „G*Power 3: A flexible statistical power analysis program for the social, behavioral, and biomedical sciences“. Behavior research methods 39 (2): 175–91. http://www.gpower.hhu.de/fileadmin/redaktion/Fakultaeten/Mathematisch-Naturwissenschaftliche_Fakultaet/Psychologie/AAP/gpower/GPower3-BRM-Paper.pdf.\n\n\nFeyerabend, Paul K. 1975/2002. Against method. Reprinted der 3. ed. 1993. London: Verso.\n\n\nFišar, Miloš, Ben Greiner, Christoph Huber, Elena Katok, Ali I Ozkes, und Management Science Reproducibility Collaboration. 2024. „Reproducibility in Management Science“. Management Science 70 (3): 1343–56.\n\n\nFletcher, Samuel C. 2022. „Replication is for meta-analysis“. Philosophy of Science 89 (5): 960–69.\n\n\nFricker Jr, Ronald D, Katherine Burke, Xiaoyan Han, und William H Woodall. 2019. „Assessing the statistical analyses used in basic and applied social psychology after their p-value ban“. The American Statistician 73 (sup1): 374–84.\n\n\nFriese, Malte, David D. Loschelder, Karolin Gieseler, Julius Frankenbach, und Michael Inzlicht. 2018. „Is Ego Depletion Real? An Analysis of Arguments“. Personality and social psychology review : an official journal of the Society for Personality and Social Psychology, Inc. https://doi.org/10.1177/1088868318762183.\n\n\nGiehl, Kathrin, Henk-Jan Mutsaerts, Kristien Aarts, Frederik Barkhof, Svenja Caspers, Gaël Chetelat, Marie-Elisabeth Colin, u. a. 2024. „Sharing brain imaging data in the Open Science era: how and why?“ The Lancet Digital Health 6 (7): e526–35.\n\n\nGigerenzer, Gerd. 2004. „Mindless statistics“. The journal of socio-economics 33 (5): 587–606.\n\n\nGlöckner, Andreas, Mario Gollwitzer, Lena Hahn, Jens Lange, Kai Sassenberg, und Christian Unkelbach. 2024. „Quality, replicability, and transparency in research in social psychology: Implementation of recommendations in Germany.“ Social Psychology.\n\n\nHagger, Martin S., Nikos L. D. Chatzisarantis, Hugo Alberts, Calvin Octavianus Anggono, Cédric Batailler, Angela R. Birt, Ralf Brand, u. a. 2016. „A Multilab Preregistered Replication of the Ego-Depletion Effect“. Perspectives on psychological science : a journal of the Association for Psychological Science 11 (4): 546–73. https://doi.org/10.1177/1745691616652873.\n\n\nHagger, Martin S., Chantelle Wood, Chris Stiff, und Nikos L. D. Chatzisarantis. 2010. „Ego depletion and the strength model of self-control: A meta-analysis“. Psychological Bulletin 136 (4): 495–525. https://doi.org/10.1037/a0019486.\n\n\nHarrer, Mathias, Pim Cuijpers, Toshi Furukawa, und David Ebert. 2021. Doing meta-analysis with R: A hands-on guide. Chapman; Hall/CRC.\n\n\nHartgerink, Chris HJ. 2016. „688,112 statistical results: Content mining psychology articles for statistical test results“. Data 1 (3): 14.\n\n\nHeirene, Robert, Debi LaPlante, Eric R. Louderback, Brittany Keen, Marjan Bakker, Anastasia Serafimovska, und Sally M. Gainsbury. 2021. „Preregistration specificity & adherence: A review of preregistered gambling studies & cross-disciplinary comparison“. https://doi.org/10.31234/osf.io/nj4es.\n\n\nHolcombe, Alex. 2019. „Farewell authors, hello contributors“. Nature 571 (7763): 147–48.\n\n\nHolcombe, Alex O. 2019. „Contributorship, not authorship: Use CRediT to indicate who did what“. Publications 7 (3): 48.\n\n\nHolcombe, Alex O, Marton Kovacs, Frederik Aust, und Balazs Aczel. 2020. „Documenting contributions to scholarly articles using CRediT and tenzing“. PLoS One 15 (12): e0244611.\n\n\nHyman, Michael. 2024. „Freeing Social and Medical Scientists from the Replication Crisis“. Available at SSRN 4898637.\n\n\nJacobsen, Nadine Svenja Josee, Daniel Kristanto, Suong Welp, Yusuf Cosku Inceler, und Stefan Debener. 2024. „Preprocessing Choices for P3 Analyses with Mobile EEG: A Systematic Literature Review and Interactive Exploration“. bioRxiv, 2024–04.\n\n\nKepes, Sven, Brad J. Bushman, und Craig A. Anderson. 2017. „Violent video game effects remain a societal concern: Reply to Hilgard, Engelhardt, and Rouder (2017)“. Psychological Bulletin 143 (7): 775–82. https://doi.org/10.1037/bul0000112.\n\n\nKepes, Sven, Wenhao Wang, und Jose M Cortina. 2023. „Assessing publication bias: A 7-step user’s guide with best-practice recommendations“. Journal of Business and Psychology 38 (5): 957–82.\n\n\nKörner, Robert, Lukas Röseler, Astrid Schütz, und Brad J. Bushman. 2022. „Dominance and prestige: Meta-analytic review of experimentally induced body position effects on behavioral, self-report, and physiological dependent variables“. Psychological Bulletin 148 (1-2): 67–85. https://doi.org/10.1037/bul0000356.\n\n\nLakens, Daniel. 2021. „The practical alternative to the p value is the correctly used p value“. Perspectives on psychological science 16 (3): 639–48.\n\n\n———. 2023. „Concerns about Replicability, Theorizing, Applicability, Generalizability, and Methodology across Two Crises in Social Psychology“. https://doi.org/10.31234/osf.io/dtvs7.\n\n\nLakens, Daniël. 2017. „Equivalence Tests: A Practical Primer for t Tests, Correlations, and Meta-Analyses“. Social Psychological and Personality Science 8 (4): 355–62. https://doi.org/10.1177/1948550617697177.\n\n\n———. 2021a. „Sample Size Justification“. https://doi.org/10.31234/osf.io/9d3yf.\n\n\n———. 2021b. „The Practical Alternative to the p Value Is the Correctly Used p Value“. Perspectives on psychological science : a journal of the Association for Psychological Science 16 (3): 639–48. https://doi.org/10.1177/1745691620958012.\n\n\n———. 2024. „When and How to Deviate from a Preregistration“. Collabra: Psychology 10 (1).\n\n\nLakens, Daniël, und Alexander J. Etz. 2017. „Too True to be Bad: When Sets of Studies With Significant and Nonsignificant Findings Are Probably True“. Social Psychological and Personality Science 8 (8): 875–81. https://doi.org/10.1177/1948550617693058.\n\n\nLakens, Daniël, Anne M. Scheel, und Peder M. Isager. 2018. „Equivalence Testing for Psychological Research: A Tutorial“. Advances in Methods and Practices in Psychological Science 1 (2): 259–69. https://doi.org/10.1177/2515245918770963.\n\n\nLevy, Neil. 2022. „Do your own research!“ Synthese 200 (5): 356.\n\n\nLight, Richard J, und David B Pillemer. 1984. Summing up: The science of reviewing research. Harvard University Press.\n\n\nLindsay, D Stephen. 2023. „A plea to psychology professional societies that publish journals: Assess computational reproducibility“. Meta-Psychology 7.\n\n\nLove, Jonathon, Ravi Selker, Maarten Marsman, Tahira Jamil, Damian Dropmann, Josine Verhagen, Alexander Ly, u. a. 2019. „JASP: Graphical statistical software for common statistical designs“. Journal of Statistical Software 88: 1–17.\n\n\nMaier, Maximilian, František Bartoš, Nichola Raihani, David R Shanks, TD Stanley, Eric-Jan Wagenmakers, und Adam JL Harris. 2024. „Exploring open science practices in behavioural public policy research“. Royal Society Open Science 11 (2): 231486.\n\n\nOpen Science Collaboration. 2015. „Psychology: Estimating the reproducibility of psychological science“. Science (New York, N.Y.) 349 (6251): aac4716. https://doi.org/10.1126/science.aac4716.\n\n\nPeirce, Jonathan, Rebecca Hirst, und Michael MacAskill. 2022. Building experiments in PsychoPy. Sage.\n\n\nPerezgonzalez, Jose D. 2014. „A reconceptualization of significance testing“. Theory & Psychology 24 (6): 852–59.\n\n\nPriem, Jason, Heather Piwowar, und Richard Orr. 2022. „OpenAlex: A fully-open index of scholarly works, authors, venues, institutions, and concepts“. arXiv preprint arXiv:2205.01833.\n\n\nPrimbs, Maximilian A, Charlotte R Pennington, Daniël Lakens, Miguel Alejandro A Silan, Dwayne SN Lieck, Patrick S Forscher, Erin M Buchanan, und Samuel J Westwood. 2023. „Are small effects the indispensable foundation for a cumulative psychological science? A reply to Götz et al.(2022)“. Perspectives on Psychological Science 18 (2): 508–12.\n\n\n„Promoting reproduction and replication at scale“. 2024. Nat. Hum. Behav. 8 (1): 1.\n\n\nPuckett, Jason. 2011. Zotero: A guide for librarians, researchers, and educators. Assoc of Cllge & Rsrch Libr.\n\n\nQuan, Joshua. 2021. „Toward reproducibility: Academic libraries and open science“. Cambridge University Press.\n\n\nR Core Team. 2018. „R: A language and environment for statistical computing.“ Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.\n\n\nRichter, S Helene. 2024. „Challenging current scientific practice: how a shift in research methodology could reduce animal use“. Lab Animal 53 (1): 9–12.\n\n\nRodriguez, Josue E, und Donald R Williams. 2022. „Psymetadata: An R package containing open datasets from meta-analyses in psychology“.\n\n\nRöseler, Lukas. 2023. „Predicting Replication Rates with Z-Curve: A Brief Exploratory Validation Study Using the Replication Database“.\n\n\nScheel, Anne M, Mitchell RMJ Schijen, und Daniël Lakens. 2021. „An excess of positive results: Comparing the standard psychology literature with registered reports“. Advances in Methods and Practices in Psychological Science 4 (2): 25152459211007467.\n\n\nSchönbrodt, Felix, Mario Gollwitzer, und Andrea Abele-Brehm. 2017. „Der Umgang mit Forschungsdaten im Fach Psychologie: Konkretisierung der DFG-Leitlinien“. Psychologische Rundschau.\n\n\nSimmons, Joseph P., Leif D. Nelson, und Uri Simonsohn. 2012. „A 21 Word Solution“. SSRN Electronic Journal. https://doi.org/10.2139/ssrn.2160588.\n\n\nSimmons, Joseph P., Leif Nelson, und Uri Simonsohn. 2020. „Pre–registration Is A Game Changer. But, Like Random Assignment, It Is Neither Necessary Nor Sufficient For Credible Science“. Journal of Consumer Psychology. https://doi.org/10.1002/jcpy.1207.\n\n\nSimmons, Joseph P., und Uri Simonsohn. 2017. „Power Posing: P-Curving the Evidence“. Psychological Science 28 (5): 687–93. https://doi.org/10.1177/0956797616658563.\n\n\nSimonsohn, Uri. 2015. „Small telescopes: detectability and the evaluation of replication results“. Psychological Science 26 (5): 559–69. https://doi.org/10.1177/0956797614567341.\n\n\nSimonsohn, Uri, Leif D. Nelson, und Joseph P. Simmons. 2014a. „p-Curve and Effect Size: Correcting for Publication Bias Using Only Significant Results“. Perspectives on psychological science : a journal of the Association for Psychological Science 9 (6): 666–81. https://doi.org/10.1177/1745691614553988.\n\n\n———. 2014b. „P-curve: A key to the file-drawer“. Journal of experimental psychology. General 143 (2): 534–47. https://doi.org/10.1037/a0033242.\n\n\nSimonsohn, Uri, Joseph P. Simmons, und Leif D. Nelson. 2015. „Better P-curves: Making P-curve analysis more robust to errors, fraud, and ambitious P-hacking, a Reply to Ulrich and Miller (2015)“. Journal of experimental psychology. General 144 (6): 1146–52. https://doi.org/10.1037/xge0000104.\n\n\nSoergel, David AW. 2014. „Rampant software errors may undermine scientific results“. F1000Research 3.\n\n\nSotola, Lukas Karel. 2023. „How Can I Study from Below, that which Is Above?“ Meta-Psychology 7. https://doi.org/10.15626/MP.2022.3299.\n\n\nSotola, Lukas Karel, und Marcus Credé. 2022. „On the predicted replicability of two decades of experimental research on system justification: A Z–curve analysis“. European Journal of Social Psychology 52 (5-6): 895–909. https://doi.org/10.1002/ejsp.2858.\n\n\nSyed, Moin. 2023. „Some data indicating that editors and reviewers do not check preregistrations during the review process“. PsyArXiv Preprints.\n\n\nSymonds, Jennifer E, und Xin Tang. 2024. „Quality Appraisal Checklist for Quantitative, Qualitative, and Mixed Methods Studies“.\n\n\nTeboul, Lydia, James Amos-Landgraf, Fernando J Benavides, Marie-Christine Birling, Steve DM Brown, Elizabeth Bryda, Rosie Bunton-Stasyshyn, u. a. 2024. „Improving laboratory animal genetic reporting: LAG-R guidelines“. Nature Communications 15 (1): 5574.\n\n\nThelwall, Mike. 2024. „Can ChatGPT evaluate research quality?“ Journal of Data and Information Science.\n\n\nThibault, Robert T, Charlotte R Pennington, und Marcus R Munafò. 2023. „Reflections on preregistration: core criteria, badges, complementary workflows“. Journal of Trial & Error 2 (1): 10–36850.\n\n\nThibault, Robert T, Emmanuel A Zavalis, Mario Malicki, und Hugo Pedder. 2024. „An evaluation of reproducibility and errors in published sample size calculations performed using G* Power“. medRxiv, 2024–07.\n\n\nUygun Tunç, Duygu, Mehmet Necip Tunç, und Daniël Lakens. 2023. „The epistemic and pragmatic function of dichotomous claims based on statistical hypothesis tests“. Theory & Psychology 33 (3): 403–23.\n\n\nvan ’t Veer, Anna Elisabeth, und Roger Giner-Sorolla. 2016. „Pre-registration in social psychology—A discussion and suggested template“. Journal of Experimental Social Psychology 67: 2–12. https://doi.org/10.1016/j.jesp.2016.03.004.\n\n\nVanpaemel, Wolf, Maarten Vermorgen, Leen Deriemaecker, und Gert Storms. 2015. „Are we wasting a good crisis? The availability of psychological research data after the storm“. Collabra 1 (1): 3.\n\n\nVohs, Kathleen D, Brandon J Schmeichel, Sophie Lohmann, Quentin F Gronau, Anna J Finley, Sarah E Ainsworth, Jessica L Alquist, u. a. 2021. „A multisite preregistered paradigmatic test of the ego-depletion effect“. Psychological Science 32 (10): 1566–81.\n\n\nWagenmakers, Eric-Jan. 2007. „A practical solution to the pervasive problems ofp values“. Psychonomic Bulletin & Review 14 (5): 779–804. https://doi.org/10.3758/BF03194105.\n\n\nWicherts, Jelte M., Coosje L. S. Veldkamp, Hilde E. M. Augusteijn, Marjan Bakker, Robbie C. M. van Aert, und Marcel A. L. M. van Assen. 2016. „Degrees of Freedom in Planning, Running, Analyzing, and Reporting Psychological Studies: A Checklist to Avoid p-Hacking“. Frontiers in psychology 7: 1832. https://doi.org/10.3389/fpsyg.2016.01832.\n\n\nWillinsky, John. 2005. „Open journal systems: An example of open source software for journal management and publishing“. Library hi tech 23 (4): 504–19.\n\n\nWilson, Greg, Jennifer Bryan, Karen Cranston, Justin Kitzes, Lex Nederbragt, und Tracy K Teal. 2017. „Good enough practices in scientific computing“. PLoS computational biology 13 (6): e1005510.\n\n\nYagnik, Jignasu. 2014. „PSPP a free and open source tool for data analysis“.\n\n\nZhang, Z., und Y. Mai. 2022. „WebPower: Basic and Advanced Statistical Power Analysis (R package version 0.7)“. https://CRAN.R-project.org/package=WebPower.",
    "crumbs": [
      "Lösungen",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Methoden</span>"
    ]
  },
  {
    "objectID": "lösungen_theorien.html",
    "href": "lösungen_theorien.html",
    "title": "Theorien",
    "section": "",
    "text": "Theorien übernehmen in der Wissenschaft oft die Funktion eines Wegweisers. Durch sie lässt sich einschätzen, wie die Ergebnisse einer Untersuchung aussehen, bevor sie durchgeführt werden. Falls die Ergebnisse also nicht wie erwartet sind, zum Beispiel wenn eine Replikation fehlschlägt, besteht der Verdacht, die Theorie könnte falsch oder unvollständig sein. In der Psychologie, in welcher Theorien häufig in Alltagssprache und nicht formalisiert (also als mathematische Formeln) notiert werden, ist der Sinn von formalen Varianten schon lange diskutiert (Meehl 2004; Platt 1964; Wilcox und Rousselet 2024) und Replikationsfehlschläge werden darauf zurückgeführt, dass die Theorien zu unpräzise, missverständlich, und nicht gut ausgearbeitet sind. Es herrscht das Motto, Theorien seien wie Zahnbürsten und man solle niemals die von jemand anderem nehmen. Daran, dass das Problem seit Jahrzehnten beschrieben wird, hat sich wenig geändert (Lakens 2023; Muthukrishna und Henrich 2019). Inzwischen gibt es jedoch Bemühungen, das Problem auch zu lösen (Sion und Earp 2024; Erkan und Devezer 2023), beispielsweise durch das herausstellen der Vorteile von bekannten formalen Theorien (Robinaugh u. a. 2021), Entwicklung von Werkzeugen zur Formalisierung von Theorien (https://www.theorymaps.org), oder durch einzelne Forschende. Der Wert von Replikationsstudien wird im Rahmen der „Theorie-Krise” darin gesehen, dass nur durch wiederholte Untersuchung festgestellt werden kann, unter welchen Bedingungen ein bestimmtes Phänomen nachweisbar ist (Erkan und Devezer 2023).\n\nSpezifizierung und Formalisierung\nDas Problem einer Theorie in Alltagssprache lässt sich wie folgt veranschaulichen: Wagenmakers u. a. (2016) versuchten, eine Studie zur Facial Feedback Hypothese zu replizieren. Die Hypothese besagt, dass Personen, wenn sie einen Stift so im Mund halten, dass dabei diejenigen Muskeln beansprucht werden, die auch beim Grinsen genutzt werden, Dinge lustiger finden, als wenn sie den Stift anders im Mund halten. Damit geprüft werden konnte, ob Versuchspersonen ihre Stifte richtig in den Mund nahmen, wurden sie in der Replikationsstudie gefilmt. Das war in der Originalstudie nicht der Fall. Die Replikationsstudien schlugen fehl und der Unterschied der Kamera wurde von den Autoren der Originalstudie als Grund genannt, weshalb die Replikation fehlschlug (Strack 2016). Ob und wie die Präsenz einer Kamera in dem Versuchsaufbau relevant ist, war nicht Teil der Theorie beziehungsweise Hypothese. Durch die Formulierung der Theorie in gewöhnlicher Sprache können unendlich viele solcher „Post Hoc” Erklärungen in die Diskussion eingebracht werden, die jegliche Replikationsfehlschläge relativieren. Bei einer formalen Theorie sind solche Missverständnisse seltener.\nAus dem Problem, dass Theorien schwer verständlich und wenig informativ sind, folgt die Tatsache, dass die meisten Theorien nie falsifiziert werden, aber auch nicht weiter untersucht werden („Zombie-Theorien”, Ferguson und Heene (2012)). Ähnlich verhält es sich mit psychologischen Skalen, bei denen die meisten nur einmal verwendet werden (Elson u. a. 2023). Flexible Theorien vereinfachen außerdem das p-Hacking: In der Theorie ist keine Vorgabe zum Aufbereiten der Daten, also kann diejenige Variante gewählt werden, die die Theorie bestätigt (Van den Akker u. a. 2022).\n\n\nGegnerische Kollaborationen\nEin Ansatz, bei dem verhindert werden soll, dass uninformative Studien durchgeführt werden und sich Diskussionen auf Verstehensweisen von Theorien fokussieren, sind gegnerische Kollaborationen (adversarial collaborations, Cleeremans (2022)). Dabei arbeiten Vertreter*innen von zwei inkompatiblen Standpunkten miteinander und erstellen beispielsweise eine Studie, die den Konflikt beilegen soll (Cowan u. a. 2020; Mellers, Hertwig, und Kahneman 2001).\n\nWeiterführende Informationen\n\nTrafimow und Earp (2016) beschreiben, wie Replikationen auch ohne Theorie möglich sind.\n\n\n\n\nLiteratur\n\n\n\n\nCleeremans, Axel. 2022. „Theory as adversarial collaboration“. Nature Human Behaviour 6 (4): 485–86.\n\n\nCowan, Nelson, Clément Belletier, Jason M. Doherty, Agnieszka J. Jaroslawska, Stephen Rhodes, Alicia Forsberg, Moshe Naveh-Benjamin, Pierre Barrouillet, Valérie Camos, und Robert H. Logie. 2020. „How Do Scientific Views Change? Notes From an Extended Adversarial Collaboration“. Perspectives on psychological science : a journal of the Association for Psychological Science 15 (4): 1011–25. https://doi.org/10.1177/1745691620906415.\n\n\nElson, Malte, Ian Hussey, Taym Alsalti, und Ruben C. Arslan. 2023. „Psychological measures aren’t toothbrushes“. Communications Psychology 1 (1). https://doi.org/10.1038/s44271-023-00026-9.\n\n\nErkan, Buzbas, und Berna Devezer. 2023. „Tension between theory and practice of replication“. J. Trial Error 4 (1): 73–81.\n\n\nFerguson, Christopher J., und Moritz Heene. 2012. „A Vast Graveyard of Undead Theories: Publication Bias and Psychological Science’s Aversion to the Null“. Perspectives on psychological science : a journal of the Association for Psychological Science 7 (6): 555–61. https://doi.org/10.1177/1745691612459059.\n\n\nLakens, Daniel. 2023. „Concerns about Replicability, Theorizing, Applicability, Generalizability, and Methodology across Two Crises in Social Psychology“. https://doi.org/10.31234/osf.io/dtvs7.\n\n\nMeehl, Paul E. 2004. „Theoretical risks and tabular asterisks: Sir Karl, Sir Ronald, and the slow progress of soft psychology“. Applied and Preventive Psychology 11 (1): 1. https://doi.org/10.1016/j.appsy.2004.02.001.\n\n\nMellers, Barbara, Ralph Hertwig, und Daniel Kahneman. 2001. „Do frequency representations eliminate conjunction effects? An exercise in adversarial collaboration“. Psychological Science 12 (4): 269–75.\n\n\nMuthukrishna, Michael, und Joseph Henrich. 2019. „A problem in theory“. Nature Human Behaviour 349 (Suppl 1): aac4716. https://doi.org/10.1038/s41562-018-0522-1.\n\n\nPlatt, John R. 1964. „Strong Inference: Certain systematic methods of scientific thinking may produce much more rapid progress than others.“ science 146 (3642): 347–53.\n\n\nRobinaugh, Donald J., Jonas M. B. Haslbeck, Oisín Ryan, Eiko I. Fried, und Lourens J. Waldorp. 2021. „Invisible Hands and Fine Calipers: A Call to Use Formal Theory as a Toolkit for Theory Construction“. Perspectives on psychological science : a journal of the Association for Psychological Science 16 (4): 725–43. https://doi.org/10.1177/1745691620974697.\n\n\nSion, Tomos G ap, und Brian D Earp. 2024. „Replication and theory development in the social and psychological sciences“.\n\n\nStrack, Fritz. 2016. „Reflection on the Smiling Registered Replication Report“. Perspectives on psychological science : a journal of the Association for Psychological Science 11 (6): 929–30. https://doi.org/10.1177/1745691616674460.\n\n\nTrafimow, David, und Brian D Earp. 2016. „Badly specified theories are not responsible for the replication crisis in social psychology: Comment on Klein“. Theory Psychol. 26 (4): 540–48.\n\n\nVan den Akker, Olmo, Marcel A L M van Assen, Manon Enting, M de Jonge, How Hwee Ong, Franziska Frauke Rüffer, Martijn Schoenmakers, Andrea Helena Stoevenbelt, Jelte M Wicherts, und Marjan Bakker. 2022. „Selective hypothesis reporting in psychology: Comparing preregistrations and corresponding publications“. BITSS.\n\n\nWagenmakers, Eric-Jan, Titia Beek, Laura Dijkhoff, Quentin F. Gronau, Alberto Acosta, Reginald B. Adams, Daniel N. Albohn, u. a. 2016. „Registered Replication Report: Strack, Martin, & Stepper (1988)“. Perspectives on psychological science : a journal of the Association for Psychological Science 11 (6): 917–28. https://doi.org/10.1177/1745691616674458.\n\n\nWilcox, Rand R, und Guillaume A Rousselet. 2024. „More reasons why replication is A difficult issue“.",
    "crumbs": [
      "Lösungen",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Theorien</span>"
    ]
  },
  {
    "objectID": "lösungen_welt.html",
    "href": "lösungen_welt.html",
    "title": "Die Welt",
    "section": "",
    "text": "Nachdem nun systemische, methodische, und theoretische Gründe für die Replikationskrise diskutiert wurden, endet die Diskussion mit einem allgemeinen und ontologischen Punkt: Die Erwartung, dass sich Befunde wiederholt feststellen lassen müssen beruht auf der Annahme oder Theorie, dass die Welt – oder zumindest die jeweils in der Wissenschaft untersuchte Gesetzmäßigkeit – stabil ist. Bei fluktuierenden Mustern in der Welt oder im menschlichen Verhalten sind Replikationsfehlschläge wenig überraschend. Für die Psychologie hat beispielsweise Smedslund (2015) kritisiert, dass diese Annahme falsch ist und hat diese Historizität als einen der Gründe angeführt, weshalb Psychologie keine Naturwissenschaft sein kann. Ähnlich argumentierte ebenfalls Gergen (1973). Beide Sichtweisen haben ihren Ursprung möglicherweise in der Grundlagendiskussion der Psychologie: Anfang des 20. Jahrhunderts, als die Psychologie sich langsam von der Philosophie distanzierte und vermehrt empirisch (also durch Beobachtungen statt nur Überlegungen) arbeitete, unterteilte Dilthey und Riedel (1970) Naturwissenschaften und Geisteswissenschaften dahingehend, dass in es in den Naturwissenschaften nicht um das Nacherleben und Begreifen nachrangig sind und es um abstrakte und allgemeine (nomothetische) Gesetzmäßigkeiten geht. Geisteswissenschaften hingegen betrachten Vorgeänge in einem konkreten und einzigartigen (idiographischen) Zusammenhang mit allen dazugehörigen Besonderheiten. Es etablierte sich langfristig der naturwissenschaftliche Ansatz, der bereits 50 Jahre zuvor fruchtbare Erkenntnisse lieferte (Fechner 1860).\nEine besondere Rolle beim Verallgemeinern von einzelnen Beobachtungen auf allgemeingültige Gesetzmäßigkeiten spielt auch der Zufall. Das Besondere bei statistischen Methoden ist beispielsweise, dass die Unsicherheit als Zahl ausgedrückt wird und damit quantifiziert wird. Die wichtige Frage im Rahmen von Replikationsproblemen ist, ob die Unterschiede zwischen Befunden zufällig oder aufgrund irgendwelche Unterschiede entstanden sind. Diese zwei Arten des Zufalls werden als ontologischer und epistemischer Zufall bezeichnet. Ontologischer Zufall bedeutet, dass ein einfach keinen Grund gibt, während wir ihn beim epistemologischen Zufall bloß nicht kennen. Der Ausgang bei einem Münzwurf, also ob die Münze auf Kopf oder Zahl landet, ist beispielsweise ein Fall epistemischen Zufalls: Würden wir alle wichtigen Parameter kennen (Höhe der Hand, Stärke des Wurfes, welche Seite beim Werfen oben liegt, usw.), könnten wir eindeutig vorhersagen, wie die Münze landet. Übrigens landen faire Münzen meistens auf der Seite, mit der sie gestartet sind (Bartoš u. a. 2023).\n\nLiteratur\n\n\n\n\nBartoš, František, Alexandra Sarafoglou, Henrik R Godmann, Amir Sahrani, David Klein Leunk, Pierre Y Gui, David Voss, u. a. 2023. „Fair coins tend to land on the same side they started: Evidence from 350,757 flips“.\n\n\nDilthey, Wilhelm, und Manfred Riedel. 1970. Der Aufbau der geschichtlichen Welt in den Geisteswissenschaften. Suhrkamp Frankfurt am Main.\n\n\nFechner, Gustav Theodor. 1860. Elemente der Psychophysik. Leipzig: Breitkopf und Härtel.\n\n\nGergen, Kenneth J. 1973. „Social psychology as history.“ Journal of personality and social psychology 26 (2): 309.\n\n\nSmedslund, Jan. 2015. „Why psychology cannot be an empirical science“. Integrative psychological & behavioral science. https://doi.org/10.1007/s12124-015-9339-x.",
    "crumbs": [
      "Lösungen",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Die Welt</span>"
    ]
  },
  {
    "objectID": "einleitung.html",
    "href": "einleitung.html",
    "title": "Einleitung",
    "section": "",
    "text": "Nachvollziehbare Wissenschaft\nBeginnen wir mit einem hypothetischen Beispiel: In Deutschland könnte der Standard zur Organspende verändert werden. So wie es momentan ist, könnten auch nach der Veränderung alle weiterhin frei entscheiden, ob sie ihre Organe spenden möchten oder nicht. Wie in Österreich wäre die Alternative eine “Widerspruchslösung”, das heißt, alle Personen sind per se Organspender und müssen diesem Status aktiv widersprechen. Beim aktuellen Modell müssen sie stattdessen aktiv zustimmen (zum Beispiel durch Bestellen eines Organspendeausweises).\nDie Basis für eine solche hypothetische und politische Entscheidung wäre ein wissenschaftlicher Befund: Bei der Widerspruchslösung ist der Anteil an Organspender*innen1 höher, als wenn sich Personen aktiv um den Status “Organspender*in” bemühen müssen. Den ursprünglichen Befund berichteten Johnson und Goldstein (2003). Fast 20 Jahre später konnten Chandrashekar u. a. (2023) den Befund mit neu erhobenen Daten erneut nachweisen, ihn also replizieren. Alle Informationen zur Replikation lassen sich online nachlesen und ihre Ergebnisse sind online vollständig einsehbar. Mit kostenlosen Programmen lässt sich alles nachrechnen und prüfen, was seitens der Fachzeitschrift, in der die Ergebnisse veröffentlicht wurden, bereits von einer unabhängigen Person getan wurde. Darüber hinaus gilt der Effekt nicht nur für Organspende sondern auch für alle möglichen anderen Entscheidungen von Menschen. Das allgemeine Phänomen heißt Default Effect. Die wissenschaftliche Basis der politischen Entscheidung ist hier also überprüfbar und nachvollziehbar.\nDie Transparenz des wissenschaftlichen Befundes ist in diesem Beispiel deutlich gegeben. Doch leider ist das nicht repräsentativ für Wissenschaft. Häufig lassen sich Forschungsbefunde nicht einfach nachlesen, sondern Forschungsberichte müssen von Fachzeitschriften gekauft werden (bzw. bei den Autor*innen angefragt per Mail werden), Daten müssen angefragt werden, und Ergebnisse lassen sich nur mithilfe von kostspieligen Programmen nachrechnen. Diese Offenheit spielt zur Zeit eine besondere Rolle: Es herrscht eine Skepsis gegenüber Wissenschaft (Sultan u. a. 2024) und politische Entscheidungen über Heizungen, Impfungen, Ernährung, oder Mobilität stützen sich auf Wissenschaft, werden gleichzeitig aber als falsch, kostspielig, oder bedrohlich wahrgenommen. Irreführende oder schlicht falsche Informationen werden verbreitet und Menschen nutzen künstliche Intelligenz (Large Language Models), welche auf potenziell falschen Informationen basieren und nicht fähig sind, die Herkunft der Informationen anzugeben.\nWie sich die Offenheit von Wissenschaft ändert, was Forschende auf der ganzen Welt dafür tun, Wissenschaft zu öffnen, und welche Lösungsvorschläge diesbezüglich diskutiert und umgesetzt werden, ist Gegenstand dieses Buches. Ein Überblick über Dimensionen von Offenheit in Anlehnung an Silveira u. a. (2023) ist in der folgenden Tabelle. Dabei befinden sich viele wissenschaftliche Disziplinen zwischen den Extrema der “klassischen Wissenschaft” und der “offenen Wissenschaft”. Letztere stellt dabei das Ideal dar, also den Zustand, der für wissenschaftlichen Fortschritt am besten wäre.",
    "crumbs": [
      "Einleitung"
    ]
  },
  {
    "objectID": "einleitung.html#literatur",
    "href": "einleitung.html#literatur",
    "title": "Einleitung",
    "section": "Literatur",
    "text": "Literatur\n\n\n\n\nBrohmer, Hilmar, Gabriela Hofer, Sebastian A Bauch, Julia Beitner, Jana Berkessel, Katja Corcoran, David Garcia, u. a. 2024. „Effects of the generic masculine and its alternatives in germanophone countries - A multi-lab replication and extension of Stahlberg, Sczesny, and Braun, 2001“.\n\n\nChandrashekar, Subramanya Prasad, Nadia Adelina, Shiyuan Zeng, Yan Ying Esther Chiu, Grace Yat Sum Leung, Paul Henne, Bo Ley Cheng, und Gilad Feldman. 2023. „Defaults versus framing: Revisiting Default Effect and Framing Effect with replications and extensions of Johnson and Goldstein (2003) and Johnson, Bellman, and Lohse (2002)“. Meta-Psychology 7 (Juli).\n\n\nJohnson, Eric J, und Daniel Goldstein. 2003. „Medicine. Do defaults save lives?“ Science 302 (5649): 1338–39.\n\n\nSilveira, Lúcia da, Nivaldo Calixto Ribeiro, Remedios Melero, Andrea Mora-Campos, Daniel Fernando Piraquive-Piraquive, Alejandro Uribe Tirado, Priscila Machado Borges Sena, u. a. 2023. „Taxonomia da Ciência Aberta: revisada e ampliada“. Encontros Bibli Rev. Eletrônica Bibliotecon. Ciênc. Inf. 28 (Juni).\n\n\nSultan, Mubashir, Alan Novaes Tump, Nina Ehmann, Philipp Lorenz-Spreen, Ralph Hertwig, Anton Gollwitzer, und Ralf Kurvers. 2024. „Susceptibility to online misinformation: A systematic meta-analysis of demographic and psychological factors“. PsyArXiv.",
    "crumbs": [
      "Einleitung"
    ]
  },
  {
    "objectID": "einleitung.html#footnotes",
    "href": "einleitung.html#footnotes",
    "title": "Einleitung",
    "section": "",
    "text": "Die Entscheidung, auf das generische Maskulin zugunsten des Gender-Sternchens zu verzichten, beruht auf der wissenschaftlichen Tatsache, dass sich damit besser das Ziel erreichen lässt, Personen verschiedener Geschlechter gleichermaßen anzusprechen(Brohmer u. a. 2024).↩︎",
    "crumbs": [
      "Einleitung"
    ]
  },
  {
    "objectID": "geschichte.html",
    "href": "geschichte.html",
    "title": "Die Geschichte der Open Science-Bewegung",
    "section": "",
    "text": "Vertiefende Informationen",
    "crumbs": [
      "Die Geschichte der Open Science-Bewegung"
    ]
  },
  {
    "objectID": "geschichte.html#vertiefende-informationen",
    "href": "geschichte.html#vertiefende-informationen",
    "title": "Die Geschichte der Open Science-Bewegung",
    "section": "",
    "text": "Die Geschichte der Replikationskrise beschreiben Nelson, Simmons, und Simonsohn (2018) und Schimmack (2020). Eine positive Perspektive nehmen Korbmacher u. a. (2023) ein.",
    "crumbs": [
      "Die Geschichte der Open Science-Bewegung"
    ]
  },
  {
    "objectID": "geschichte.html#literatur",
    "href": "geschichte.html#literatur",
    "title": "Die Geschichte der Open Science-Bewegung",
    "section": "Literatur",
    "text": "Literatur\n\n\n\n\nFeest, Uljana. 2019. „Why replication is overrated“. Philosophy of Science 86 (5): 895–905.\n\n\nKorbmacher, Max, Flavio Azevedo, Charlotte R. Pennington, Helena Hartmann, Madeleine Pownall, Kathleen Schmidt, Mahmoud Elsherif, u. a. 2023. „The replication crisis has led to positive structural, procedural, and community changes“. Communications Psychology 1 (1). https://doi.org/10.1038/s44271-023-00003-2.\n\n\nNelson, Leif D., Joseph Simmons, und Uri Simonsohn. 2018. „Psychology’s Renaissance“. Annual review of psychology 69: 511–34. https://doi.org/10.1146/annurev-psych-122216-011836.\n\n\nSchimmack, Ulrich. 2020. „A meta-psychological perspective on the decade of replication failures in social psychology“. Canadian Psychology/Psychologie canadienne. https://doi.org/10.1037/cap0000246.",
    "crumbs": [
      "Die Geschichte der Open Science-Bewegung"
    ]
  },
  {
    "objectID": "probleme.html",
    "href": "probleme.html",
    "title": "Probleme",
    "section": "",
    "text": "Probleme, die im Rahmen der Revolution identifiziert wurden\nIm folgenden werden zahlreiche Probleme des Wissenschaftssystems erklärt und aufgelistet. Manche der Probleme sind seit vielen Jahrzehnten bekannt und andere erst seit wenigen Jahren. Es sei bei der Lektüre zu beachten, dass es zu allen diesen Problemen bereits ausgearbeitete und teilweise auch ausgeführte Lösungsansätze gibt. Wie die Lösungsansätze aussehen und welche Lösungen auf welche Probleme zugeschnitten sind, wird ausführlich im Kapitel zu Lösungen erläutert.",
    "crumbs": [
      "Probleme"
    ]
  },
  {
    "objectID": "lösungen.html",
    "href": "lösungen.html",
    "title": "Lösungen",
    "section": "",
    "text": "Lösungen und Ansätze zur Verbesserung der Lage der Psychologie\nWerfen wir ein Blick darauf, was sich seit 2012 in der Psychologie verändert hat. Eingeteilt sind die Veränderungen dahingehend, welchen Teil des Problems sie vor allem betreffen: Ist der Zweck einer neuen Vorgabe, das System, die Methodik, oder die Theorien der Forschung zu verbessern? Einige Lösungen sind mit vielen Problemen gleichzeitig verknüpft und manche sind auf bestimmte Angelegenheiten maßgeschneidert. Die Abbildung enthält eine grobe Unterteilung.",
    "crumbs": [
      "Lösungen"
    ]
  },
  {
    "objectID": "fazit.html",
    "href": "fazit.html",
    "title": "Fazit und Ausblick",
    "section": "",
    "text": "Wissenschaftler*innen arbeiten unter starkem Druck und das wissenschaftliche System belohnt Sorgfalt und Offenheit weniger als Innovation und klare Ergebnisse. In den Sozialwissenschaften tun sich immer mehr Forschende zusammen, diese Probleme zu lösen. Aus der Vertrauenskrise hervorgehend befindet sich große Teile der Psychologie bereits in einer Renaissance. Täglich werden Diskussionen auf persönlicher, institutioneller, und politischer Ebene darüber geführt, wie Wissenschaft betrieben werden sollte. Es ist dabei klar geworden, dass es in der Wissenschaft nicht wie in manch einem ökonomischen Markt eine “unsichtbare Hand” gibt, die dafür sorgt, dass alles ins Gleichgewicht findet - stattdessen sind es die Forschenden, die sich aktiv um die Selbstkorrektur von Wissenschaft bemühen. Bevor jedoch eine Intervention zur Verbesserung von Forschung stattfinden kann, wird zuerst eine Inventur (Wo sind Verbesserungen nötig?) und anschließend eine Beobachtung benötigt. Eine Inventur über Probleme und Lösungsvorschläge biete ich mit diesem Buch an. Es ist nun die Rolle der Meta-Wissenschaft, also Wissenschaft über Wissenschaft, zu prüfen, welche Lösungsansätze wie gut funktionieren.\n\nWas hat sich verbessert?\nIch hoffe, dass die Kapitel zu den Lösungen klar gemacht haben, dass es kein Problem gibt, für dass es nicht mindestens einen Lösungsvorschlag gibt und dass diese bereits vielfältig umgesetzt werden (Korbmacher u. a. 2023). Beispielsweise wurden für Replikationsforschung Leitfäden erarbeitet und Replikationen werden seltener als persönliche Attacken und mehr als sinnvolles Werkzeug angesehen (Nosek u. a. 2022). Strengere Vorgaben bei Berichten statistischer Tests haben sich deutlich auf deren Nachvollziehbarkeit ausgewirkt (Giofrè u. a. 2023), Präregistrierungen haben P-Hacking reduziert (Brodeur u. a. 2024), und vereinzelt konnten mit modernen Methoden extrem hohe Replikationsraten erreicht werden (Protzko u. a. 2023; siehe aber dazu die Kritik von Bak-Coleman und Devezer 2023). Wie Forschende in der Psychologie diese Ansätze nutzen, ist zwischen 2010 und 2020 von 49% auf 87% stark angestiegen (Ferguson u. a. 2023).\n\n\nWas ist noch zu tun?\nBei vielen Disziplinen ist die aktive Auseinandersetzung mit Open Science noch ausstehend. Während Sozial- und Persönlichkeitspsychologie unter den Sozialwissenschaftlern eine Vorreiterrolle spielen, hat die Auseinandersetzung in der Konsumentenpsychologie oder jenseits der Psychologie in Medizin, Erziehungswissenschaften, oder Soziologie erst vor wenigen Jahren begonnen. Und selbst in der Psychologie gibt es noch große Defizite. Zum Beispiel veröffentlichten nur 34 von 88 Zeitschriften Replikationsstudien mit einem Gesamtanteil von 0.2% an allen veröffentlichten Studien (Clarke u. a. 2023; Feldman 2024).\nAllgemein gilt: Um Diskussionen dabei mehr als nur intuitiv sinnvoll zu führen, ist eine meta-wissenschaftliche Evaluation der neuen Methoden unabdingbar (Phaf 2024b; Soderberg u. a. 2021). Es kehrt das Spektrum der möglichen Reaktionen auf Replikationsfehlschläge zurück: Sollte eine Theorie nach einem Replikationsfehlschlag sofort verworfen werden, oder ist das zu radikal und eine Beharrungstendenz von Forschenden könnte helfen, starke Theorien zu etablieren (Phaf 2024a)? Ebenfalls ist eine interdisziplinäre Zusammenarbeit sinnvoll: Beispielsweise würde es viel Arbeit ersparen, wenn Terminologien fächerübergreifend entwickelt werden, statt wie aktuell einzeln (und stark redundant) für Psychologie (Hüffmeier, Mazei, und Schultze 2016), Geisteswissenschaften (Schöch 2023), und Marketing (Urminsky und Dietvorst 2024). In manchen Bereichen ist sogar gänzlich unklar, wie Replikationserfolg aussieht, es lässt sich also kaum beurteilen, ob zwei Studien, die dasselbe untersuchen, zum selben Ergebnis kommen.\nOpen Science wird gelegentlich mit Open Access gleichgestellt. Zwar ist Open Science viel mehr als das, allerdings ist die Diskussion um Open Access eine der ältesten und stückweit unabhängig von Problemen wissenschaftlicher Integrität. Das soziale Dilemma, also wie sich Forschende von kommerziellen Verlagen unabhängig machen können, ist weit davon gelöst zu werden. Ob Abkommen zwischen Universitäten und Verlagen (z.B. DEAL Verträge) erfolgreich werden, ist noch ungewiss.\n\n\n\n\n\n\nOpen Washing\n\n\n\nSelbst Forschung, die sich mit dem Thema Open Science befasst, hält sich manchmal nicht an die Kriterien, die sie fordert. Beispielsweise steht ein Artikel von Open Science Verfechtern (Protzko u. a. 2023) in der Kritik, weil dort - ohne transparent darüber zu berichten - von der Präregistrierung abgewichen wurde.\n\n\n\n\nWas geschieht jetzt gerade?\nDie hier diskutierten Ideen und Befunde sind ein aktueller Schnappschuss eines dynamischen Diskurses. Mehr und mehr Universitäten gründen Zentren für Open Science und setzen sich mit den dazugehörigen Themen auseinander. Im Rahmen eines Schwerpunktprogramms zu Meta-Wissenschaft und Replizierbarkeit werden von der LMU München aus Forschungsprojekte gefördert. Das amerikanische Center for Open Science arbeitet an zahlreichen Projekten, und im Rahmen der europäischen FORRT-Initiative werden Projekte und Teams organisiert. Es ist nun nur noch eine Frage der Zeit, bis sich alle Forschenden der Probleme und Lösungsansätze bewusst sind und sich an der Verbesserung von Wissenschaft beteiligen.\n\n\n\n\n\n\nKrieg und Open Science\n\n\n\nEin aktuell noch relativ wenig behandeltes Thema ist das des Krieges. Viele Forschungsinstitutionen unterliegen einem Embargo, durch welches Kollaborationen mit Forschenden aus bestimmten Ländern wie China oder Russland gemeldet oder sogar unterlassen werden müssen. Je nach epistemologischem Standpunkt sind verschiedene Entwicklungen möglich:\n\nPositivismus: Davon ausgehend, dass es die eine Wahrheit gäbe (Wittgenstein 1922/2004), der nur alle auf die Schliche kommen müssen, sind gegnerische Nationen als Konkurrenten zu sehen, mit denen Wissen nicht geteilt werden sollte. Ihnen gegenüber müsste sich Wissenschaft also verschließen. Wie das mit Datenbanken im Internet zu bewerkstelligen ist, ist vorwiegend eine technische Frage.\nRelationistisch: Im Sinne von Fleck (1935/2015), ist Wahrheit gesellschaftlich verankert und verschiedene Gesellschaften können theoretisch mit verschiedenen Wahrheiten arbeiten. Die Forschenden von gegnerischen Nationen könnten sich dadurch möglicherweise gar nicht für die Sichtweisen der anderen interessieren oder diese als politisch durchtränkt abtun.\nNationalistisch: Aus dieser Perspektive ließe sich anbringen, dass eine Gesellschaft, die die Wissenschaft finanziert, primär davon profitieren sollte.\nAltruistisch/Idealistisch: In Anlehnung an die Mertonschen Normen (Merton 1968) lässt sich Wissenschaft als ein Gemeingut von allen Menschen verstehen, auf das niemand privilegierten Zugriff hat (Kommunismus des wissenschaftlichen Wissens) und wovon alle Menschen profitieren können. Bei dieser Haltung macht Wissenschaft an politischen Grenzen keinen Halt. Aus dieser Perspektive ließe sich Open Science als diplomatische Strategie zum wissenschaftlichen Austausch trotz politischer Differenzen verstehen.\n\nWelche Perspektive sich langfristig herauskristallisiert ist noch ungewiss. Im Zuge des Krieges in Israel und Gaza und einem offenen Brief von Wissenschaftler*innen dazu, erstellte das Bildungsministerium im Juni 2024 Listen von Hochschullehrenden, mit bestimmter politischer Einstellung sind und prüfte die Möglichkeit der Kürzung von Forschungsgeldern. Diese Fördergeld-Affäre wird aus der Wissenschaft heraus - ungeachtet der politischen Hintergründe - als eine Bedrohung der Freiheit der Wissenschaft wahrgenommen und ist eher mit nationalistischen und positivistischen Perspektiven vereinbar.\n\n\nIn der Kommunikation zwischen Wissenschaftler*innen und der Gesellschaft besteht dabei ein Spannungsverhältnis: Untereinander müssen Forschende kommunizieren, dass Reformen und Verbesserungen in der Wissenschaft nötig sind - was am einfachsten mit Hinweisen auf die Probleme funktioniert. Im Dialog mit der Gesellschaft kann ein Hinweis auf diese Probleme jedoch das Vertrauen in Wissenschaft gefährden (Penders 2024). Es bleibt hierbei also zu betonen, dass Wissenschaft nicht abgeschlossen (genau genommen nie abgeschlossen) ist, das aber nicht bedeutet, dass es weniger vertrauenswürdig als Gerüchte, Verschwörungstheorien, Einzelerfahrungen, oder Religion ist. Die selbst-kritische Haltung ist stattdessen ein zentraler Bestandteil von Wissenschaft und ein Zeichen dafür, dass die Selbstkorrektur-Mechanismen funktionieren.\n\nWeiterführende Informationen\n\nÜber aktuelle Neuigkeiten zu Fehler in der Forschung berichtet Retractionwatch: https://retractionwatch.com\nAuf ihrem Youtube-Kanal diskutiert Mai Thi Nguyen-Kim Probleme wie Publikationsbias und Selbstkorrekturmechanismen: https://www.youtube.com/watch?v=DHyRaUeHcGY\nSabine Hossenfelder behandelt in einem Youtube-Video das Problem, wie Klima-Wissenschaftler*innen ihre Befunde kommunizieren (sollten): https://www.youtube.com/watch?v=gMOjD_Lt8qY\nForschende für Vorträge und Workshops zum Thema Open Science vermittelt das deutsche Netzwerk der Open-Science Initiativen: https://osf.io/tbkzh/wiki/home/\nEine Einführung in das Thema Open Science für Studierende (englisch) bieten Pennington (2023), „Matters of significance“ (2024), und Chambers (2017).\nEin deutsches Glossar mit Open Science Begriffen ist online via FORRT verfügbar: https://forrt.org/glossary/german/\nOpen Science ist noch nicht die Norm, wie jeder dazu beitragen kann, fassen Kohrs u. a. (2023) zusammen.\n\n\n\nLiteratur\n\n\n\n\nBak-Coleman, Joseph, und Berna Devezer. 2023. „Causal claims about scientific rigor require rigorous causal evidence.“\n\n\nBrodeur, Abel, Nikolai M Cook, Jonathan S Hartley, und Anthony Heyes. 2024. „Do Preregistration and Preanalysis Plans Reduce p-Hacking and Publication Bias? Evidence from 15,992 Test Statistics and Suggestions for Improvement“. Journal of Political Economy Microeconomics 2 (3): 000–000.\n\n\nChambers, CD. 2017. „The seven deadly sins of psychology. The Seven Deadly Sins of Psychology“. Princeton University Press. https://doi. org/10.2307/j. ctvc779w5.\n\n\nClarke, Beth, Pui Yu Lee, Sarah R Schiavone, Mijke Rhemtulla, und Simine Vazire. 2023. „The Prevalence of Direct Replication Articles in Top-Ranking Psychology Journals“.\n\n\nFeldman, Gilad. 2024. „The value of replications goes beyond replicability and is tied to the value of the research it replicates: Commentary on Isager et al. (2024)“. OSF.\n\n\nFerguson, Joel, Rebecca Littman, Garret Christensen, Elizabeth Levy Paluck, Nicholas Swanson, Zenan Wang, Edward Miguel, David Birke, und John-Henry Pezzuto. 2023. „Survey of open science practices and attitudes in the social sciences“. Nat. Commun. 14 (1): 5401.\n\n\nFleck, Ludwik. 1935/2015. Entstehung und Entwicklung einer wissenschaftlichen Tatsache [Formation and development of a scientific fact]: Einführung in die Lehre vom Denkstil und Denkkollektiv [Introduction to thinking style and thinking collective]. 10. Auflage. Bd. 312. Suhrkamp-Taschenbuch Wissenschaft. Frankfurt am Main: Suhrkamp.\n\n\nGiofrè, David, Ingrid Boedker, Geoff Cumming, Carlotta Rivella, und Patrizio Tressoldi. 2023. „The influence of journal submission guidelines on authors’ reporting of statistics and use of open research practices: Five years later“. Behav. Res. Methods 55 (7): 3845–54.\n\n\nHüffmeier, Joachim, Jens Mazei, und Thomas Schultze. 2016. „Reconceptualizing replication as a sequence of different studies: A replication typology“. Journal of Experimental Social Psychology 66: 81–92. https://doi.org/10.1016/j.jesp.2015.09.009.\n\n\nKohrs, Friederike E, Susann Auer, Alexandra Bannach-Brown, Susann Fiedler, Tamarinde Laura Haven, Verena Heise, Constance Holman, u. a. 2023. „Eleven strategies for making reproducible research and open science training the norm at research institutions“. Elife 12 (November).\n\n\nKorbmacher, Max, Flavio Azevedo, Charlotte R. Pennington, Helena Hartmann, Madeleine Pownall, Kathleen Schmidt, Mahmoud Elsherif, u. a. 2023. „The replication crisis has led to positive structural, procedural, and community changes“. Communications Psychology 1 (1). https://doi.org/10.1038/s44271-023-00003-2.\n\n\n„Matters of significance“. 2024. UCL Press.\n\n\nMerton, Robert K. 1968. „The Matthew Effect in Science“. Science (New York, N.Y.) 159 (3810): 56–63. https://doi.org/10.1126/science.159.3810.56.\n\n\nNosek, Brian A, Tom E Hardwicke, Hannah Moshontz, Aurélien Allard, Katherine S Corker, Anna Dreber, Fiona Fidler, u. a. 2022. „Replicability, robustness, and reproducibility in psychological science“. Annu. Rev. Psychol. 73 (1): 719–48.\n\n\nPenders, Bart. 2024. „Scandal in scientific reform: the breaking and remaking of science“. Journal of Responsible Innovation 11 (1): 2371172.\n\n\nPennington, Charlotte. 2023. A student’s guide to open science: Using the replication crisis to reform psychology. McGraw-Hill Education (UK).\n\n\nPhaf, R Hans. 2024a. „Positive Deviance Underlies Successful Science: Normative Methodologies Risk Throwing out the Baby With the Bathwater“. Review of General Psychology, 10892680241235120.\n\n\n———. 2024b. „Positive deviance underlies successful science: Normative methodologies risk throwing out the baby with the bathwater“. Rev. Gen. Psychol., Februar.\n\n\nProtzko, John, Jon Krosnick, Leif Nelson, Brian A. Nosek, Jordan Axt, Matt Berent, Nicholas Buttrick, u. a. 2023. „High replicability of newly discovered social-behavioural findings is achievable“. Nature Human Behaviour. https://doi.org/10.1038/s41562-023-01749-9.\n\n\nSchöch, Christof. 2023. „Repetitive research: a conceptual space and terminology of replication, reproduction, revision, reanalysis, reinvestigation and reuse in digital humanities“. International Journal of Digital Humanities 5 (2-3): 373–403. https://doi.org/10.1007/s42803-023-00073-y.\n\n\nSoderberg, Courtney K., Timothy M. Errington, Sarah R. Schiavone, Julia Bottesini, Felix Singleton Thorn, Simine Vazire, Kevin M. Esterling, und Brian A. Nosek. 2021. „Initial evidence of research quality of registered reports compared with the standard publishing model“. Nature Human Behaviour. https://doi.org/10.1038/s41562-021-01142-4.\n\n\nUrminsky, Oleg, und Berkeley J Dietvorst. 2024. „Taking the Full Measure: Integrating Replication into Research Practice to Assess Generalizability“. Journal of Consumer Research 51 (1): 157–68.\n\n\nWittgenstein, Ludwig. 1922/2004. Logisch-philosophische Abhandlung: Tractatus logico-philosophicus. 1. Aufl. Bd. 12. Edition Suhrkamp. Frankfurt am Main: Suhrkamp.",
    "crumbs": [
      "Fazit und Ausblick"
    ]
  }
]