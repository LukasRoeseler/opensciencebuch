[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Open Science: Wie sich die Wissenschaft öffnet",
    "section": "",
    "text": "Willkommen\nViele wissenschaftliche Disziplinen haben sich in den letzten Jahren stark verändert. Neue Zeitschriften, Methoden, Initiativen, Gesellschaften, Strukturen, und vieles mehr sind entstanden. Es ist die Rede von einer Krise (Mede et al., 2020), einer Revolution (Nosek et al.) oder einer Renaissance (Nelson et al., 2018). All diese Veränderungen haben gemeinsam zu einem stärkeren Grad an Offenheit geführt und das Wissenschaftssystem maßgeblich verändert.\nIn diesem Buch werden niedrigschwellig der Begriff der Offenheit, die Ursprünge des Wandels, und die Open Science Bewegung erklärt. Es werden Hintergründe des wissenschaftlichen Systems, neu entwickelte Methoden, und Probleme amtierender Methoden erläutert. Dabei sind aufgrund der Verortung des Autors in der Psychologie Beispiele und Entwicklungen aus der Psychologie überrepräsentiert.\n\n\n\n\n\n\nWarnung\n\n\n\nDieses Projekt befindet sich in Arbeit und ist aktuell noch unfertig! Texte sind unvollständig oder fehlen, Quellenangaben und Abbildungen sind vorläufig, und es wimmelt von Rechtschreibfelehrn…",
    "crumbs": [
      "Willkommen"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "1  Über das Buch",
    "section": "",
    "text": "Wissenschaft wird zu einem Maßgeblichen Teil von Steuergeldern finanziert. Das bedeutet, eine Gesellschaft fördert einzelne Personen, um ein tieferes Verständnis von der Welt zu erlangen und im Gegenzug Unterstützung bei der Lösung verschiedener Probleme zu erhalten. Dadurch sind Wissenschaftler*innen der Gesellschaft rechenschaft nötig. Inwiefern das in verschiedenen Disziplinen der Fall ist und wie Personen sich dafür einsetzen, diese Rechenschaft zu ermöglichen, möchte ich in diesem Buch darlegen.\n\n\n\n\n\n\nÜber mich\n\n\n\nMein Name ist Lukas Röseler und ich bin Wissenschaftler. Ich habe in Wernigerode im Harz Psychologie und Wirtschaftswissenschaften studiert, mich darüber hinaus mit Philosophie und Wissenschaftstheorie beschäftigt, und habe 2021 in Bamberg meinen Doktortitel in Psychologie erhalten. Seit 2023 bin ich Geschäftsführer des Münster Center for Open Science, darf mich im Rahmen dessen mit transparenter und vertrauenswürdiger Wissenschaft in zahlreichen Disziplinen auseinandersetzen, und betreibe dabei Meta-Wissenschaft, also Wissenschaft über Wissenschaft.\nIch habe dieses Buch geschrieben, um das Thema Open Science anderen Wissenschaftler*innen, Studierenden, und allen anderen Personen auf einem einfachen aber ausführlichen Niveau zu erklären. Ein großer Aspekt von offener Wissenschaft ist nämlich, dass auch Personen von außerhalb verstehen können, was dort vor sich geht. Angesichts dieses Zieles ist das Buch kostenlos und online verfügbar.\n\n\n\nBlick über Wernigerode",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Über das Buch</span>"
    ]
  },
  {
    "objectID": "wasistopenscience.html",
    "href": "wasistopenscience.html",
    "title": "2  Was ist Open Science?",
    "section": "",
    "text": "In der Wissenschaft dreht es sich oft um Details: Was genau passierte in der Studie? Welche Bilder sahen Versuchspersonen auf was für einem Bildschirm? Was war die Bildwiederholungsrate des Bildschirmes, und waren die Farben dabei mittels Spektralphotometer kalibriert? Wissenschaftliche Untersuchungen und ihre Befunde werden heutzutage in Zeitschriftenartikeln beschrieben. Es kommt nicht selten vor, dass darin eines der oben aufgeführten Details fehlt, in den Zusatzmaterialien nicht auffindbar ist, oder die Autor*innen nicht gefragt werden können, weil ihre E-Mail-Adresse nicht mehr aktuell ist. Wenn nun aber der Befund von genauso einem Detail abhängt, werden zukünftige Forschende Probleme haben, ihn zum Vorschein zu bringen. Auf dieser konkreten Ebene bedeutet Open Science die Möglichkeit für jede Person, im Rahmen von ethischen und legalen Einschränkungen (z.B. Anonymität), detaillierte Einsicht in den gesamten wissenschaftlichen Prozess der Erkenntnisgewinnung zu erhalten. Es sollte also genau beschrieben werden, wie die Untersuchung ablief, welche Materialien dafür verwendet werden, welche Daten daraus resultierten, wie diese weiterverarbeitet und ausgewertet wurden, und schließlich, was daraus zu lernen ist.\nAuf einer abstrakten Ebene meint Open Science einen höheren Grad an Offenheit und Transparenz in allen Facetten der Wissenschaft. Dazu gehören wie beschrieben Studienmaterialien (z.B. Fragebögen, Bilder, oder Videos), der wissenschaftliche Bericht, oder der Diskurs im Rahmen der Begutachtung wissenschaftlicher Berichte durch ihre Kolleg*innen (Peer-Review). Aber auch auf der systemischen Ebene kann der Grad an Transparenz steigen: Beispielsweise gibt es Listen mit Zeitschriften (https://doaj.org) oder Listen mit Professuren in einem bestimmten Wissenschaftsbereich (z.B. für die Persönlichkeitspsychologie in Deutschland: https://docs.google.com/spreadsheets/d/1XwO4n88zkdH1bDGpW6Uz8f4sH24P5FG0ihUp7i0Y-gY/edit#gid=1684024252). Schließlich kann mit Offenheit auch die Durchführung einer Konferenz im „hybriden Format”, also an einem bestimmten Ort aber mit der Möglichkeit zur Online-Teilnahme angeboten werden, um Personen, die nicht Anreisen (können) nicht auszuschließen, ihnen gegenüber also offen zu sein.\nInsgesamt wird Open Science als Lösung [LR1] für zahlreiche[1] aktuelle, meist verworrene Probleme verstanden, allen vorweg das Problem, dass sich ungefähr 50% aller wissenschaftlichen Studien in der Psychologie nicht replizieren lassen (Open Science Collaboration, 2015). Eine Übersicht über Facetten von Open Science ist in Abbildung X. Genaue Schätzungen sind schwierig, grob lässt sich dennoch sagen: Sucht man sich eine zufällige sozialwissenschaftliche Studie aus einer Fachzeitschrift aus, führt sie nach wissenschaftlichem Goldstandard erneut aus, und prüft die dort getestete Hypothese ein weiteres Mal, dann ist die Wahrscheinlichkeit, zum selben Ergebnis zu kommen, so hoch, wie bei einem Münzwurf “Zahl” (vs. Kopf) zu erhalten. Damit teilweise verbundene Probleme sind Betrug bei wissenschaftlichen Publikationen (Gopalakrishna et al., 2021), oder psychische Probleme bei Jungwissenschaftler*innen (Satinsky et al., 2021).\n\n\n\nFacetten von Open Science",
    "crumbs": [
      "Einleitung",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Was ist Open Science?</span>"
    ]
  },
  {
    "objectID": "umgehenmitopenscience.html",
    "href": "umgehenmitopenscience.html",
    "title": "3  Wie ist mit Open Science umzugehen?",
    "section": "",
    "text": "Wie ist mit Open Science umzugehen?\nDas Thema Open Science und Replikationskrise ist aus mehreren Gründen schwer kommunizierbar: Zuerst einmal ist es harte Kritik an der Wissenschaft und dem Wissenschaftssystem, die dazu missbraucht werden kann, wissenschaftliche Befunde kleinzureden und das Vertrauen in Wissenschaft insgesamt gefährdet. Mithilfe der hier präsentierten Argumente und Befunde lassen sich zum Beispiel auf Wissenschaft beruhende politische oder individuelle Entscheidungen kritisieren. Dagegen sei gesagt: Weitaus nicht alle wissenschaftlichen Befunde sind “falsch” oder nicht replizierbar. Bei vielen Punkten herrscht immer noch ein großer Konsens.\nAuf der anderen Seite gehen die hier diskutierten Probleme über das hinaus, was ganz natürlich in fast jedem wissenschaftlichen Zweig wiederfindet. Denn je genauer man hinschaut, desto wahrscheinlicher ist es, dass ein wissenschaftlicher Befund relativierbar ist. Widersprüche oder Konflikte lassen sich überall finden, treten natürlicherweise auf, und werden von Wissenschaftler*innen ins Visier genommen. Jede*r Forschende kann berichten: Bei genauerem Hinschauen bleibt nichts schwarz oder weiß, sondern zerfließt in viele Grautöne. Bei den im Folgenden behandelten Replikationsfehlschlägen handelt es sich um mehr als die wissenschaftsinhärente Ungewissheit: Sie gefährden gesamte Wissenschaftsstränge.\nBevor wir in die Details fortschreiben, ist es außerdem wichtig festzuhalten, dass aktuell (Herbst, 2023) die meisten der in diesem Buch besprochenen Probleme noch nicht oder erst teilweise gelöst wurden und auf wöchentlicher Basis heiß diskutiert werden. Nach einem Jahrzehnt der Open Science Bewegungen ist ein Punkt erreicht, an dem die meisten Wissenschaftler*innen ein Bewusstsein über das Problem haben. Einige sind mit Lösungsansätzen vertraut, doch haben sich diese weder komplett durchgesetzt, noch ist klar, welche Probleme überhaupt bereits gelöst worden sind.\nIch verstehe Open Science als eine große Trigger-Warnung, die vor die Sozialwissenschaften vorgeschaltet sein sollte und lautet: Achtung, wir haben hier gerade sehr große Probleme. Es ist gut möglich, dass die Hälfte von allem, was wir zu wissen glauben, schlichtweg falsch ist. Meinungen über die Open Science Bewegung sind aber keineswegs homogen: Es lässt sich hier ein Spektrum spannen zwischen “Sollen wir wirklich die nächsten 20 Jahre damit verbringen, herauszufinden, welche Erkenntnisse der letzten 100 Jahre korrekt sind? Fangen wir doch einfach nochmal bei Null an.” und “Eigentlich ist das ganz normal, ich sehe keinen Grund, hier von einer ‘Krise’ zu sprechen”.\n\n\n\nSpektrum der Reaktionen auf die Replikationskrise\n\n\nBesonders groß ist das Problem bei Lehrbüchern: Stellen Sie sich vor, sie haben ein Buch über einen Teil der Psychologie (Aushängeschild ist hier oft die Sozialpsychologie) verfasst, das auf Jahrzehnten an Forschung besteht, hunderten Veröffentlichungen, und tausenden Studien. Hier hilft es kaum, das Buch komplett zu ignorieren. Praktikabel, alle Studien selbst zu replizieren, ist es allerdings auch nicht. Klar ist den meisten Wissenschaftler*innen hierbei: So wie es bisher gelaufen ist, kann es nicht weitergehen. Eine fünfzig prozentige Garantie für wissenschaftliche Erkenntnisse sollte nicht der Anspruch von etwas sein, das sich Wissenschaft nennt. Von vorne müssen wir aber auch nicht starten.",
    "crumbs": [
      "Einleitung",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Wie ist mit Open Science umzugehen?</span>"
    ]
  },
  {
    "objectID": "anfänge.html",
    "href": "anfänge.html",
    "title": "4  Anfänge einer Revolution",
    "section": "",
    "text": "4.0.1 Stapel-Affäre\nDurch einen Zufall entdeckten Nachwuchswissenschaftler im Jahr 2011, dass die Daten einer Studie ihres Kollegen, Diederik Stapel, von niemandem jemals erhoben wurden. Sie waren ausgedacht, bzw. fabriziert. Stand Juli 2024 wurden 58 von Stapels Fachartikel identifiziert und zurückgezogen, deren Daten fabriziert oder geschönt wurden.1 Wissenschaftliche Institutionen wie der Begutachtungsprozess von Artikeln durch Fachkolleg*inen, deren Zweck die Qualitätssicherung war, hatten versagt. Seit dem Vorfall sind einige weitere Fälle bekannt geworden, teilweise durch erneute Analyse von Daten der jeweiligen Studien (O’Grady 2021) und oft durch Whistleblower, also durch Personen, die zu ihrem Schutz anonym bleiben wollen. Umfragen in den Niederlanden unter Forschenden haben ergeben, dass Fälschung oder Schönigung von Daten von bis zu 10% aller Personen durchgeführt wird (Gopalakrishna et al. 2021). Dabei ist zu beachten, dass Studien durch gefälschte Daten besonders innovativ, überraschend, oder klar werden - Eigenschaften, die die Veröffentlichung in einer Fachzeitschrift wahrscheinlicher machen.",
    "crumbs": [
      "Die Geschichte der Open Science Bewegung",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Anfänge einer Revolution</span>"
    ]
  },
  {
    "objectID": "anfänge.html#footnotes",
    "href": "anfänge.html#footnotes",
    "title": "4  Anfänge einer Revolution",
    "section": "",
    "text": "Mittels der Retraction Database lassen sich nach Thema, Autor*in, Zeitschrift, usw. zurückgezogene Artikel durchsuchen: http://retractiondatabase.org/↩︎",
    "crumbs": [
      "Die Geschichte der Open Science Bewegung",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Anfänge einer Revolution</span>"
    ]
  },
  {
    "objectID": "bestandsaufnahme.html",
    "href": "bestandsaufnahme.html",
    "title": "5  Bestandsaufnahme",
    "section": "",
    "text": "Eine ähnliche Vertrauenskrise gab es in der Sozialpsychologie in den 1960er Jahren (Lakens 2023). Ein entscheidender Unterschied war diesmal die Bestandsaufnahme: Parallel zu diesen eigenartigen Befunden oder Anomalien vernetzten sich Psycholog*innen um Brian Nosek international und untersuchten die Replizierbarkeit von 100 Studien aus namhaften psychologischen Fachzeitschriften (Open Science Collaboration 2015). Sie fanden heraus, dass sich nur 39 der 100 Originalbefunde replizieren ließen. Bei allen anderen Studien, waren die Replikationsergebnisse anders als die ursprünglichen Ergebnisse. Viele weitere Großprojekte folgten, alle mit ähnlichen Ergebnissen: Die Replikationsraten lagen weit unter den Gewünschten.\nZahlreiche Verbünde folgten. Einige Projekte konzentrierten sich auf einzelne Phänomene. Beispielsweise haben sich 17 Forschungsgruppen zusammengetan, um den Befund des Facial Feedback (Strack, Martin, and Stepper 1988) zu replizieren (Wagenmakers et al. 2016). Dabei geht darum, dass Personen einen Stift mit den Zähnen festhalten und dabei je nach Ausrichtung des Stiftes entweder diejenigen Muskeln anspannen, die sie auch zum Lachen benötigen oder eben nicht. In der “Lachen”-Bedingung fanden die Versuchspersonen im Anschluss Comics witziger. Die Replikation schlug fehl. 2022 wurde eine weitere Studie mit über 3000 Versuchspersonen aus 19 Ländern veröffentlicht - diesmal auch mit direkter Beteiligung von Fritz Strack, der die Originalstudie durchgeführt hatte (Coles et al. 2022). Wieder zeigte sich, dass die Position eines Stiftes im Mund sich nicht auf die Bewertung von Stimuli auswirkt. Jenseits von sozialpsychologischen Befunden konzentrierten sich Forschende auch auf Bereiche wie Forschung mit Babys (Byers-Heinlein et al. 2020) oder auf ganze Zeitschriften (Camerer et al. 2018).\nListe großer Replikationsprojekte (siehe auch https://forrt.org/replication-hub/)\n\n\n\nProjekt\nThema\nLink\n\n\nReproducibility Project: Psychology\nPsychologie\nhttps://osf.io/ezcuj/\n\n\nCORE\nEntscheidungsforschung\nhttps://osf.io/5z4a8/\n\n\nData Replicada\nKonsumentenverhalten und Entscheidungsforschung\nhttps://datacolada.org/archives/category/replication\n\n\nMany Labs 1\nPsychologie\nhttps://osf.io/wx7ck/\n\n\nMany Labs 2\nPsychologie\nhttps://osf.io/8cd4r/\n\n\nMany Labs 3\nPsychologie\nhttps://osf.io/ct89g/\n\n\nMany Labs 4\nPsychologie\nhttps://osf.io/8ccnw/\n\n\nMany Labs 5\nPsychologie\nhttps://osf.io/7a6rd/\n\n\nSoto\nPersönlichkeitspsychologie\nhttps://doi.org/10.1177/0956797619831612\n\n\nSocial Sciences Replication Project\nVerhaltensforschung\nhttp://www.socialsciencesreplicationproject.com\n\n\nRegistered Replication Reports\nVerschiedene\n–\n\n\nMany Babies 1\nEntwicklungspsychologie\nhttps://manybabies.org\n\n\nSports Sciences Replications\nSportwissenschaften\nhttps://ssreplicationcentre.com\n\n\nHagen Cumulative Science Project\nPsychologie\nhttps://osf.io/d7za8/\n\n\nI4R Replications\nPolitikwissenschaften\nhttps://i4replication.org/reports.html\n\n\nExperimental Philosophy\nExperimentelle Philosophie\nhttps://doi.org/10.1007/s13164-018-0400-9\n\n\nReproducibility Project: Cancer\nKrebsforschung (Medizin)\nhttps://www.cos.io/rpcb\n\n\nSCORE\nSozialwissenschaften\nhttps://www.cos.io/score\n\n\nREPEAT\nGesundheitssystem\nhttps://www.repeatinitiative.org\n\n\nCREP\nPsychologie\nhttps://www.crep-psych.org\n\n\nBoyce et al., 2023\nPsychologie\nhttps://doi.org/10.1098/rsos.231240\n\n\nReproSci\nBiologie\nhttps://reprosci.epfl.ch\n\n\nBoyce et al. 2024\nPsychologie\nhttps://doi.org/10.31234/osf.io/an3yb\n\n\n\n\n5.0.1 Definition von Replizierbarkeit\nZu sagen, was repliziert werden konnte und was nicht, ist erst nach einer Definition möglich. Im Sprachgebrauch von Forschenden wird mit „wurde repliziert” gemeint, dass ein Replikationsversuch zu gleichen Ergebnissen wie eine Originalstudie gekommen ist. Zu Replikationsfehlschläge wird „konnte nicht repliziert werden” gesagt, subtil davon abweichend kann „wurde nicht repliziert” meinen, dass keine Replikationsversuche existieren oder sie fehlschlugen. Für eine Wissenschaft, die über 100 Jahre alt ist, scheint es überraschend, dass noch immer keine klare Definition wichtiger Konzepte rund um das Thema Replikation vorliegt, geschweige denn es zur Routine gehört, Studien zu replizieren. Während sich in verschiedenen Feldern abweichende Taxonomien durchgesetzt haben, sieht die Verwendung in diesem Buch wie in der Tabelle beschrieben aus.\n\nReplikations-Taxonomie nach Turing Way (The Turing Way Community and Scriberia 2024)\n\n\n\n\nDaten\n\n\n\n\n\n\n\ngleich\nunterschiedlich\n\n\nAnalyse\ngleich\nreproduzierbar\nreplizierbar\n\n\n\nunterschiedlich\nrobust\nverallgemeinerbar\n\n\n\n\n\n5.0.2 Weiterführende Literatur\nFür eine systematischere, in den Informationswissenschaften verankerte Taxonomie zur Art der Replikation siehe (Plesser 2018). Eine an den statistischen Methoden angelehnte Taxonomie für die Ergebnisse von Replikationsstudien haben LeBel et al. (LeBel et al. 2019) vorgeschlagen. Philosophisch diskutiert wird Replikationsnähe zum Beispiel von (Choi 2023) und (Leonelli 2023). \n\nReplikationstaxonomie\n\n\n\n\n\n\nUnterscheidungskriterium\nAusprägungen\n\n\n\n\nErgebnisse einer Replikationsstudie\n-Erfolgreich\n-Fehlgeschlagen\n-Unklar oder gemischt\n\n\nNähe einer Replikationsstudie zur Originalstudie (in Anlehnung an Lebel REF und Hüffmeier et al REF)\nDirekte Replikation\n(selbe Versuchsleiter*innen,\nselbe Versuchsmaterialien,\nneue Versuchspersonen)\nNahe Replikation\n(andere Versuchsleiter*innen,\nmöglichst ähnliche Versuchsmaterialien,\nneue Versuchspersonen)\nKonzeptuelle oder konstruktive Replikation\n(andere Versuchsleiter*innen,\nandere Versuchsmaterialien,\nneue Versuchspersonen)\n\n\nZiel der Replikation\nReproduktion\nMit selben Daten und selbem Programmiercode zu denselben Ergebnissen gelangen\nReplikation\nMit anderen Daten zu denselben Ergebnissen gelangen\n\n\n\n\n\n5.0.3  „Eine Schwalbe macht noch keinen Sommer”\nOb an einem wissenschaftlichen Befund „etwas dran ist”, er also einen Wahrheitsanspruch hat, hängt – neben seiner eigentlichen Art der Etablierung – bei der Replikationsforschung von vielen Faktoren ab. Was waren die Ergebnisse der Replikationsstudie? Wie viele und wie unterschiedliche Studien wurden durchgeführt? Wie sahen die genauen Methoden aus? Was waren die Unterschiede zwischen Replikationen und Originalstudie? Während Einzelstudien immer einen Erkenntnisgewinn liefern (mindestens, ob eine bestimmte Methode praktikabel ist, (Sikorski and Andreoletti 2023), können sie je nach Forschungsgebiet stark variieren Landy et al. (2020). Für das Gesamtbild braucht es mehr, wie zum Beispiel eine statistische Aggregation aller Einzelbefunde im Rahmen einer Meta-Analyse. Ein Beispiel mit Fantasiedaten befindet sich dazu in Abbildung X.\nABBILDUNG X: Forest plot mit simulierten Ergebnissen von Original + vielen Einzelstudien; in den Notes dann ausführliche Erklärung.\ntitle: \"Wald Diagramm (Forest Plot)\"\nformat:\n  html:\n    code-fold: true\n---\n  \n{r}\nlibrary(ggplot2)\nlibrary(metafor)\n\nset.seed(10)\nk &lt;- 15\ncors &lt;- data.frame(\"Stichprobenumfang\" = round(rchisq(n = k, df = 2, ncp = 0)*5+10, digits = 0)\n                   , \"Korrelation\" = rnorm(n = k, mean = .05, sd = .3)\n                   , \"Studie\" = paste(\"Studie\", toupper(letters[1:k]), sep = \" \")\n                   , \"yi\" = NA\n                   , \"vi\" = NA\n                   )\n\ncors[, 4:5] &lt;- metafor::escalc(ni = cors$Stichprobenumfang, ri = cors$Korrelation, measure = \"COR\")\ncors$ucb &lt;- cors$yi + qnorm(.975)*cors$vi\ncors$lcb &lt;- cors$yi - qnorm(.975)*cors$vi\n\n\n\nggplot(cors, aes(x = Korrelation, y = reorder(Studie, Korrelation))) + \n  geom_point() + geom_errorbar(xmin = cors$lcb, xmax = cors$ucb) + \n  geom_vline(xintercept = 0, lty = 2) + theme_classic() + ylab(\"\") + \n  xlim(c(-.4, .6))\n\n\n5.0.4 Phänomen-zentrierte Replikationsprojekte\nIm Gegensatz zu dem breit gefächerten RPP und anderen Versuchen, die Replikationsrate zu schätzen, haben sich andere Versuche auf grundlegende Phänomene fokussiert. Dutzende Gruppen auf der ganzen Welt haben sich in solchen Fällen zusammengeschlossen, auf einen Versuchsaufbau geeinigt, und führen die Studien mit einer enormen Anzahl an Versuchspersonen durch. Die meisten dieser Vorhaben stammen aus der Psychologie. Während die dabei gefundenen Effektstärken, also sozusagen die Deutlichkeit eines Zusammenhanges oder Befundes, in fast allen Fällen weit unter denen bisheriger Studien lagen (Kvarven, Strømland, and Johannesson 2020), waren sie zudem beim Großteil der Studien null, die Phänomene waren also „nicht sichtbar” Rife et al. (2024). So konnte beispielsweise mit einer enormen Präzision gezeigt werden, dass eine Geschichte über einen Professor Versuchspersonen in einem anschließenden Leistungstest *nicht* schlauer macht (O’Donnell et al. 2018).\n\n\n\n\n\n\nEffiziente Nutzung von Ressourcen?\n\n\n\nWie geht man mit Ressourcen bei Replikationen um? Bei Zusammenschlüssen vieler Forschender stellt sich diese Frage unweigerlich. Erstellen alle Gruppen unabhängig voneinander die Studie? Halten sich alle an ein zuvor abgestimmtes Protokoll? Führen sie die Studie nacheinander durch um voneinander zu lernen? Bei Registered Replication Reports wird für gewöhnlich von einem zuvor mit anderen Forschenden (z.B. den Autor*innen der Originalstudie) ein Versuchsaufbau abgestimmt. Teams in verschiedenen Ländern übersetzen das Protokoll dann und halten sich bei der Durchführung eng daran. Diese Protokolle sind manchmal nicht im Vorhinein getestet (https://doi.org/10.31234/osf.io/c6r8x?), basieren oft aber auf erfolgreichen, namhaften Studien. Das hat den Vorteil, dass Unterschiede zwischen den Gruppen nicht auf Unterschiede in der Durchführung zurückzuführen sind und sich Kulturen vergleichen lassen (Kakinohana, Pilati, and Klein 2022). Ein Nachteil dabei ist jedoch, dass, wenn an einem, zwei, oder fünf Standorten das Experiment schon nicht funktioniert, es fraglich ist, ob die übrigen 30 Gruppen es auch probieren sollten. In den Worten von Buttliere (https://doi.org/10.31234/osf.io/c6r8x?): “Wer bekommt bessere Ergebnisse? 39 Personen, die etwas zum ersten Mal tun, oder eine Person, die etwas 39 Mal tut?”\n\n\n\n\n5.0.5 Disziplin-zentrierte Replikationsprojekte\nUngefähr die Hälfte aller psychologischen Befunde ist also nicht replizierbar. Heißt das, alle Sozialwissenschaftlichen Lehrbücher aus allen Disziplinen sind zur Hälfte falsch? Die klare Antwort heißt nein. Die akkurate Antwort lautet kommt darauf an.\n\n5.0.5.1 Jenseits der Psychologie\nInwiefern es auf die Disziplin innerhalb der Sozialwissenschaften ankommt wurde bisher vor allem in der Psychologie untersucht. Aktuelle Tendenzen weisen darauf hin, dass Replikationsraten in der Persönlichkeitspsychologie und kognitiven Psychologie (Soto 2019) höher liegen als die in der Sozialpsychologie (Open Science Collaboration 2015) oder im Marketing (Charlton 2022). Während schon hunderte Replikationsversuche für sozialpsychologische Studien veröffentlicht sind, sind es in anderen Bereichen wie dem Marketing aktuell weniger - Stand Oktober 2022 sogar nur 9. Bereiche außerhalb der Psychologie sind von Replikationsproblemen ebenfalls betroffen. Von Problemen der Replizierbarkeit, Reproduzierbarkeit, und Nachvollziehbarkeit sind fast alle Disziplinen betroffen. Neue Lösungsansätze werden in Medizin, Biologie, Chemie, Physik, Geschichtswissenschaften, Politikwissenschaften, Erziehungswissenschaften, Informatik, und vielen weiteren Bereichen diskutiert.\n\n\n\nLiteratur\n\n\n\n\nAlogna, V. K., M. K. Attaya, P. Aucoin, Štěpán Bahník, S. Birch, A. R. Birt, B. H. Bornstein, et al. 2014. “Registered Replication Report: Schooler and Engstler-Schooler (1990).” Perspectives on Psychological Science : A Journal of the Association for Psychological Science 9 (5): 556–78. https://doi.org/10.1177/1745691614545653.\n\n\nBouwmeester, S., P. P. J. L. Verkoeijen, B. Aczel, F. Barbosa, L. Bègue, P. Brañas-Garza, T. G. H. Chmura, et al. 2017. “Registered Replication Report: Rand, Greene, and Nowak (2012).” Perspectives on Psychological Science : A Journal of the Association for Psychological Science 12 (3): 527–42. https://doi.org/10.1177/1745691617693624.\n\n\nByers-Heinlein, Krista, Christina Bergmann, Catherine Davies, Michael C Frank, J Kiley Hamlin, Melissa Kline, Jonathan F Kominsky, et al. 2020. “Building a Collaborative Psychological Science: Lessons Learned from ManyBabies 1.” Canadian Psychology/Psychologie Canadienne 61 (4): 349.\n\n\nCamerer, Colin F., Anna Dreber, Felix Holzmeister, Teck-Hua Ho, Jürgen Huber, Magnus Johannesson, Michael Kirchler, et al. 2018. “Evaluating the Replicability of Social Science Experiments in Nature and Science Between 2010 and 2015.” Nature Human Behaviour 2 (9): 637–44. https://doi.org/10.1038/s41562-018-0399-z.\n\n\nCharlton, Aaron. 2022. “Replications of Marketing Studies.” OpenMKT.org. https://openmkt.org/research/replications-of-marketing-studies/.\n\n\nCheung, Irene, Lorne Campbell, Etienne P. LeBel, Robert A. Ackerman, Bülent Aykutog˘lu, Štěpán Bahník, Jeffrey D. Bowen, et al. 2016. “Registered Replication Report: Study 1 from Finkel, Rusbult, Kumashiro, & Hannon (2002).” Perspectives on Psychological Science : A Journal of the Association for Psychological Science 11 (5): 750–64. https://doi.org/10.1177/1745691616664694.\n\n\nChoi, Hong Hui. 2023. “In Defense of the Resampling Account of Replication.” Journal of Theoretical and Philosophical Psychology 43 (4): 249–51. https://doi.org/10.1037/teo0000224.\n\n\nColes, Nicholas A., David S. March, Fernando Marmolejo-Ramos, Jeff T. Larsen, Nwadiogo C. Arinze, Izuchukwu L. G. Ndukaihe, Megan L. Willis, et al. 2022. “A Multi-Lab Test of the Facial Feedback Hypothesis by the Many Smiles Collaboration.” Nature Human Behaviour. https://doi.org/10.1038/s41562-022-01458-9.\n\n\nEerland, Anita, Andrew M. Sherrill, Joseph P. Magliano, Rolf A. Zwaan, Jack D. Arnal, Philip Aucoin, Stephanie A. Berger, et al. 2016. “Registered Replication Report: Hart & Albarracín (2011).” Perspectives on Psychological Science : A Journal of the Association for Psychological Science 11 (1): 158–71. https://doi.org/10.1177/1745691615605826.\n\n\nKakinohana, Regis K., Ronaldo Pilati, and Richard A. Klein. 2022. “Does Anchoring Vary Across Cultures? Expanding the Many Labs Analysis.” European Journal of Social Psychology. https://doi.org/10.1002/ejsp.2924.\n\n\nKvarven, Amanda, Eirik Strømland, and Magnus Johannesson. 2020. “Comparing Meta-Analyses and Preregistered Multiple-Laboratory Replication Projects.” Nature Human Behaviour 4 (4): 423–34. https://doi.org/10.1038/s41562-019-0787-z.\n\n\nLakens, Daniel. 2023. “Concerns about Replicability, Theorizing, Applicability, Generalizability, and Methodology Across Two Crises in Social Psychology.” https://doi.org/10.31234/osf.io/dtvs7.\n\n\nLandy, Justin F., Miaolei Liam Jia, Isabel L. Ding, Domenico Viganola, Warren Tierney, Anna Dreber, Magnus Johannesson, et al. 2020. “Crowdsourcing Hypothesis Tests: Making Transparent How Design Choices Shape Research Results.” Psychological Bulletin 146 (5): 451–79. https://doi.org/10.1037/bul0000220.\n\n\nLeBel, Etienne P., Wolf Vanpaemel, Irene Cheung, and Lorne Campbell. 2019. “A Brief Guide to Evaluate Replications.” Meta-Psychology 3. https://doi.org/10.15626/MP.2018.843.\n\n\nLeonelli, Sabina. 2023. Philosophy of Open Science. Cambridge University Press. https://doi.org/10.1017/9781009416368.\n\n\nMcShane, Blakeley B, Ulf Böckenholt, and Karsten T Hansen. 2022. “Variation and Covariation in Large-Scale Replication Projects: An Evaluation of Replicability.” J. Am. Stat. Assoc. 117 (540): 1605–21.\n\n\nO’Donnell, Michael, Leif D. Nelson, Evi Ackermann, Balazs Aczel, Athfah Akhtar, Silvio Aldrovandi, Nasseem Alshaif, et al. 2018. “Registered Replication Report: Dijksterhuis and van Knippenberg (1998).” Perspectives on Psychological Science : A Journal of the Association for Psychological Science 13 (2): 268–94. https://doi.org/10.1177/1745691618755704.\n\n\nOpen Science Collaboration. 2015. “Psychology: Estimating the Reproducibility of Psychological Science.” Science (New York, N.Y.) 349 (6251): aac4716. https://doi.org/10.1126/science.aac4716.\n\n\nPlesser, Hans E. 2018. “Reproducibility Vs. Replicability: A Brief History of a Confused Terminology.” Frontiers in Neuroinformatics 11 (January). https://doi.org/10.3389/fninf.2017.00076.\n\n\nRife, Sean, Quinn Lambert, Robert Calin-Jageman, Adamkovic Matus, Gabriel Banik, Itxaso Barberia, Jennifer Beaudry, et al. 2024. “Registered Replication Report: Study 3 from Trafimow and Hughes (2012).” Advances in Methods and Practices in Psychological Science.\n\n\nSikorski, Michał, and Mattia Andreoletti. 2023. “Epistemic Functions of Replicability in Experimental Sciences: Defending the Orthodox View.” Found. Sci., February.\n\n\nSoto, Christopher J. 2019. “How Replicable Are Links Between Personality Traits and Consequential Life Outcomes? The Life Outcomes of Personality Replication Project.” Psychological Science 30 (5): 711–27. https://doi.org/10.1177/0956797619831612.\n\n\nStrack, Fritz, Leonard L. Martin, and Sabine Stepper. 1988. “Inhibiting and Facilitating Conditions of the Human Smile: A Nonobtrusive Test of the Facial Feedback Hypothesis.” Journal of Personality and Social Psychology 54 (5): 768–77. https://doi.org/10.1037/0022-3514.54.5.768.\n\n\nThe Turing Way Community, and Scriberia. 2024. “Illustrations from the Turing Way: Shared Under CC-BY 4.0 for Reuse.” Zenodo. https://doi.org/10.5281/ZENODO.3332807.\n\n\nVaidis, David C, Willem WA Sleegers, Florian Van Leeuwen, Kenneth G DeMarree, Bjørn Sætrevik, Robert M Ross, Kathleen Schmidt, et al. 2024. “A Multilab Replication of the Induced-Compliance Paradigm of Cognitive Dissonance.” Advances in Methods and Practices in Psychological Science 7 (1): 25152459231213375.\n\n\nWagenmakers, Eric-Jan, Titia Beek, Laura Dijkhoff, Quentin F. Gronau, Alberto Acosta, Reginald B. Adams, Daniel N. Albohn, et al. 2016. “Registered Replication Report: Strack, Martin, & Stepper (1988).” Perspectives on Psychological Science : A Journal of the Association for Psychological Science 11 (6): 917–28. https://doi.org/10.1177/1745691616674458.",
    "crumbs": [
      "Die Geschichte der Open Science Bewegung",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Bestandsaufnahme</span>"
    ]
  },
  {
    "objectID": "wandel.html",
    "href": "wandel.html",
    "title": "6  Wandel im System",
    "section": "",
    "text": "6.0.1 Hat sich die Replizierbarkeitsrate erhöht?\nDie Replikationskrise dauert nun schon über ein Jahrzehnt an und einiges hat sich geändert. Ist dadurch auch das Problem der Replizierbarkeit gelöst? Zum einen ist für viele Bereiche noch gar nicht klar, was sich replizieren lässt. Im Marketing führte die Zeitschrift Journal of Business Research kurzzeitig eine Replikationsecke ein [@https://doi.org/10.1016/j.jbusres.2012.05.001], welche dann jedoch in eine andere Zeitschrift verlagert wurde. Aktuell sind Projekte in Arbeit, die die Replizierbarkeit für verschiedene Disziplinen schätzen. Deren Ergebnisse sind größtenteils noch vorläufig und unklar. In einem eigenen Projekt, sammeln wir Replikationsergebnisse, um langfristig je Disziplin und über die Zeit zu schauen, wie sich Replikationsraten verändert haben. Tagesaktuelle Werte sind online verfügbar (https://forrt-replications.shinyapps.io/fred_explorer/). Eine Evaluation über mehrere Disziplinen und Jahre erfordert noch hunderte weitere Replikationsstudien.",
    "crumbs": [
      "Die Geschichte der Open Science Bewegung",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Wandel im System</span>"
    ]
  },
  {
    "objectID": "wandel.html#footnotes",
    "href": "wandel.html#footnotes",
    "title": "6  Wandel im System",
    "section": "",
    "text": "Sozialwissenschaftler*innen wissen, dass das Aufsummieren der Werte unsinnig ist, da Level 2 nicht “doppelt so gut” wie Level 1 ist, und die verschiedenen Faktoren nicht immer gleich gewichtet werden sollten - rechnerisch wird aber genau das angenommen. Als heuristische Schätzung der Offenheit von Zeitschriften ist dieses Vorgehen jedenfalls sinnvoller als bibliometrische Maße.↩︎",
    "crumbs": [
      "Die Geschichte der Open Science Bewegung",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Wandel im System</span>"
    ]
  },
  {
    "objectID": "beimerstenversuch.html",
    "href": "beimerstenversuch.html",
    "title": "7  Beim ersten Versuch klappt es nie: Der Paradigmenwechsel in der Psychologie",
    "section": "",
    "text": "Ein Paradigmenwechsel ist vergleichbar mit einem Kippbild die dem Hase-Ente-Bild (Abbildung 1) wie es zum Beispiel Wittgenstein (Tractatus logico-philosophicus, 1984) gezeichnet hat. Bis zu einem gewissen Punkt sind sich alle einig, dass es ein Hase ist. Doch nach und nach kommen Erkenntnisse und Perspektiven hinzu. Der Punkt wird überschritten, die Ente ist anerkannt, und niemand würde es mehr für einen Hasen halten. Beim Hase-Ente-Bild lässt sich natürlich beliebig hin und her springen. Beim wissenschaftlichen Fortschritt kommt neues Wissen hinzu und eine Rückkehr ist nur noch schwer möglich.\n\n\n\nHase-Ente-Kippbild frei nach Wittgenstein: Die Quader auf der linken Seite können entweder als Schnabel einer nach links blickenden Ente oder Ohren eines nach rechts blickenden Hasen interpretiert werden.\n\n\nTatsächlich wurde ich von Beginn meiner Methodenausbildung an darin geschult, entsprechend des amtierenden psychologischen Paradigmas mit fehlgeschlagenen Replikationen umzugehen. Ich begann mein Studium bevor die Replikationskrise sich herauskristallisierte. Bei der ersten Studie, an der ich beteiligt war, replizierten wir den Befund, dass bunte Mengen in ihrer Anzahl weniger aussahen als einfarbige Mengen. Im Rahmen der Konsumentenpsychologie ist das hinsichtlich Slogans wie “viele viele bunte Smarties” etwas kontraintuitiv, es lässt sich aber gestaltpsychologisch plausibel darlegen. Nachdem eine Vorgruppe das Gegenteil herausfand, nämlich das bunte Smarties tatsächlich nach mehr aussahen als einfarbige Smarties, konnten wir in zwei eigenen Studien nichts vom beiden nachweisen. Egal ob die Teller, Tassen, oder Schalen, die wir den Leuten gaben, aus blauen, roten, gelben, oder bunten Schokolinsen bestanden: Unsere Versuchspersonen schätzten immer unabhängig von der Farbe. In den folgenden Jahren durfte ich als Tutor dann weitere Replikationsversuche durchführen. Der betreuende Professor erklärte: Ich habe es eigentlich noch nie erlebt, dass die Hypothese gleich beim ersten Versuch bestätigt wird. Nach jedem Experiment ist man schlauer und weiß, was man nächstes Mal besser machen muss. Es ist ganz natürlich, dass es ein paar Versuche dauert, bis man weiß, wie sich die Hypothese bestätigen lässt. Nachdem wir 5 (?XXX) Studien mit insgesamt über XXX Versuchspersonen durchgeführt hatten, die Autoren der Originalstudie um Rat gebeten haten, haufenweise Schokolinsen verzehrt hatten, und die Hypothese über alle Studien hinweg nicht bestätigt wurde, hatte ich das Vertrauen in den Befund verloren.\nUngefähr sechs Jahre später dachte ich während meiner Promotionszeit an die Studien zurück und diskutierte mit dem Professor. Im Rahmen der Replikationskrise war klar: Wenn man mehrere Experimente durchführt und die Hypothese eigentlich falsch ist, wird alleine durch den Zufall ab und zu trotzdem die Hypothese bestätigt. Das ist vergleichbar damit, dass selbst eine faire Münze sechs Mal hintereinander auf derselben Seite landen kann. Es passiert nicht oft, wenn man jedoch 10 Münzen gleichzeitig sechs Mal wirft, ist es nicht selten, dass eine davon sechs Mal auf derselben Seite landet. Aus dieser Perspektive hat das Zitat, dass die Ergebnisse beim ersten Versuch eigentlich nie so rauskommen, wie man es sich wünscht, einen bitteren Beigeschmack. Wenn die Hypothese falsch ist, ist es tatsächlich unwahrscheinlich, dass sie dennoch bestätigt wird. Nicht aber, wenn man mehrere Studien hintereinander durchführt. Dann ist es sogar zu erwarten, dass irgendwann eine Studie die Hypothese - auch wenn sie eigentlich falsch ist (!) - bestätigt.",
    "crumbs": [
      "Die Geschichte der Open Science Bewegung",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Beim ersten Versuch klappt es nie: Der Paradigmenwechsel in der Psychologie</span>"
    ]
  },
  {
    "objectID": "struktur.html",
    "href": "struktur.html",
    "title": "8  Struktur der Vertrauenskrise",
    "section": "",
    "text": "Im Gespräch über Präregistrierungen – eine Methode zur Erhöhung der Replizierbarkeit eines Befundes – entgegnete ein Kollege: “Schön und gut, aber solange sich das System nicht ändert, wird sich an Replikationsraten nichts ändern.” Wie lässt sich dieser Einwand verstehen? Zäumen wir dazu das Pferd von hinten auf: Spätestens seit dem Reproducibility Project Psychology (Open Science Collaboration, 2015) ist den meisten Psycholog*innen klar, dass wir massive Probleme bei der Replizierbarkeit von Befunden haben. Woher kommen diese genau? Ein wichtiges Problem ist hierbei der Publikationsbias, womit gemeint ist, dass bei der Veröffentlichung von wissenschaftlichen Berichten eine Auswahl getroffen werden muss und dadurch nur bestimmte Ergebnisse publiziert werden (z.B. Ergebnisse, die eine bestimmte Theorie stützen). Dadurch steht ein verzerrtes Bild der Realität. Oft schreiben Wissenschaftler*innen sogar nur bestimmte Befunde zu Berichten zusammen. “Fehlgeschlagene Experimente”, also solche, bei denen eine Hypothese nicht bestätigt oder eine Theorie nicht gestützt werden konnte, landen in der Schublade (engl. File-Drawer-Problem; Rosenthal, 1979; Sterling, 1959). In extremeren Fällen bedienen sich Wissenschaftler*innen verschiedener größtenteils anerkannter Methoden, die Daten so darzustellen, dass sie die Hypothese stützen oder tun so, als ob das, was sich aus den Daten lesen lässt, von Beginn an die Hypothese gewesen wäre (HARKing, REF). Aber was treibt Personen, die sich vor allem für den Beruf wegen ihres Interesse an der Funktionsweise der Welt (oder im Falle der Psychologie der Funktionsweise des Menschen) und wegen Ihrer Suche nach Wahrheit entschieden haben dazu, die Wahrheit zu schönen? Am Anfang des komplexen Prozesses, welcher zur Replikationskrise geführt hat, steht das aktuelle wissenschaftliche System: Ein Großteil der in der Wissenschaft beschäftigten arbeitet unter extremem Druck und prekären Arbeitsbedingungen. Um nach ein bis zwei Jahren eine Vertragsverlängerung zu erhalten, müssen Publikationen von Artikeln in möglichst angesehenen Fachzeitschriften nachgewiesen werden. Diese kriegt man durch besonders aussagekräftige und spannende Ergebnisse. Das Anreizsystem der Wissenschaft belohnt also nicht Wahrheit, Genauigkeit, Bescheidenheit, oder Transparenz, sondern vor allem diejenigen Dinge, die nicht in der Hand eine*r Wissenschaftler*in liegen: Spannende und eindeutige Ergebnisse.\nDer Prozess von prekären Arbeitsbedingungen bis zur niedrigen Replikationsrate ist in Abbildung 1 veranschaulicht. In den folgenden Kapiteln werden die Probleme und Lösungsansätze im Detail diskutiert.\nAbbildung 2\n\n\n\nDas Anreizsystem der Wissenschaft als Ursache für Replikationsfehlschläge\n\n\n[LR10] \n[1]Obgleich dieses „Reproducibility Project Psychology” die gesamte Fachgemeinschaft zutiefst erschütterte und den Weg für einen Paradigmenwechsel ebnete, bemängeln manche Forschende auch negative Auswirkungen auf nachfolgende Replikationsforschung. Indem 100 Studien gleichzeitig von einer Gruppe aus über 100 Forschenden veröffentlicht wurden, setzte das Projekt unrealistische Maßstäbe für Replikationsforschung (REF Gilad Feldman twitter siehe eine Mail Sommer 2022). Gleichzeitig war die Qualitätskontrolle dabei weniger streng, da die Einzelstudien nicht alle in dem Maße begutachtet werden konnten, wie es bei einer traditionellen Veröffentlichung der Fall gewesen wäre (z.B. Röseler 2022 OSF Replications). Einige gleichermaßen ambitionierte Vorhaben wurden veröffentlicht, wie zum Beispiel die ManyLabs Studien (REF) oder Versuche, bei denen unabhängige Gruppen dieselben Hypothesen testeten und replizierten (REF crowdsourcing hypothesis test). Oft beschränken sich diese Vorhaben auf Studien, die sich im Rahmen einer Online-Befragung replizieren lassen. Verhaltensbeobachtungen sind dabei unterrepräsentiert.",
    "crumbs": [
      "Die Geschichte der Open Science Bewegung",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Struktur der Vertrauenskrise</span>"
    ]
  },
  {
    "objectID": "probleme_system.html",
    "href": "probleme_system.html",
    "title": "9  Probleme des Wissenschaftlichen Systems",
    "section": "",
    "text": "9.1 Probleme des Wissenschaftlichen Systems\nNeben dem Idealbild davon, was Wissenschaft sein sollte oder wie sie funktionieren sollte, existiert die Wissenschaft, wie sie in unser Gesellschaftssystem integriert ist. Besonderheiten sind dabei, dass Wissenschaftler*innen ihre Tätigkeit als Beruf ausüben, also Geld dabei verdienen. Wie das Geld, das größtenteils aus Steuergeldern stammt, verteilt werden soll entscheiden Gremien, die wiederum selbst aus Wissenschaftler*innen bestehen. Durch die hohe Arbeitsbelastung, gleichzeitig Wissenschaft zu betreiben und zu verwalten vereinfachen sich die Entscheider*innen die Arbeit und verwenden zur Auswahl hochqualifizierter Personen Abkürzungen. So konnte es passieren, dass die Währung in der Wissenschaft die Anzahl an Publikationen und Zitationszahlen sind. Vorbilder wie Charles Darwin oder William James hätten nach dem heutigen Maßstab keine Chance auf eine unbefristete Stelle - sie haben einfach nicht genug Paper geschrieben. Viele der hier diskutierten Problemen, sind beispielsweise in der Psychologie seit mehreren Jahrzehnten bekannt, sodass eine Replikationskrise unabwendbar erschienen haben muss (Cronbach et al., 1991; Greenwald, 1976).",
    "crumbs": [
      "Probleme",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Probleme des Wissenschaftlichen Systems</span>"
    ]
  },
  {
    "objectID": "probleme_system.html#probleme-des-wissenschaftlichen-systems",
    "href": "probleme_system.html#probleme-des-wissenschaftlichen-systems",
    "title": "9  Probleme des Wissenschaftlichen Systems",
    "section": "",
    "text": "9.1.1 Wissenschaft versus Academia\nWährend Wissenschaften in ihrer Anfangszeit oft von Buchveröffentlichungen leben, die eine umfangreiche Basis von meist einzelnen Personen darlegen (e.g., Galileo, James, Darwin) stehen heutzutage wissenschaftliche Fachzeitschriften im Fokus. Diese Zeitschriften sind vergleichbar mit solchen, die es im Kiosk gibt, nur bestehen sie halt aus (meist englischsprachigen) Artikeln, die Wissenschaftler*innen verfasst und eingereicht haben und sind in vielen Fällen nur noch online oder in Büchereien erhältlich. Forschende laden sich dann einzelne Artikel aus den Zeitschriften herunter und Bibliotheken haben Verträge mit Verlagen und zahlen Geld, damit Universitätsangehörige Zugriff zu den Katalogen haben. Jeder eingereichte Artikel befasst sich mit einer Fragestellung, die von den Forschenden selbst festgelegt wurde. Darin werden meistens Studien mit deren Ergebnissen berichtet. Vor der Veröffentlichung werden die Artikel begutachtet, nicht von Mitarbeitenden der Zeitschrift sondern von Kolleg*innen. Mit diesem “peer-review” wird die Qualität von Forschung sichergestellt. Üblicherweise wird dabei darauf geachtet, dass die Schlussfolgerungen auf Basis der erhobenen Daten gerechtfertigt sind, die Fragestellung klar beantwortet wird, und die Befunde spannend oder überraschend sind. Zeitschriften unterscheiden sich darin, welche Themen sie abdecken (z.B. Sozialpsychologie, Konsumentenverhalten, Angewandte Sportwissenschaft, usw.) und von wie vielen Forschenden sie gelesen und zitiert werden. Wissenschaften sind also stark integriert in ein System, das Forschenden und Verlagen erlaubt, ein festes Gehalt zu verdienen.\n\n\n9.1.2 Prekäre Arbeitsbedingungen\nSoweit die Idealbedingungen – Wer in der Wissenschaft arbeitet, befindet sich jedoch in einem harten Konkurrenzkampf um eine der wenigen unbefristeten Stellen (Rahal et al., 2023). Vom Start der Promotion (Prozess der Erlangung eines Doktorgrades) bis zur Berufung auf eine Professur, also eine der raren unbefristeten Stellen, dauern Verträge meistens nur ein bis drei Jahre [LR1] und haben oft einen Umfang von weniger als 100% [LR2] - während es allerdings unüblich ist, mit weniger als 40 Stunden pro Woche (zumindest wurde das mir zu Beginn meiner Promotion erklärt). Während ein Großteil aller wissenschaftlichen Veröffentlichungen auf Studien beruht, die im Rahmen von Doktorarbeiten durchgeführt wurden, sind Doktorand*innen gleichzeitig diejenigen Personen im System, die den geringsten Wert haben bzw. deren Arbeitskraft am günstigsten ist. Mentale Probleme wie Burnout oder Depressionen sind unter Promovierenden weit verbreitet (Jaremka et al., 2020; Liu et al., 2019). Den Weg zur Professur schaffen vor allem die Personen erfolgreich, die viele Artikel in prestigeträchtigen Zeitschriften veröffentlichen. Durch die immense Arbeitsbelastung und große Zahl an Artikeln, die bei Fachzeitschriften eingereicht werden, ist keine Zeit mehr, Ergebnisse genau zu prüfen und nachzurechnen (Nuijten et al., 2017) sondern es wird vor allem darauf geachtet, wie eindeutig die Ergebnisse die Fragestellung beantworten - oder genauer gesagt: bestätigen (Mynatt et al., 1977). Mit anderen Worten: Es wird ausgerechnet der Teil der wissenschaftlichen Arbeit belohnt, der nicht in der Hand der Forschenden liegt, nämlich die Ergebnisse von Untersuchungen. Veröffentlichte Artikel und Prestige statt Qualität (Brembs, 2018) sind ab dort die Währung der Wissenschaft: Auf ihrer Basis wird entschieden, wer Forschungsgelder erhält und auf Basis von Forschungsgeldern und Publikationen werden Professuren vergeben. In den darüber entscheidenden Berufungskommissionen lesen die Beteiligten üblicherweise nicht die Artikel der Bewerbenden, sie zählen bloß, wie viele in welchen Zeitschriften aufgelistet werden. Im Folgenden gehe ich auf die einzelnen Probleme näher ein.\nZu diesen verheerenden Problemen kommen außerdem systemische Probleme der sexuellen Belästigung (Hoebel et al., 2022) und des Machtmissbrauches, die in dem aktuell streng hierarchischen Aufbau des Systems nur schwierig zu lösen sind (REF[LR3] , siehe auch https://www.netzwerk-mawi.de/ und https://www.jmwiarda.de/2023/11/20/das-stille-leiden-der-betroffenen/). Wer es in dabei besonders leicht hat, erklären Elsherif et al. (2022) anschaulich an dem „Academic Wheel of Privilege” (S. 85; siehe auch https://www.psychologicalscience.org/observer/gs-navigating-academia-as-neurodivergent-researchers). Beispielsweise haben Doktorandinnen [LR4] in den Niederlanden vor allem dann schlechtere Noten als Doktoranden bekommen, wenn der Promotionsausschuss nur aus Männern bestand (REF[LR5] ).\n\n\n9.1.3 Zu viel Forschung\nIm Rahmen von Promotionen müssen Forschende in insgesamt 3-6 Jahren üblicherweise drei wissenschaftliche Artikel veröffentlichen (bzw. in fairen Fällen drei veröffentlichungs-würdige Artikel vorweisen). Bei Post Docs, also Personen nach der Promotion und vor dem Wissenschaftsausschluss beziehungsweise in selteneren Fällen der Professur (siehe Kapitel XXX) müssen es noch mehr sein. Für eine Habilitation, für die eine ähnliche Zeit angesetzt ist, sind circa 6 Artikel die Daumenregel. Dabei spielt es eine nachrangige Rolle, wie umfangreich die Artikel sind. Beispielsweise dauert die Durchführung einer Meta-Analyse, in der bisherige Befunde zu einem bestimmten Thema systematisch gesammelt und statistisch zusammengefasst bzw. verglichen werden, oft mehrere Jahre. Eine Querschnittserhebung über einen Online-Fragebogen lässt sich in wenigen Wochen durchführen. Eine Doktorandin, die eine einzige Meta-Analyse durchführt, könnte damit nicht promovieren. Hätte sie stattdessen drei einfache Online-Studien durchgeführt und einzeln veröffentlicht, wäre es kein Problem.[1] Diese willkürlichen Vorgaben haben dazu geführt, dass sich Wissenschaftler*innen alleine durch die Begutachtung der Artikel ihrer Kolleg*innen einen enormen Arbeitsaufwand auferlegen, der den wissenscchaftlichen Fortschritt behindert (Hanson et al., 2023).\nZur Veranschaulichung des Aufgabenpensums nun ein Gedankenspiel: Angenommen es gäbe 10 Wissenschaftler*innen, die gemeinsam 10 Artikel im Jahr veröffentlichen würden - manchmal alleine, manchmal in einer Gruppe - und jeder der Artikel würde von zwei Personen begutachtet, so müsste jede*r zwei Artikel begutachten. Damit das System funktioniert, müsste jede Person die Anzahl der im Schnitt veröffentlichten Artikel mal die Anzahl der benötigten Gutachtenden begutachten. Bei 10 Veröffentlichungen pro Person und drei Gutachtenden wären es 10x3=30 Gutachten. Nun werden aber nicht alle Artikel von der Zeitschrift, bei der sie eingereicht werden, veröffentlicht, noch werden sie sofort veröffentlicht. Wissenschaftler*innen reichen ihre Artikel oft bei den “hochrangigsten” Zeitschriften ein. Nachdem dort mehrere Gutachter*innen den Artikel geprüft haben, wird er abgelehnt (Jaremka et al., 2020). Mindestens werden Revisionen angefordert, welche oft eine weitere Runde peer-review auslösen und nicht immer werden Artikel danach veröffentlicht. Unsere Rechnung geht also nicht auf: Nehmen wir vorsichtshalber an, ein Artikel würde neun Mal begutachtet (z.B. einmal drei Gutachtende, dann Ablehnung, dann erneut drei Gutachtende, Revision, zweites Gutachten, Akzeptanz). Aus 10x3 wird 10x9, bei etwas Urlaubszeit also etwas mehr als zwei Gutachten pro Woche, idealerweise bis zu zwei Arbeitstage. Bei dieser Rechnung bleibt weniger Zeit für Lehre, Wissenstransfer, Betreuung von Studierenden oder Promovierenden, Einwerbung von Forschungsgeldern, universitäre Selbstverwaltung, usw. Durch die vielen zu publizierenden Artikel und das strenge Review-System bürgt sich die Wissenschaft einen großen Berg Arbeit auf - einen der realistisch nicht machbar ist und unter dem am Ende die Qualität der Forschung leidet. Beispielsweise fiel es weder den Gutachtenden, noch den Herausgebern von Zeitschriften auf, dass in über 30 Artikeln mitten im Text „Regenerate response” stand – ein Satz, der in OpenAIs ChatGTP Programm auf einem Knopf erlaubt, einen von einer künstlichen Intelligenz erstellten Text umzuformulieren (https://retractionwatch.com/2023/10/06/signs-of-undeclared-chatgpt-use-in-papers-mounting/). In manchen Artikeln hieß es sogar „As an AI language model, I …” (https://pubpeer.com/search?q=“As+an+AI+language+model%2C+I“). In einem Fall wurde der Artikel von dem Verlag Elsevier geändert, und zwar nicht auf dem empfohlenen Weg[2] mittels eines transparenten Errandum oder Corrigendum, also einer öffentlichen Mitteilung über die Änderung und ihre Gründe, sondern ohne Erklärung oder Zustimmung der Autor*innen (https://predatory-publishing.com/elsevier-changed-a-published-paper-without-any-explanation/).\n\n\n9.1.4 Publish or Perish[LR6] \nWer in der Wissenschaft arbeitet sollte die wichtigste Spielregel kennen: Wer überleben will, muss Artikel veröffentlichen. Zur Promotion, Habilitation, Einwerbung von Forschungsgeldern, und zur Berufung auf eine Professur sind Veröffentlichungen das oberste Kriterium. Kennzeichen einer Währung ist, dass sich Dingen ein Wert zuweisen lässt. Wie sieht der Wert in der Forschung aus?\nBibliometriker*innen entwarfen zur Beschreibung (also explizit nicht zur Bewertung) verschiedener Forschungsgebiete verschiedene Kennzahlen, wie den Impact Factor, oder Hirsch-Index. [ERKLÄRUNGEN] - impact factor - h index - ccc factor.\nIF wird geschönigt (REF[LR7] ), gehört Verlag, der damit Geld verdient, ist inflationiert, selbe Jahre erhalten unterschiedliche Werte: https://quantixed.org/2016/01/05/the-great-curve-ii-citation-distributions-and-reverse-engineering-the-jif/\nEvidenz, dass methodische Sauberkeit negativ bis gar nicht mit traditionellen Produktivitätsmetriken zusammenhängt (deutlich negativ mit Zitationszahlen und h-index). https://www.researchgate.net/publication/380433173_Inter-Rater_Reliability_in_Assessing_the_Methodological_Quality_of_Research_Papers_in_Psychology Table 3\nSeit längerem wird für die verantwortungsvolle Verwendung dieser Metriken plädiert [LR8] (Hicks et al., 2015). Wie sich Anreizstrukturen und Karrierestatus auf wissenschaftliches Fehlverhalten auswirken wird mit gemischten Ergebnissen untersucht (REF[LR9] ). Das Problem: Sobald es in einem System ein klares Bewertungskriterium gibt, wird alles darauf ausgerichtet.\nDas hat zum Beispiel dazu geführt, dass die meisten in der Psychologie entwickelten Instrumente zur Messung von Persönlichkeitseigenschaften nur wenige Male verwendet werden – und das auch hauptsächlich von ihren eigenen Entwickler*innen (Elson et al., 2023). Ein noch extremerer Ausmaß ist bei sogenannten „Paper Mills” zu sehen (van Noorden, 2023): Personen erstellen dabei automatisiert große Mengen von wissenschaftlichen Artikeln, nur ohne die darin beschriebenen Untersuchungen wirklich durchzuführen. Wissenschaftler*innen können dann Ko-Autor*innenschaften kaufen. Je nach Zeitschrift werden diese Artikel nicht im Peer-Review aufgedeckt. Es wird befürchtet, dass Käufer*innen solcher Artikel selbst sehr erfolgreich werden können, selbst Herausgeber von Zeitschriften werden, und sich dadurch vor Entlarvung beschützen. Der genaue Ausmaß des Paper-Mill-Problems ist unklar und weitgehend unerforscht (Byrne & Christopher, 2020). Ein Indiz, mithilfe künstlicher Intelligenz erstellte Artikel zu erkennen sind sogenannte „tortured phrases” (Cabanac et al., 2021).\n\n\n9.1.5 Flaschenhals-Hypothese und Innovationsdrang\nNamhafte wissenschaftliche Zeitschriften erhalten täglich unzählige Einreichungen, veröffentlichen aber nur eine begrenzte Anzahl an Artikeln. Sie müssen also streng selektieren, was begutachtet und gegebenenfalls veröffentlicht wird. Weil das Ziel einer Zeitschrift ist, viel gelesen zu werden, wählen Herausgeber*innen von Zeitschriften diejenigen Artikel, welche möglichst großes Potenzial haben, bekannt und viel zitiert zu werden (Giner-Sorolla, 2012). Das betrifft zum Beispiel Beiträge mit besonderen praktischen Implikationen, überraschenden Befunden, oder besonders konsistenten Befunden. Studien, deren Ergebnisse keine eindeutigen Schlüsse zulassen – oder deren Autor*innen mit zu großer Vorsicht Schlüsse ziehen – kommen also nicht infrage. In den Neurowissenschaften kommunizieren manche Zeitschriften beispielsweise öffentlich, dass sie keine Replikationsstudien veröffentlichen und nach Neuheit selektieren, während die meisten keine Stellung dazu nehmen (Yeung, 2017). In der Psychologie nahmen 2017 nur 33 von 1151 Zeitschriften Stellung dazu, dass sie Replikationen akzeptieren (Martin & Clarke, 2017). Zwar werden innovative Befunde dann häufiger zitiert, die Zeitschrift erhält also mehr Leser*innen, mehr Einreichungen, und damit mehr Geld über Abonnements und Veröffentlichungskosten, vielzitierte Artikel lassen sich jedoch schlechte replizieren als weniger zitiert (Serra-Garcia & Gneezy, 2021) und prestigereiche Zeitschriften sing Magneten für fragwürdige Forschungspraktiken (Kepes et al., 2022) und nachweisbar gleichwertige oder sogar qualitativ schlechtere Forschung (Brembs, 2018).[LR10] \nWie kommt es dazu? Diese strenge Selektion, die einem Flaschenhals ähnelt (viele Einreichungen aber wenige Veröffentlichungen) führt gemeinsam mit dem Anreiz in eben solchen Zeitschriften zu publizieren dazu, dass Forschende alle möglichen Mittel nutzen, um eine Chance auf eine Publikation zu erhalten. Die Tatsache, dass prestigereiche Zeitschriften wie Nature Human Behavior vor allem Artikel mit klaren Botschaften veröffentlichen[LR11] , spornt also Forschende an, klare Ergebnisse zu berichten. Passt mal ein Befund nicht zu der geprüften Hypothese, wird er nicht veröffentlicht und landet in der Schublade.\n\n\n9.1.6 Schubladen-Problem[LR12] \nSeit mehreren Jahrzehnten ist bekannt, dass Wissenschaftler*innen vor allem diejenigen Studien veröffentlichen, die ihre Theorien stützen (REF; rosenthal; sterling). Im Extremfall hat jemand zum Prüfen einer Theorie fünf Studien durchgeführt, in nur einer davon die Theorie bestätigt, und nur diese veröffentlicht. Andere Forschende, die dann die (veröffentlichte) Literatur durchsuchen, sehen nur die “erfolgreiche” Studie. Es entsteht der Eindruck, dass die Theorie stimmt, während die Mehrheit der Studien diesen Schluss eigentlich nicht nahelegt. Durch dieses Problem konnten sich ganze Forschungsstränge entwickeln, die seit dem Bewusstsein für Replikationsstudien komplett ausgestorben sind (Brockman, 2022).\nFür Meta-Analysen, also Studien, die bisherige Befunde zusammenfassen, wurden bereits verschiedene Methoden entwickelt, die Stärke des Schubladen-Problems (engl. File-Drawer-Problem) zu prüfen. Auch Methoden, die dadurch entstandene Verzerrung zu korrigieren existieren bereits vielzählige (REF, PET-PEESE, p-uniform*, robust bayesian meta-analysis robumeta, hedges vevea selection models). Allerdings funktioniert keine der Methoden in allen möglichen Szenarien (Carter et al., 2019). Um eine Veröffentlichung der fehlgeschlagenen Studien werden wir möglicherweise nicht herumkommen.\nIn der Medizin gibt es den besonderen Fall, dass alle dort durchgeführten Studien öffentlich registriert [LR13] werden müssen. Bei einer Veröffentlichung muss dann eine Registrierungsnummer angegeben werden. Über öffentliche Angaben zu registrierten Studien lässt sich somit nachverfolgen, welche Personen, Institutionen, oder Länder wie viele ihrer tatsächlich durchgeführten Studien veröffentlichen. Forschende haben dazu ein sogenanntes Dashboard entwickelt (Franzen et al., 2023; Riedel et al., 2022) mittels dem nach aktuellem Stand (Herbst 2023, https://quest-cttd.bihealth.org/) nachvollziehbar ist, dass unter den registrierten Studien, die auf die Registrierung verweisen, nur 46% innerhalb der folgenden zwei Jahre und 74% innerhalb der folgenden fünf Jahre veröffentlicht wurden. Personen, die sich für die Studien als Versuchspersonen melden oder Drittmittelgeber erhalten somit Aufschluss über die Größe der Schublade, „in der die nicht so spannenden Ergebnisse landen”.\n\n\n9.1.7 Konzerne\n-          https://stoptrackingscience.eu\n\n\n9.1.8 Zugängigkeit von Wissen\n-          Global south\n-          Paywalls\n-          Soziales Dilemma\n-          VG Wort[LR14] ?\n[1] Hier ließe sich einwenden, dass einige Zeitschriften nur Artikel veröffentlichen, in denen mehrere Studien durchgeführt wurden. Das Ziel, nämlich die Replikation der eigenen Befunde, verfehlen diese Zeitschriften damit deutlich. Stattdessen reizt es Forschende dazu an, mehrere Studien mit wenigen Versuchspersonen durchzuführen, statt eine Studie mit vielen Befragten.\n[2] Ethische Richtlinien im Publikationsprozess sind zum Beispiel verfügbar über das Committee on Publication Ethics (https://publicationethics.org/guidance/Guidelines).\n [LR1]REF?\n [LR2]REF?\n [LR3]https://onlinelibrary.wiley.com/doi/10.1002/joe.21897\n [LR4]https://www.laborjournal.de/rubric/essays/essays2023/e23_09.php\nQuote schwerbehinderter extrem niedrig (1% unter wiss Mas) und Ignoranz von Quoten in der Wissenschaft\n [LR5]https://www.nature.com/articles/s41598-023-46375-7\n [LR6]Liste: World’s Top 2% most influential scientists à fehlerhaft https://www.authorea.com/users/571220/articles/620441-a-critical-analysis-of-the-world-s-top-2-most-influential-scientists-examining-the-limitations-and-biases-of-highly-cited-researchers-lists\n [LR7]https://osf.io/preprints/socarxiv/5t8v7?fbclid=IwZXh0bgNhZW0CMTAAAR0fPKhqCqVY5GIVdZp5TWvlOmBE_m-NdchII9TDpy3Nd5uarSltMcxuEnE_aem_AZ1HBApXlv6zfx2OE42kC-JUhL46lvBK94fnsx2udMBZ7L__WdOVM3iSlXFRgD7MscOgnIZe1y-6Vjlnz7uHAX9W\n [LR8]COARA + SF DORA\n [LR9]https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0255334\n [LR10]+ https://osf.io/preprints/psyarxiv/4w7rb\n [LR11]https://www.frontiersin.org/articles/10.3389/fnhum.2018.00037/full\n-          gute Zeitschriften haben schlechtere Qualität\nhttps://doi.org/10.1107/S0907444907033847\n-          impact factor ist verhandelbar und inflated:\nhttps://quantixed.org/2016/01/05/the-great-curve-ii-citation-distributions-and-reverse-engineering-the-jif/\n-          noch viele weitere Beispiele im Paper, felderübergreifend entweder kein Zusammenhang oder negativer Zusammenhang\n-          höherer Impact Factor geht mit von Excel zerstörten Gen-Namen einher (Excel denkt, es seien Datumsangaben, konvertiert sie, und man kriegt die eigentlichen Werte nicht wieder): https://genomebiology.biomedcentral.com/articles/10.1186/s13059-016-1044-7#:~:text=Indeed%2C%20the%20number%20of%20papers,(3.8%20%25%20per%20year).\nAlternative Auswahlkriterien\n-          Schauen, mit wem man Kollege sein möchte\n-          „Wie gut komplementiert das die Expertise” statt „macht die Person, was ich mache” (nötig für Forschungsgelder)\n-          Jemanden, mit dem man eng zusammenarbeit, findet man auch online; interessant wird es, wenn jemand von wo ganz anders kommt; etwas, was man nicht online findet\n-          Sehr vorsichtig sein mit quantitativen Maßen, bei vielen Bewerber*innen aber schwierig; wenn, dann bräuchte man viele, die genau auf den Bedarf zugeschnitten ist\n [LR12]Problem von Nullbefunden erklären, 7 Alternativerklärungen\nhttps://www.cambridge.org/core/journals/journal-of-experimental-political-science/article/more-than-meets-the-itt-a-guide-for-anticipating-and-investigating-nonsignificant-results-in-survey-experiments/C01250EB50598D6328B6065F1DF86BA7?utm_source=hootsuite&utm_medium=twitter&utm_campaign=MNE_campaign\n [LR13]Warum? Ethic-gutachten?\n [LR14]https://de.wikipedia.org/wiki/Verwertungsgesellschaft_Wort",
    "crumbs": [
      "Probleme",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Probleme des Wissenschaftlichen Systems</span>"
    ]
  },
  {
    "objectID": "probleme_karriere.html",
    "href": "probleme_karriere.html",
    "title": "10  Karriere in der Wissenschaft",
    "section": "",
    "text": "Ein Beruf in der Wissenschaft setzt üblicherweise einen einschlägigen Studienabschluss (Bachelor und Master) voraus und beginnt mit einer Promotion. Im Rahmen der Promotion wird das wissenschaftliche Handwerk erlernt und der Doktor*ingrad erlangt: Studien werden durchgeführt, Daten analysiert, und Forschungsartikel veröffentlicht. In den meisten Fällen arbeiten Promovierende mit einer 50 – 66% Stelle als wissenschaftliche Mitarbeiter[LR1] *innen, begutachten für ihre Vorgesetzten Artikel und Forschungsgelder-Anträge, schreiben eigene Anträge, verbringen mehrere Monate im Ausland, erstellen, beaufsichtigen, und korrigieren Klausuren, betreuen Abschlussarbeiten, und beteiligen sich an der universitären Selbstverwaltung durch die Teilnahme an Sitzungen und Mitgliedschaften bei Kommissionen. Zeitlich sind dabei bei Stellen oder Stipendien üblicherweise drei Jahre angesetzt. Am Anfang meiner Promotion sagte man mir, mit einer 40-Stunden-Woche schafft man es nicht, in der vorgesehenen Zeit zu promovieren – 50% Gehalt für über 100% Arbeitszeit und einem befristeten Vertrag also. Verträge sind dabei jedoch nicht immer auf die Zeit der Promotion befristet. Viele wissen erst wenige Monate vor Vertragsende, wie viel Prozent Gehalt sie erhalten, wie umfangreich die Lehrverpflichtung ist, und wie lange der nächste Vertrag geht. Kurz: Die Arbeitsbedingungen sind nicht optimal und man muss ein hohes Maß an Flexibilität mitbringen, wenn man sich für den Weg in die Wissenschaft entscheidet.\n\n10.0.1 Doppelabhängigkeit\nFür die Promotionsphase gibt es verschiedene Finanzierungsmöglichkeiten: Über Unternehmen lässt sich berufsbegleitend promovieren oder Stipendien zahlen über eine begrenzte Zeit Geld, das den Grunderhalt sichert (z.B. 1100€ über 36 Monate bei der Graduiertenförderung des Landes Sachsen-Anhalt, dazu kommen noch zusätzliche Kosten für die Sozialversicherung). Der häufigste Weg ist jedoch über eine Stelle als wissenschaftliche*r Mitarbeiter*in. Dabei ist die vorgesetzte Person diejenige Person, die auch die Promotionsarbeit bewertet. Die einem auferlegte Korrektur der 120 Erstsemester-Klausuren steht dann im Extremfall in Konflikt mit der Zeit, die für die Arbeit an der wissenschaftlichen Studie benötigt wird. Promovierende hängen also meistens von den betreuenden Professor*innen in Form der Beschäftigung und über die Bewertung ihrer Arbeit ab, was also als Doppelabhängigkeit bezeichnet wird.\n\n\n10.0.2 Depressionen, Burnout, und #IchbinHanna\nFächerübergreifend hat sich als Antwort auf ein inzwischen gelöschtes Erklärvideo vom Bundesministerium für Bildung und Forschung zum Wissenschaftszeitvertragsgesetz (WissZeitVG) eine Bewegung unter dem Hashtag #IchbinHanna entwickelt, die die dortige sachliche Erklärung (https://www.youtube.com/watch?v=PIq5GlY4h4E) und die Arbeitsbedingungen in der Wissenschaft stark kritisiert. Es heißt „damit auch nachrückende Wissenschaftlerinnen und Wissenschaftler die Chance auf den Erwerb dieser Qualifizierung haben und nicht eine Generation alle Stellen verstopft, dürfen Hochschulen und Forschungseinrichtungen befristete Verträge nach den besonderen Regeln des WissZeitVG abschließen. So kommt es zur Fluktuation und die fördert die Innovationskraft”. 90% aller Wissenschaftler*innen sind unbefristet angestellt (https://www.youtube.com/watch?v=H1wJmqpGhJc).\nPlanungsunsicherheit und massiver Konkurrenzdruck fördern die Innovationskraft? Das scheint unwahrscheinlich: Unter Forschenden mentale Erkrankungen wie Burnout (REF common academic problems) und Anzeichen für Angststörungen und Depression (REF, anxiety depression) stark verbreitet.\n\n\n10.0.3 Top-Down-Wandel\nDiejenigen, die das System ändern können, also alle mit unbefristeten Verträgen, leiden nicht mehr unter ihm. Für diejenigen, die unter dem System leiden, ist es unklug, das System ändern zu wollen und sich an die Professor*innen zu wenden, denn das sind die Leute, die über ihren späteren Verbleib in der Wissenschaft im Rahmen von Berufungskommissionen bei der Entscheidung der Vergabe von Professuren entscheiden. Im Extremfall kann das dazu führen, dass jemand Kritik an Arbeiten von Wissenschaftler*innen in höheren Positionen aus Angst, die eigenen Chancen auf eine Professur zu schmälern, zurückfährt. Auf der anderen Seite haben Professor*innen bewiesen, dass sie sehr gut nach den Spielregeln spielen können. Zuzugeben, dass sie eine der seltenen und heiß begehrten Stellen nicht über Qualität sondern Quantität ihrer Forschung erhalten haben, hieße, sich selbst abzuwerten.\n [LR1]REF",
    "crumbs": [
      "Probleme",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Karriere in der Wissenschaft</span>"
    ]
  },
  {
    "objectID": "probleme_methoden.html",
    "href": "probleme_methoden.html",
    "title": "11  Probleme der wissenschaftlichen Methoden",
    "section": "",
    "text": "Im Alltagsdenken herrscht noch oft der Mythos, dass Wissenschaft sich von Nicht-Wissenschaft durch die wissenschaftliche Methode unterscheide. Das ist falsch (e.g., Feyerabend, 1975/2002). Zwar unterscheidet sich wissenschaftliches Wissen von alltäglichem Wissen (und auch Religion) durch einen höheren Grad an Systematizität (REF Hoyningen-Huene, 2013), allerdings gibt es weder eine einzige noch eine persistente wissenschaftliche Methode. Methoden haben sich stattdessen über die Zeit gewandelt, und das ist auch gut so. Neue Technologien ermöglichen beispielsweise in der Physik hochpräzise Messungen mittels Elektronenlaser, in den Geschichtswissenschaften 3D-Scans von Artefakten, die sonst nur wenige sehen würden, oder in den Sozialwissenschaften Datenbanken mit freiwillig bereitgestellten und anonymisierten Chatverläufen (https://db.mocoda2.de/c/home).\nSo wie ein Hammer und andere Werkzeuge nicht per se gut oder schlecht sind, so sind Methoden nicht gut oder schlecht, nicht richtig oder falsch, sondern sie werden angemessen und korrekt verwendet oder missbraucht. Statt Missbrauch ist in den Sozialwissenschaften von fragwürdigen Forschungspraktiken (Questionable Research Practices), kurz QRP, die Rede. Sie erlauben Wissenschaftler*innen Befunde zu generieren, die sie wollen, statt Befunden, die es tatsächlich gibt. Im Folgenden werden verbreitete und oft angewandte (John et al., 2012) Techniken vorgestellt (für einen Überblick über die Forschung dazu in den letzten 50 Jahren, siehe auch Neoh et al., 2023).\n\n11.0.1 Exploratorische versus konfirmatorische Forschung\nZum Verständnis der fragwürdigen Forschungspraktiken (QRP) ist eine wichtige forschungstechnische Unterscheidung unabdingbar: Wie bei einem Spaziergang kann eine wissenschaftliche Untersuchung erkundend oder zielgerichtet sein. Mal wird frei durch die Gegend spaziert und dabei neue Entdeckungen gemacht, mal ist das Ziel und die Route klar und im Vorhinein bestimmt. Im wissenschaftlichen Kontext ist die Rede von exploratorischer und konfirmatorischer Forschung. Bei der Exploration stehen höchstens die Forschungsfrage und grobe Züge der Methode fest, bei einem konfirmatorischen Test ist alles durchdacht: Vorgehen, mögliche Ergebnisse, sowie Erklärungsansätze für jedes mögliche Resultat. Es wird dann eine spezifizierte Hypothese mit der dazugehörigen Theorie bestätigt oder eben nicht. Kein Vorgehen ist dem anderen per se überlegen (siehe Abbildung Z). Üblicherweise beginnen Untersuchungen in einem bisher wenig erschlossenen Gebiet mit Exploration, während mehr vorhergehende Forschung mit klareren Erwartungen einhergeht. Es sei dazu gesagt, dass es sich hier um Extremtypen von Forschung handelt, die ein Spektrum bilden. Geisteswissenschaftlich erlauben außerdem erst beide Ansätze zusammen Erkenntnisgewinn. Im Rahmen des hermeneutischen Zirkel, (einfach gesagt “dem Kreis des Verstehens”, Abbildung Y) wird aus einzelnen Beobachtungen eine allgemeine Gesetzmäßigkeit formuliert (Induktion) und diese Gesetzmäßigkeit wird im Anschluss bei weiteren Einzelbeobachtungen geprüft (Deduktion).\n\n\n\nHermeneutischer Zirkel\n\n\nProblematisch wird es, wenn exploratorische Forschung als konfirmatorische kommuniziert wird, also so getan wird, als hätte eine Einzelbeobachtung eine bereits formulierte Gesetzmäßigkeit bestätigt, statt sie bloß inspiriert. Diese Art Unlogik heißt Zirkelschluss: Die Gesetzmäßigkeit gilt wegen der Beobachtung. Und die Beobachtung entspricht der Gesetzmäßigkeit.\nAbbildung Z\nSkizziertes Vorgehen bei explorativer (a) versus konfirmatorischer (b) Forschung. Exploratives Vorgehen ist nicht zielgerichtet, die Richtung kann sich ändern, und ist manchmal mit unvorhergesehenen Ergebnissen verbunden. Konfirmatorisches Vorgehen bildet oft einen engen und kontrollierten Ausschnitt eines Sachverhaltes ab.\n\n11.0.1.1 Methoden der Datengenerierung\nWissenschaftliche Disziplinen bedienen sich für gewöhnlich vieler verschiedener Methoden. Idealerweise sind Erkenntnisse unabhängig von der Methode, die zu ihrer Entdeckung geführt hat und verschiedene Methoden führen zur selben Erkenntnis. Typische sozialwissenschaftliche Methoden sind die Befragung mittels standardisiertem Fragebogen, Verhaltensbeobachtung mittels Kameraaufzeichnungen und anschließender Kodierung von Verhaltensweisen durch mehrere Personen, die den Untersuchungszweck nicht kennen, indirekte Methoden (Schimmack, 2019), bei welchen etwas anderes gefragt wird, als was gemessen werden soll, Verhaltensmessungen wie Blickrichtungsmessung (eye tracking), oder Simulationsstudien, mittels derer zum Beispiel Verkehrsflüsse auf Basis vorher festgelegter Prinzipien per Computer berechnet werden oder Panikattacken vorhergesagt werden (Robinaugh et al., 2021). Diese Methoden generieren fast immer Daten, also beispielsweise eine Tabelle, in der je Beobachtungseinheit (z.B. je Versuchsperson) in mehreren Spalten Daten festgehalten werden und diese Daten werden fast immer statistisch ausgewertet. Die Notwendigkeit einer solchen Auswertung ergibt sich daraus, dass die Beobachteten Gesetzmäßigkeiten keine absoluten Gesetze im Sinne von „alle männlichen Babys wiegen mehr als alle weiblichen Babys” sind, sondern statistische Regelmäßigkeiten im Sinne von „im Mittel wiegen kurz nach der Geburt männliche Babys ein paar hundert Gramm mehr als weibliche Babys, aber nicht jedes männliche Baby wiegt mehr als jedes weibliche Baby” sind (siehe Abbildung X).\nABBILDUNG X\nStatistische Phänomen; männliche babys vs. Weibliche babys mit 2 Normalverteilungen generieren[LR1]\n\n\n11.0.1.2 Statistische Signifikanz und Fehler erster Art\nEine der am weitesten verbreiteten Methoden in den Sozialwissenschaften (und auch darüber hinaus) ist Statistik, genauer Inferenzstatistik. Dabei wird von einer begrenzten Menge von Beobachtungen (z.B. ausgefüllte Fragebogen von 100 Personen) auf alle möglichen Beobachtungen (z.B. alle Menschen) verallgemeinert. Untersuchte Zusammenhänge sind selten eindeutig, es gibt aber häufig statistische Regelmäßigkeiten. Charakteristisch ist dabei ein gewisses Zufallselement. Wiegt man ein kürzlich geborenes männliches und weibliches Baby, dann ist die Wahrscheinlichkeit sehr hoch, dass das männliche Baby mehr wiegt. Es kommt aber auch häufig vor, dass das nicht der Fall ist. Ähnlich verhält es sich bei einer fairen Münze, also einer die im Mittel gleich häufig auf Kopf und auf Zahl landet: Dass sie bei insgesamt vier Würfen immer auf Kopf landet ist unwahrscheinlich, dass sie 1 oder 2 Mal auf Kopf landet, es kommt aber durch aus vor (nämlich in 12,5% aller Fälle, in denen eine faire Münze vier Mal hintereinander geworfen wird).\nInferenzstatistische Tests gehen nun davon aus, dass bei der Betrachtung eines statistischen Zusammenhanges (z.B. Geschlecht und Geburtsgewicht, Körpergewicht und Größe, Bildungsniveau der Eltern und Bildungsniveau der Kinder) „nur der Zufall am Werk ist” (e.g., Röseler & Schütz, 2022b). Unter der Annahme wird berechnet, wie häufig ein beobachteter Zusammenhang mit der beobachteten Stärke vorkommen würde, wenn eigentlich kein Zusammenhang vorliegen würde. Also zum Beispiel „dass eine faire Münze vier Mal auf Kopf landet, passiert in 12,5% aller Fälle”. Bei sechs Würfen wären es 1,5625%. Die Kunst des statistischen Schließens besteht nun darin, den Punkt zu finden, ab dem Forschende davon ausgehen, dass der Zufall nicht am Werk war, weil die berechnete Wahrscheinlichkeit so gering ist. Konventionell liegt dieser bei 5%, für neue Befunde manchmal bei 0,5% (Benjamin et al., 2018), und in besonders prekären Fällen noch niedriger. Fachtechnisch wird von einem Alpha-Niveau oder einem Signifikanzniveau gesprochen und die berechnete Wahrscheinlichkeit heißt p-Wert. P-Werte unter 5% werden statistisch signifikant oder auf dem 5%-Niveau signifikant bezeichnet. Forschende würden also sagen, dass eine Münze nicht fair ist, wenn sie sechs Mal hintereinander auf Kopf landet (sogar schon bei fünf Mal, was in 3,125% der Fälle vorkommt). Dabei nehmen sie in Kauf, dass sie, wenn die Münze eigentlich doch fair ist, in 5% aller Fälle falsche Schlüsse ziehen.\nAuf der anderen Seite ist es durchaus möglich, dass eine Münze nicht fair ist, zum Beispiel in 60% der Fälle auf Kopf landet und in 40% auf Zahl. [LR2] \n\n\n\nEine einzelne Studie führt noch nicht zu sicherer Erkenntnis. Auch, wenn ein untersuchter Zusammenhang nicht vorliegt, kann er in Daten zufällig aufscheinen. Und auch, wenn eigentlich ein Zusammenhang vorliegt, kann dieser in den Daten durch Zufallsschwankungen nicht zu erkennen sein. Mithilfe von Open Science Praktiken soll der Zustand in den linken Kästen wiederhergestellt werden. Aus Röseler, L. (2021). Wissenschaftliches Fehlverhalten [Abbildung]. https://osf.io/uf7gz/. Lizenziert unter CC BY-Attribution International 4.0.\n\n\n\n\n\n11.0.2 Freiheitsgrade[LR3]  von Forschenden (Researchers’ Degrees of Freedom)\nVollständige Studien mehrfach durchzuführen ist sehr aufwändig. Obwohl es ein relativ sicherer Weg zu signifikanten P-Werten ist, gibt es weitaus sparsamere Lösungen. Die meisten Analysen sind um ein vielfaches komplexer als die oben beschriebene Münzwurfstudie. Betrachten wir den immer noch sehr einfachen Signifikanztest für einen Korrelationskoeffizienten. Der Koeffizient ist eine Zahl zwischen -1 und 1 und beschreibt die Art des Zusammenhanges zwischen zwei Variablen (z.B. Einkommen und Lebenszufriedenheit). 0 bedeutet, dass kein Zusammenhang vorliegt, positive Werte bedeuten, dass wenn die eine Variable hohe Werte hat, dann hat auch die andere hohe Werte, und negative Korrelationen bedeuten, dass wenn eine Variable hohe Werte hat, dann hat die andere Variable eher niedrige Werte. In Abbildung X sind verschiedene Korrelationen dargestellt.\nAbbildung X: Verschiedene Zusammenhänge zwischen zwei Variablen und deren Korrelationskoeffizienten (simulierte Daten).\nObwohl es sich hierbei um einen sehr einfachen Test handelt, bringt er viele Entscheidungen mit sich. Selbst nach der Datenerhebung muss entschieden werden: Welche der befragten Personen werden für den Test verwendet? Sollen Personen ausgeschlossen werden und falls ja, warum (z.B. extreme Werte oder unplausible Werte)? Wie werden die Werte der Variablen berechnet? Welche Art der Korrelation soll verwendet werden (z.B. Bravais-Pearson, Kendall, oder Spearman)? Gibt es eine Erwartung der Richtung der Korrelation (Gerichtetheit der Hypothese)?\nDiese Fragen entsprechen Freiheitsgraden – Forschende sind also dahingehend flexibel, welche Optionen sie wählen. Keine der Optionen ist per se allen anderen überlegen und jede Entscheidung lässt sich in einem gewissen Rahmen rechtfertigen. Das Problem dieser Flexibilität ist, dass die Ergebnisse von ihr abhängen und je nach den Entscheidungen kann das Ergebnis eine positive, negative, oder keine Korrelation bedeuten. Je komplexer die Untersuchung und das statistische Verfahren ist, desto größer ist auch die Flexibilität bei der Datenanalyse. An sich sind diese Freiheitsgrade nichts Schlechtes, problematisch wird es bloß dann, wenn nur diejenigen Ergebnisse dargestellt werden, die sich gut veröffentlichen lassen oder zu den Überzeugungen der Forschenden passen. Dieses Vorgehen heißt HARKing (hypothesizing after the results are known = Hypothesen aufstellen, nachdem die Ergebnisse bekannt sind) und stellt einen Zirkelschluss dar. Die Hypothese, die geprüft wurde, stammt aus den Daten, die sich natürlich bestätigen. Verschiedene Lösungswege erlauben auch die Reduktion oder komplette Auslöschung von Freiheitsgraden (z.B. Präregistrierung, siehe Kapitel XXX). Auch ist es möglich, das Vorgehen als explorativ, also nicht vorher durchdacht und vorbestimmt, zu kommunizieren.\nIm Datenanalyseprozess wird die Analogie des „garden of forking paths” verwendet. In einem vereinfachten (!) Beispiel in Abbildung 4 haben wir 3x4x4x4 = 192 verschiedene Ergebnisse, die das gesamte Spektrum der Schlussfolgerungen abdecken werden – egal, ob unsere Hypothese stimmt oder nicht.\n\n\n\n192 verschiedene Wege von einem Rohdatensatz zum (gewünschten) Ergebnis\n\n\n[LR4] \nDemonstrationen des garden of forking paths existieren für verschiedenste Felder und wurden bereits für Evolutionsbiologie (Gould et al., 2023), Sozialpolitik (Breznau et al., 2022), Strukturgleichungsmodelle (REF[LR5] ), und Sprachanalysen (Coretta et al., 2023) überzeugend nachgewiesen.\n\n\n11.0.3 Tippfehler\n-          Statcheck und Paper dazu\n-          Fehlerhafte Zitate: https://royalsocietypublishing.org/doi/10.1098/rspa.2020.0538\n-          Fehler beim notieren der Ergebnisse: Meta-Analysen bei Interventionen der klein. Psychologie: https://osf.io/preprints/psyarxiv/gvqrn/  \n\n\n11.0.4 P-Hacking\nDer P-Wert bei statistischen Tests gibt an, wie hoch die Wahrscheinlichkeit für das beobachtete Muster ist, gegeben eines vorausgesetzten Musters. Für eine Korrelation heißt das: Wie wahrscheinlich ist es, eine Korrelation der vorgefundenen Höhe zu beobachten, wenn eigentlich kein Zusammenhang (also r = 0) zwischen den untersuchten Variablen besteht. Konkret könnte das heißen: Wie wahrscheinlich ist es, dass in meinem Datensatz von 100 Personen die Korrelation zwischen Intelligenz und Alter genau r(98) = .420 ist, wenn ich eigentlich davon ausgehen, dass beide Variablen nicht zusammenhängen. Die zusätzliche Annahme des fehlenden Zusammenhanges heißt Nullhypothese. Wenn das beobachtete Muster gegeben der Nullhypothese extrem unwahrscheinlich ist (oft unter 5%) wird von einem statistisch signifikanten Zusammenhang gesprochen. Wichtig ist dabei, dass Signifikanz (also „Bedeutsamkeit”) hier wirklich nur im statistischen Sinne zu verstehen ist. Die Frage, wie bedeutsam ein Befund für die Welt und das Leben ist, lässt sich mit Statistik in diesem Rahmen nicht beantworten. Weil P-Werte Wahrscheinlichkeiten sind, liegen sie zwischen 0 und 100%.\nUnter den QRPs (fragwürdigen Forschungspraktiken) ist p-hacking eine weitere Kategorie, die wiederum selbst verschiedene Techniken beinhaltet. Mit p-hacking ist gemeint, dass Forschende ihre Freiheitsgrade nutzen, um den P-Wert „signifikant zu machen”, also unter 5% zu bringen. Eine oft fälschlicherweise gemachte Annahme zu P-Werten ist, dass hohe P-Werte für die Abwesenheit eines Zusammenhanges sprächen, oder dass P-Werte nur dann niedrig sind, wenn tatsächlich ein Zusammenhang vorliegt. Stattdessen sind P-Werte tendenziell klein, wenn ein Zusammenhang vorliegt, der auch mit der Menge der erhobenen Daten nachgewiesen werden kann. Wenn kein Zusammenhang vorliegt, sind P-Werte gleichverteilt, das heißt, alle P-Werte kommen gleich häufig vor. Im Sinne der oben genannten Definition ist a priori klar, dass bei 100 durchgeführten Studien tendenziell 5 einen signifikanten Zusammenhang aufweisen, wenn eigentlich keiner vorliegt. Diese Tatsache erlaubt diverse P-Hacking Methoden. Simonsohn et al. (2014) zeigten, die Wahrscheinlichkeit, ein signifikantes Ergebnis zu kriegen, wenn eigentlich kein Zusammenhang in den Daten herrscht, von 5% auf ca. 60% steigen kann. Abbildung X zeigt die Verteilung von P-Werten bei verschieden hoher Teststärke (bzw. Power: der Wahrscheinlichkeit, einen Zusammenhang einer bestimmten Größe zu finden, wenn es ihn tatsächlich gibt).\nAbbildung X\nP-Werte sind bei Abwesenheit von Unterschieden oder Zusammenhängen, also beim Gelten der Nullhypothese gleichverteilt. Je höher die statistische Teststärke (Power), desto weiter verschiebt sich die Verteilung in den Bereich statistischer Signifikanz.\n{r} # P-Value distribution ---------------------------------------------------- layout(matrix(c(1,2,3), nrow = 1)) effects &lt;- c(0, .1, .3) for (i in effects) { effect &lt;- i n &lt;- 100 pvalues &lt;- (replicate(1000, t.test(rnorm(100), rnorm(100, i))$p.value)) power &lt;- round(pwr::pwr.t.test(n = n, d = i, power = NULL, alternative = “two.sided”)$power, 3) hist(pvalues, xlab = “P-values”, main = paste(“Cohen’s d =”, i, “\\nPower =”, power, sep = ““) , xlim = c(0, 1)) } layout(1)\nDie Chance, signifikante P-Werte zu kriegen, ohne, dass die getestete Hypothese überhaupt stimmt, lässt sich durch „zerschneiden” der Stichprobe machen (z.B. werden nur Frauen analysiert), durch das Erheben zusätzlicher Daten („optional stopping”), oder durch die Verwendung mehrerer zentraler Variablen (zum Beispiel wird Intelligenz mit 3 verschiedenen Tests erfasst und alle werden einzeln mit Alter korreliert). Selbst das verändern kleiner Parameter in den statistischen Tests (z.B. Verwendung einer nicht-parametrischen Spearman Korrelation statt der Bravais-Pearson Korrelation) erhöhen die Chancen auf ein signifikantes Ergebnis (siehe Tabelle Y). Einige Formen des p-hacking lassen sich zum Beispiel hier ausprobieren: https://shinyapps.org/apps/p-hacker/ (Schönbrodt, 2016).\nTabelle Y\nWahrscheinlichkeit für ein signifikantes Ergebnis durch die Anwendung verschiedener P-Hacking Techniken\n\n\n11.0.5 Selektives Berichten (Selective Reporting)\nIm Rahmen der Planung einer sozialwissenschaftlichen Studie stellt sich oft die Frage, wie ein bestimmtes Konstrukt gemessen werden soll. Für Intelligenz, politischer Ansicht, Lebenszufriedenheit, und viele andere Variable gibt es nicht den Test sondern viele Maße, die teilweise gering miteinander zusammenhängen. Gleichzeitig sind die zu testenden Theorien meist vage und diktieren nicht, mit welchem Maß ein Konstrukt gemessen werden sollte. Theorien sind den Messmethoden gegenüber also oft agnostisch. Werden in einer Studie dann verschiedene Messmethoden für ein Konstrukt gewählt, müsste die Theorie über alle Tests gleichermaßen bestätigt werden. Falls das nicht der Fall ist, sollte die Theorie angepasst werden. Entgegen dieser Empfehlung und um die Chance der Publikation der Ergebnisse zu maximieren, berichten Forschende Ergebnisse oft selektiv. Statt aller Ergebnisse werden also nur die „passendsten” oder „spannendsten” berichtet. Wie oben im Thema P-Hacking und Freiheitsgrade von Forschenden klar geworden ist, führt das dazu, dass Zusammenhänge gefunden werden, die eigentlich nicht existieren.\nWerden zum Beispiel drei verschiedene und unabhängige Maße zum Testen einer Hypothese verwendet steigt Wahrscheinlichkeit für mindestens ein signifikantes Ergebnis von 5% [LR6] auf 14%. Abbildung K zeigt,\nAbbildung K\nSelektives Berichten: Von den sechs geprüften Korrelationen ist nur eine signifikant. Alle gemeinsam sind nicht signifikant. Um die Ergebnisse zu veröffentlichen berichten Forschende nur den spannendsten Teil der Ergebnisse und verzerren damit das Bild.\nXXX\n\n\n11.0.6 Optionales Stoppen (Optional Stopping)\nFührt man bei der Durchführung einer Studie nach jeder Beobachtung den Test erneut aus und betrachtet den P-Wert, dann gibt es zwei Möglichkeiten zu dessen Verlauf: Falls ein Zusammenhang zwischen den erhobenen Variablen besteht, wird der P-Wert konvergieren, also sich einem bestimmten Wert annähern, nämlich 0. Die Wahrscheinlichkeit für das beobachtete Ergebnis wird mit größerer Stichprobe immer geringer. Dass eine Münze nur auf “Kopf” landet ist ungewöhnlicher, wenn sie das 100 Mal getan hat, als wenn sie das 3 Mal tat. Falls kein Zusammenhang vorliegt, wird der P-Wert nicht wie oft erwartet gegen 1 gehen, sondern nicht konvergieren. Er wird dann chaotisch mal hoch und mal niedrig sein – und auch öfter mal signifikant. Diese Tatsache machen sich Forschende beim optionalen Stoppen zu Nutzen: Sie erheben so lange Daten, bis ihre Hypothese bestätigt wird. Das Problem besteht übrigens nicht für Effektstärkemaße wie zum Beispiel Korrelationen. Diese konvergieren je nach Größe ab ungefähr 250 Beobachtungen (Schönbrodt & Perugini, 2013).\nAbbildung P\nKonvergenz von P-Werten und Effektstärken je nach Effektgröße: Effektstärken (hier: Korrelationskoeffizienten) konvergieren bei großen Stichproben, P-Werte konvergieren nur, wenn die Korrelation nicht 0 ist.\n{r setup, echo = FALSE} # P-Value Convergence ----------------------------------------------------- imax &lt;- 5:5000 p0 &lt;- NULL p1 &lt;- NULL p2 &lt;- NULL for (i in imax) { set.seed(42) ds0 &lt;- MASS::mvrnorm(n = i, mu = c(0,0), Sigma = matrix(c(1, 0, 0, 1), nrow = 2)) ds1 &lt;- MASS::mvrnorm(n = i, mu = c(0,0), Sigma = matrix(c(1, .05, .05, 1), nrow = 2)) ds2 &lt;- MASS::mvrnorm(n = i, mu = c(0,0), Sigma = matrix(c(1, .1, .1, 1), nrow = 2)) p0 &lt;- c(p0, cor.test(ds0[, 1], ds0[, 2])$p.value) p1 &lt;- c(p1, cor.test(ds1[, 1], ds1[, 2])$p.value) p2 &lt;- c(p2, cor.test(ds2[, 1], ds2[, 2])$p.value) } plot( y = p0, x = imax, type = “l”, col = “grey”, xlab = “Sample size”, ylab = “P-value”) lines(y = p1, x = imax, col = “orange”) lines(y = p2, x = imax, col = “red”) abline(h = .05, lty = 2) legend(“topright”, c(“r = 0”, “r = .05”, “r = .1”), col = c(“grey”, “orange”, “red”), lty = 1)\n\n\n11.0.7 Darstellung kalibrierter Modelle als geplante Modelle (Overfitting)\nKomplexe statistische Modelle haben viele Stellschrauben. Es ist möglich, die unzähligen Entscheidungen vor Anwendung eines Modells auf Daten zu treffen, für gewöhnlich werden aber andere Kalibrationen ausprobiert und eine andere als die geplante hat eine bessere Passung. Damit ist gemeint, dass beispielsweise bestimmte Variablen mit in ein Modell aufgenommen werden, um die Vorhersagekraft zu maximieren. Zu vielen Modellen gehören sogar verschiedene Algorithmen, die auf Basis festgelegter Regeln entscheiden, wie das Modell aussehen soll. Ein Modell wird also an ein Datenmuster angepasst. Wird das Vorgehen transparent offengelegt, ist das absolut in Ordnung. Problematisch wird es, wenn das beste gefundene Modell als geplantes Modell dargestellt wird. Das in den Daten vorliegende Muster beinhaltet in sozialwissenschaftlichen Untersuchungen nämlich fast immer auch ein Rauschen, also Schwankungen, die auf Messungenauigkeiten oder andere unbekannte Einflüsse zurückzuführen sind. Diese Einflüsse schwanken definitionsgemäß (in der psychologischen Testtheorie ist z.B. die Rede vom Error, einer unsystematischen Schwankung, die sich bei häufiger Messung herausmittelt). Bei zukünftigen Untersuchen wird das an die vergangenen Daten und das darin enthaltene Rauschen angepasste Modell dann notwendigerweise schlechter abschneiden, weil das Rauschen in den neuen Daten ein anderes ist. Man spricht dann von einem „überangepassten” Modell oder Overfitting.\n\n\n11.0.8 Tendenz von Menschen, sich selbst zu bestätigen (Confirmation Bias)\nEin besonderes Problem wissenschaftlicher Methoden ist der Confirmation Bias. Das Phänomen ist in der wissenschaftlichen Literatur nicht klar definiert (REF[LR7] ), hier meine ich damit die Tendenz von Menschen (oder in diesem Kontext: Forschenden), diejenigen Muster zu finden, die sie erwarten. Der Confirmation Bias basiert auf wissenschaftlichen Befunden (Nickerson, 1998; Oswald & Grosjean, 2004), und wurde von den Wissenschaftler*innen auf sie selbst übertragen (Mynatt et al., 1977; Yu et al., 2014). Diese Gedanken führen nah an logischen Unsinnigkeiten und Paradoxa entlang, selbstironisch bemerkt zum Beispiel (Nickerson, 1998) die Möglichkeit, dass alle Befunde zu Confirmation Biases selbst nur Produkte desselben sein könnten, was die Existenz des Confirmation Biases dann wieder bestätigen würde (S. 211). Praktisch besteht die Gefahr, dass Wissenschaftler*innen nicht Wahrheiten herausfinden, sondern alles so drehen, dass ihre Vorahnungen bestätigt werden. Ludwik Fleck (1935/2015) geht in seiner Wissenschaftssoziologie, die die Grundlage für Thomas Kuhns Arbeit zu wissenschaftlichen Revolutionen (1970/1996) bildet, noch ein paar Schritte weiter: Er argumentiert für ein Modell des wissenschaftlichen Fortschritts, bei dem es nicht darum geht, der Wahrheit näher zu kommen, sondern nach bestem Wissen Probleme vor dem gesellschaftlichen Hintergrund zu verstehen. Das heißt nicht, dass es keine Wahrheit gibt, nur dass Wahrheit eben nicht bloß die Übereinstimmung von Aussagen mit Tatsachen ist. Statt dieser oft von Wissenschaftler*innen vertretenen Korrespondenztheorie von Wahrheit, findet sich bei Fleck eine Konsenstheorie von Wahrheit wieder: Die Übereinstimmung vieler Leute ist wichtig. Wissenschaftliche Tatsachen werden nicht von einzelnen Personen „entdeckt”, sondern von einem Kollektiv erschaffen. Der Confirmation Bias findet sich dabei so wieder, dass dem Konsens widersprechende Befunde ausgeblendet werden und auf den aktuellen Auffassungen so lange wie möglich beharrt wird. Wenngleich philosophische Wahrheitstheorien den Rahmen dieses Buches sprengen, sei darauf hingewiesen, dass keine der drei Wahrheitstheorien (Korrespondenz, Konsens, und Kohärenz) haltbar ist (Albert, 2010; Münchhausen Trilemma).\n\n\n11.0.9 Datenfälschung\nDie bisher diskutierten Praktiken werden oft als fragwürdig (questionable) dargestellt. Manche Wissenschaftler*innen halten das für ein Euphemismus, denn in der Verantwortung als Forscher*in sollte genügend Wissen vorliegen, um zu erkennen, dass die oben beschriebenen Techniken nicht wissenschaftlich sind und eindeutig nicht der Generierung von Wissen dienen. Sie behindern deutlich den wissenschaftlichen Fortschritt, gefährden das Vertrauen in Wissenschaft, und führen zu enorm hohen Kosten. Unglücklicherweise sind diese Problematiken vielen Wissenschaftler*innen heute immer noch nicht bekannt. „Das haben wir halt so gelernt und schon immer so gemacht” heißt es zum Beispiel. Dass bestimmte Studien sich nicht replizieren ließen, war teilweise schon vielen Personen bewusst, sie hielten es nur nicht für möglich, das im Scientific Record festzuhalten (z.B. http://daniellakens.blogspot.com/2020/11/why-i-care-about-replication-studies.html). Jedenfalls legt der Begriff der fragwürdigen Forschungspraktiken nahe, dass sich Forschende damit in einer Grauzone bewegen würden. Meiner Ansicht nach, ist das nur der Fall, da, wenn Forschende ihren Job verlieren würden, weil sie P-Hacking betrieben haben, nicht mehr viele Forschende übrig wären.\nAnders ist es beim Fälschen und Manipulieren von Daten. Wie häufig Datenmanipulationen oder -fälschungen vorkommen ist ungewiss und Schätzungen sind schwierig. In einer Meta-Analyse von Umfragen zu dem Thema wurde geschätzt, dass zwischen 0,86 und 4,45% aller Wissenschaftler*innen zugaben, Daten manipuliert zu haben. 72% gaben an, fragwürdige Forschungspraktiken anzuwenden (Fanelli, 2009). Stroebe et al. (2012) stellten später Beispiele von Datenfälschung [LR8] zusammen und empfahlen Peer Review und Replikationen als Betrugs-Detektoren. Eine neuere und extrem umfangreiche Studie von Gopalakrishna et al. (2021) berichtete, dass 8,3% aller Befragten Daten manipuliert oder gefälscht hätten und 51,3% fragwürdige Forschungspraktiken angewandt hätten (Tabelle 2) und bestätigte den Ausmaß der Probleme. Je nach Disziplinen kommen weitere Probleme hinzu, wie zum Beispiel die Verwendung bereits veröffentlichter biomedizinischer Bilder, die in ungefähr 3,8% aller veröffentlichten Artikel angewandt wurde (Bik et al., 2016). Es wird davon ausgegangen, dass Datenfälschung nur in sehr seltenen Fällen aufgedeckt wird. Diejenigen Fälle, die ans Licht kamen, hatten die Zurückziehung (Retraction) der jeweiligen wissenschaftlichen Artikel zur Folge und oft Konsequenzen für die wissenschaftliche Karriere der Verantwortlichen. Retractionwatch.org verwaltet die weltweit größte Datenbank zu zurückgezogenen Artikeln (Stand Dezember 2023: 49.628 Artikel): http://retractiondatabase.org/.\nSehr düster ist dabei die Tatsache, dass Methoden zur Datenfälschung einerseits immer einfacher werden (e.g., Naddaf, 2023)[1] und Wissenschaftler*innen, die Fehler aufdecken, häufig verklagt werden. Das betrifft beispielsweise wurden die Autoren von Datacolada.org, die bereits häufiger Probleme aufgezeigt haben, von Francesca Gino für die Veröffentlichung verklagt (https://datacolada.org/109), woraufhin tausende Wissenschaftler*innen Gelder für die finanzielle Unterstützung des Gerichtsprozesses sammelten (https://www.gofundme.com/f/uhbka-support-data-coladas-legal-defense).\n[1] Hussey (https://osf.io/preprints/psyarxiv/4kht8) und Sarstedt & Adler (https://www.sciencedirect.com/science/article/abs/pii/S0148296323003004) haben sarkastisch Methoden vorgeschlagen, direkt die berichteten Werte automatisiert fälschen zu lassen.\n [LR1]https://www.thelancet.com/journals/lancet/article/PIIS0140-6736(14)60932-6/fulltext?version%3DprinterFriendly=&code=lancet-site\nhier müssten statistiken sein\n [LR2]Hier beta-fehler erklären und power\n [LR3]Wicherts liste: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5122713/\n [LR4]Abbildung passt nicht ganz zum Beispiel\n [LR5]http://dx.doi.org/10.1111/jpim.12738\n [LR6]binom.test(1, 3, .05)\n [LR7]abschlussarbeit confirmation bias samira nickel\n [LR8]Podcast zu dem Thema:\nhttps://freakonomics.com/podcast/can-academic-fraud-be-stopped/",
    "crumbs": [
      "Probleme",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Probleme der wissenschaftlichen Methoden</span>"
    ]
  },
  {
    "objectID": "probleme_theorien.html",
    "href": "probleme_theorien.html",
    "title": "12  Theorien",
    "section": "",
    "text": "Wissenschaft [LR1] arbeitet mit Theorien. Wie diese genau aussehen, unterscheidet sich zwischen Disziplinen massiv. Während naturwissenschaftliche Bereiche häufig mit mathematischen Modellen, also Formeln, arbeiten, die den Zusammenhang zwischen Variablen explizit und unmissverständlich beschreiben und Vorhersagen erlauben, arbeiten Sozialwissenschaften häufig mit verbalen Theorien im Stile von „X und Y hängen positiv miteinander zusammen” oder „je höher X, desto höher Y” und traditionelle Geisteswissenschaften arbeiten beispielsweise mit verbalen Erklärungen. Verbale Theorien haben den Vorteil, dass sie tendenziell leicht verständlich und allgemein anwendbar sind, allerdings unterliegen die verwendeten Begriffe häufig individuellen, kulturellen, oder zeitlichen Einflüssen und Diskutant*innen droht, im wissenschaftlichen Diskurs aneinander vorbei zu reden. Für formale Theorien werden alle beteiligten Variablen genau definiert und die Theorien haben häufig einen stark eingeschränkten Geltungsbereich (z.B. gelten viele physikalische Gesetze nur unter streng kontrollierten Bedingungen wie im Vakuum, bei einer bestimmten Temperatur, usw.). Die Sorge im Rahmen der Replikationskrise ist, dass Theorien nicht klar genug sind, um vorherzusagen, wann Replikationen erfolgreich sind und damit eine der Ursachen für geringe Replikationsraten sind (z.B. REF[LR2] ). Eine Theorie über die Konsequenzen von der Identifikation mit Geschlechterrollen muss beispielsweise die Veränderung von Geschlechterrollen und Besonderheiten von Geschlechterrollen in verschiedenen Ländern berücksichtigen. Dass ein und dasselbe Experiment zu diesem Thema in den USA im Jahre 1980 andere Ergebnisse hat als in Deutschland im Jahr 2020 ist wenig überraschend. Problematisch ist allerdings, dass – auch wenn solche Ergänzungen für viele sozialwissenschaftliche Theorien sinnvoll und nötig erscheinen – nur selten Aussagen darüber gemacht werden.\nVerbale Theorien sind per se nicht weniger wissenschaftlich. Im Kontext der jeweiligen Bereiche heben sich wissenschaftliche Theorien stets durch ihren besonders hohen Grad an Systematizität (Hoyningen-Huene, 2013) von alltagswissenschaftlichen Erklärungen ab. Bereiche, die Wert auf Vorhersage von Geschehnissen legen, kommen jedoch nicht ohne formale Theorien aus (Muthukrishna & Henrich, 2019). Dabei sei hervorgehoben, dass bestimmte Wissenschaften eben keinen Wert auf Vorhersage legen (z.B. Geschichtswissenschaften oder Disziplinen, die vorwiegend hermeneutisch vorgehen). Sozialwissenschaften wie die Psychologie, quantitative Soziologie, oder Teile der Geisteswissenschaften („Digital Humanities”) nähern sich aktuell formalen Modellen an – in der Sozialpsychologie gab es den Aufruf, Theorien zu formalisieren beispielsweise schon einmal bei einer Krise in den 1970er Jahren (Lakens, 2023). Dadurch, dass sich Theorien durch ihren Mangel an Objektivität selten von verschiedenen Forschenden verwendet werden und sich durch ihre flexible Auslegung nur schwer wiederlegen lassen ist dort eine enorm große Menge an nutzlosen Theorien entstanden (Ferguson & Heene, 2012). Darunter sind auch einander widersprechende Theorien: Beispielsweise argumentieren Banker et al. (2017), dass „ego depletion”, also die Erschöpfung von Selbstkontrollressourcen, dazu führt, dass Personen sich eher an Hinweise anderer Leute orientieren (S. 2) während Francis et al. (2018) gegenteilig vermuten, dass die Erschöpfung verhindert, dass Hinweise überhaupt verarbeitet werden können. Beide lieferten Daten, die die jeweiligen Theorien bestätigten, jedoch fand eine Folgeuntersuchung, dass vermutlich beide falsch lagen (Röseler et al., 2020).\nRobinaugh et al. (2021) diskutieren Beispiele der Umwandlung verbaler Theorien in formale. Dieser Prozess hat zur Folge, dass sich neue und spezifischere Vorhersagen ableiten lassen. Wenn eine Theorie genauere Vorhersagen macht und die Menge an möglichen Ereignissen, die der Theorie widersprechen, steigt, bedeutet das einen gestiegenen empirischen Gehalt (siehe Tabelle Z; Glöckner & Betsch, 2011; Popper, 1959/2008).\nTABELLE Z\nAussagen über den Zusammenhang von X und Y mit verschiedenen Graden an empirischem Gehalt[LR3] \nXXX\n\n12.0.1 Deduktion und [LR4] Induktion\nMethoden werden reformiert und Wissenschaftler*innen diskutieren, wie Wissenschaft funktioniert, ablaufen sollte, und welche Methoden sinnvoll und unsinnig sind. Wie am hermeneutischen Zirkel klar wird, führt ein Erkenntnisweg darüber, eine Menge von Beobachtungen zu einer Regelmäßigkeit oder Gesetzmäßigkeit zusammenzufassen (Induktion) und ein weiterer besteht daraus, aus einer Gesetzmäßigkeit bzw. Theorie Vorhersagen über noch nicht angestellte Beobachtungen zu machen (Deduktion). Immer wieder wird diese Unterscheidung im wissenschaftlichen Diskurs vernachlässigt oder ausgeblendet. Beispielsweise drehte sich ein Dialog in der Konsumentenpsychologie jahrelang darum, welcher Weg besser sei, obwohl beide Wege gleichermaßen legitim sind und einander ergänzen (e.g., Calder et al., 1981). Ähnlich verhält es sich bei Konflikten zwischen qualitativer und quantitativer Vorgehensweise, die formal betrachtet jeweils eher induktiv oder deduktiv vorgehen (Borgstede & Scholz, 2021). Bei Replikationsforschung hat traditionell die induktive Seite mehr Beachtung erfahren (Hüffmeier et al., 2016): Jeder Unterschied zwischen Replikation- und Originalstudie wird als mögliche Ursache für ein Scheitern des Replikationsversuches herangezogen um die Vertrauenswürdigkeit der Originalbefunde aufrechtzuerhalten (Baumeister & Vohs, 2016). Dabei gerät außer Acht, dass kleinere Unterschiede zwischen Original- und Replikationsstudie (z.B. Verwendung der Maße, durchschnittliches Alter der Versuchspersonen, Sprache der Instruktion) von Theorien nicht erfasst werden – ihnen zufolge also unerheblich sein sollten – und eine fehlgeschlagene Replikation klar die Grenzen der Theorie aufzeigt und sich aus ihr Empfehlungen für die Modifikation von Theorien ableiten lassen (Cesario, 2014; Dijksterhuis, 2014). Ein Überblick über die Vorgehensweisen ist in Tabelle X.\nTabelle X\nMerkmale induktiver u[LR5] nd deduktiver Vorgehensweise\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFacette\n\nDeduktives Vorgehen (Theorie-geleitet)\n\nInduktives Vorgehen (Phänomen-geleitet)\n\n\n\n\n\n\n\n\n\n\n\n\n\nVerallgemeinerbarkeit steckt in…\n\nder Theorie: Sie ist a priori maximal allgemein (z.B. gilt sie, bis anderweitig nachgewiesen, für alle Menschen).\n\nden Daten: Erst vielfältige Beobachtungen in verschiedenen Kontexten erlauben die Annahme, dass das Phänomen allgemeingültig ist.\n\n\n\n\n\n\n\n\n\n\n\n\n\nVeränderung von Verallgemeinerbarkeit\n\nMit mehr Beobachtungen sinkt die Allgemeingültigkeit.\n\nMit mehr Beobachtungen steigt die Allgemeingültigkeit (sofern sie bestätigender Natur sind).\n\n\n\n\n\n\n\n\n\n\n\n\n\nArt der Prüfung\n\nVorhersagen der Theorie werden vorwiegend Versuchen der Widerlegung unterzogen.\n\nWiederholte Beobachtungen bestätigen den ursprünglichen Einzelfall.\n\n\n\n\n\n\n\n\n\n\n\n\n\nWahl des Studiensettings\n\nStudentische Stichproben aus nur einem Land oder Laboruntersuchungen sind unbedenklich.\n\nDer Kontext der Untersuchung sollte die Zielbedingungen (z.B. bei der Anwendung der Erkenntnisse in der Praxis) möglichst gut widerspiegeln.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n12.0.2 Hilfshypothesen\nÜber folgende Wege lassen sich Replikationsfehlschläge erklären:\n1.      Fehler erster Art der Originalstudie: Der Originalbefund war nur ein Zufallsbefund oder kam durch wissenschaftliches Fehlverhalten zustande (siehe Kapitel „Freiheitsgrade von Forschenden (Researchers’ Degrees of Freedom)“).\n2.      Fehler erster Art der Replikationsstudie: Die Originalstudie lag richtig, die Replikationsstudie hat einen Fehler gemacht (z.B. zu kleine Stichprobe, schlechte Kalibrierung der Instrumente, oder wissenschaftliches Fehlverhalten).\n3.      Grenzbereich des Phänomens: Beide Studien sind vertrauenswürdig. Die Replikationsstudie unterscheidet sich auf eine für die Theorie wichtige Weise (z.B. wurde die Replikationsstudie mit Personen aus einem anderen Land durchgeführt und die Theorie gilt nur für Menschen aus dem „Original-Land”). Siehe hierzu auch Fanelli (REF[LR6] ).\nVariante 3 ist konstruktiv und nimmt beide Einzelbefunde für robust hin. Notwendig dafür ist ein theoretisch relevanter Unterschied zwischen der Original- und Replikationsstudie, der durch die unendliche Anzahl möglicher wichtiger Faktoren in den meisten Fällen zutrifft (Smedslund, 2015). Über diesen Weg lässt sich die Theorie dann modifizieren oder eine weitere Theorie aufstellen, die für den Kontext der Untersuchung ebenfalls berücksichtigt werden muss.\n [LR1]einarbeiten: https://link.springer.com/article/10.1007/s43638-023-00081-3\n•       https://doi.org/10.31234/osf.io/jqw35\n•       Smaldino, P. (2019). Better methods can’t make up for mediocre theory. Nature, 575(7781), 9. https://doi.org/10.1038/d41586-019-03350-5\n•        \n [LR2]https://journal.trialanderror.org/pub/tension-between-theory/release/1\n [LR3]Siehe ppt folien von meinem seminar\n [LR4]Validitätsarten in Call for Replications in Language Teaching: https://www.sciencedirect.com/science/article/pii/S2772766123000514\nViel external und internal validity\n [LR5]Aus preprint mit Johannes genommen, evtl anpassen oder zitieren\n [LR6]12:15 https://www.youtube.com/watch?v=CEAV7420jBk",
    "crumbs": [
      "Probleme",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Theorien</span>"
    ]
  },
  {
    "objectID": "probleme_epistemische.html",
    "href": "probleme_epistemische.html",
    "title": "13  Epistemische Probleme",
    "section": "",
    "text": "Möglichkeiten der Epistemologie, also der Erkenntnislehre, Replikationsprobleme zu erklären, übersteigen systemische und methodische Faktoren, haben aber auch andere Prüfbarkeitsansprüche und sind für tätige Wissenschaftler*innen wahrscheinlich eher unplausibel.\n\n13.0.1 Komplexität von Studien\nhttps://osf.io/preprints/metaarxiv/5r36g siehe auch Fanellie talk https://www.youtube.com/watch?v=CEAV7420jBk und evtl. gibt es inzwischen prereg Test von ihm\n\n\n13.0.2 Robustheit und Historizität von [LR1] Phänomenen[LR2] \nUnter welchen Voraussetzungen ist es wenig überraschend, dass Replikationsversuche fehlschlagen? Ein Ausweg ist anzunehmen, dass die untersuchten Phänomene extrem empfindlich oder unstabil seien. Regelmäßigkeiten im menschlichen Verhalten analog zu den Planetenbewegungen zu entdecken könnte schlichtweg nicht möglich sein (Smedslund, 2015). Weniger extreme Annahmen über die Existenz von Regelmäßigkeiten, die möglicherweise nicht jede Person ausnahmslos betreffen aber „im Schnitt” gelten (also aristotelische statt galileische Gesetzmäßigkeiten; Lewin, 1930) sind allerdings unumstritten. Eine solche Regelmäßigkeit kann zum Beispiel sein, dass Männer größer als Frauen sind. Noch extremer ist die Theorie, dass Menschen sich des Wissens über sie bewusst sind und ihr Verhalten dynamisch anpassen und Verhaltenswissenschaften immer historisch bzw. zeitgebunden sind: Wird herausgefunden, dass Menschen in ihren Entscheidungen tendenziell dazu neigen nichts zu ändern, auch wenn sich dadurch ihre Situation verbessern würde, wird ihnen diese Tatsache über die Wissenschaft vor Augen geführt und sie können ihr Verhalten anpassen. Dabei handelt es sich übrigens um den Status Quo Bias, welcher im Rahmen von über 30 Jahren erfolgreich repliziert wurde (Samuelson & Zeckhauser, 1988; Xiao et al., 2021).\nWie stark sich Phänomene durch vermeintlich kleinere Unterschiede im Versuchsaufbau unterscheiden wurde bereits meta-wissenschaftlich untersucht. Landy et al. (2020) ließen mehrere Hypothesen von mehreren Forschenden prüfen und Faktoren, die laut den dahinterliegenden Theorien eigentlich keinen Unterschied machen sollten, führten dazu, dass Gegenteilige Ergebnisse entstanden. Auf Replikationsforschung übertragen ist es also möglich, dass in bestimmten Forschungsbereichen völlig unklar ist, unter welchen Bedingungen welche Zusammenhänge zu beobachten sind.\n\n\n13.0.3 Paradox des Fallibilität und des Fortschritts\n-          https://www.pnas.org/doi/full/10.1073/pnas.1711786114 [einarbeiten]\n [LR1]Error is an integral part of the process of science\nhttps://www.pnas.org/doi/full/10.1073/pnas.1711786114\n [LR2]in BA Literaturverzeichnis schauen why psych cannot be a science",
    "crumbs": [
      "Probleme",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Epistemische Probleme</span>"
    ]
  },
  {
    "objectID": "lösungen_system.html",
    "href": "lösungen_system.html",
    "title": "14  Das System",
    "section": "",
    "text": "Fangen wir mit dem System an. Ich selbst verspreche mir von diesen Ansätzen am meisten, denn solange Publikationen die Währung sind und Paper mit knackigen Titeln und eindeutigen Ergebnissen als qualitativ hochwertiger befunden werden, sind Forschende darin motiviert, statt der Wahrheit eben nach knackigen Titeln und eindeutigen Ergebnissen zu suchen.\nAllgemein ist eine positive Entwicklung sichtbar (REF https://www.nature.com/articles/s44271-023-00003-2 ) und eine Veränderung der Anreizstruktur wird anvisiert. Sie lässt sich als Angleichung des wissenschaftlichen Systems an die Mertonschen Normen (nach Robert Merton) auffassen[LR1] : (1) Kommunismus: Das wissenschaftliche Wissen sollte allen Wissenschaftler*innen gleichermaßen gehören, um die Zusammenarbeit zu fördern. (2) Universalismus: Wissenschaftliche Güte ist unabhängig vom soziopolitischen Status und persönlichen Attributen der Teilhabenden. (3) Desinteresse: Wissenschaftliche Institutionen handeln im Interesse der Wissenschaft und nicht für persönlichen Gewinn. (4) Organisierter Skeptizismus: Wissenschaftliche Behauptungen sollten einer kritischen Prüfung unterzogen werden bevor sie akzeptiert werden.\nNosek (REF, 2019 make it possible, siehe Abbildung X) empfiehlt eine Maßnahmenstruktur, nach welcher die gewünschten Veränderung nacheinander …\n1.      möglich\n(z.B. durch Infrastruktur wie online Repositorien, in denen Forschungsmaterialien öffentlich und gratis hochgeladen werden können),\n2.      einfach\n(z.B. durch barrierearme Angebote, mehrsprachige Anleitungen),\n3.      normativ\n(z.B. durch Wissenschaftliche Communities, die gemeinsam hinter Forderungen der Verbesserung stehen),\n4.      belohnend\n(z.B. durch designierte Preise), und\n5.      notwendig\n(z.B. durch Mindeststandards, die von Zeitschriften oder Drittmittelgebern gefordert werden)\ngemacht werden sollen. Wie die verschiedenen Ansätze bei den verschiedenen Akteuren, also Politik, Universitäten, oder Zeitschriften konkret aussehen, wird im Folgenden diskutiert.\nAbbildung X\nKulturwandel in der Wissenschaft nach Nosek (2019, Abbildung N, REF)\nPYRAMIDE HIER EINFÜGEN AUF DEUTSCH XXX\n\n14.0.1 Infrastruktur (Open Infrastructure)\n-          ermöglichend\n-          NFDI\n-          Prinzipien: https://openscholarlyinfrastructure.org (übersetzen, einfügen, zitieren)\n-          Beispiele\no   Wege, um Materialien, Daten, oder Ergebnisse\n§  Literaturdatenbanken (OpenAlex)\n§  (Data) Repositories, re3data.org; OSF.io\n§  Pre-Print Server (arXiv), Zeitschriften, Zeitschriftensysteme (OJS)\n§  Begutachtungssysteme (PCI)\n§  Post-Publication Review\no   Meta-Daten, um Verweise zu ermöglichen (ORCID-ID, ROR https://ror.org, DOI)\n§  ORCID-ID Kritik: https://www.sciencedirect.com/science/article/pii/S0099133324000144?dgcid=rss_sd_all\n-          Übersicht: https://kumu.io/access2perspectives/open-science#disciplines/by-os-principle/open-infrastructure?focus=1 von https://access2perspectives.org/mapping-open-science-resources/\n\n\n14.0.2 Politik\nInternational stehen politische Parteien und Vereinigungen deutlich hinter Open Science und Open Access. Beispielsweise empfiehlt die UNESCO einen universellen Zugang zu wissenschaftlichen Wissen ungeachtet von Herkunftsland, Geschlechterrolle, politischen Grenzen, Ethnizität, oder ökonomischen oder technologischen Hürden (REF https://unesdoc.unesco.org/ark:/48223/pf0000374837, p. 3). Arbeitsgruppen für politische Instrumente, Förderung, und Infrastruktur wurden entsprechend gegründet (REF[LR2] ). Die G7 setzen sich für wissenschaftliche Integrität, akademische Freiheit, und Open Science ein (REF[LR3] ). Offener Zugang (REF[LR4] ) aber auch Transparenz des wissenschaftlichen Vorgehens (REF[LR5] ) wird auch seitens der Europäischen Union gefordert. Infrastruktur (z.B. European Open Science Cloud, https://eosc-portal.eu) und diverse Open Science Forschungsprojekte werden gefördert.\nIn Deutschland hat sich die Regierung des Zyklus 2021-2025 im Rahmen des Koalitionsvertrages vorgenommen, „Open Access … als gemeinsamen Standard [zu] etablieren.” Einzelne Bundesländer wie Nordrhein-Westfalen haben in Zusammenschlüssen aus den jeweiligen Universitäten darüber hinaus Open Access Strategien entwickelt (REF[LR6] ) und arbeiten aktuell an Open Science Strategien. Andere Länder, wie zum Beispiel Schweden, haben bereits nationale Richtlinien zu Open Science entwickelt (https://www.kb.se/samverkan-och-utveckling/nytt-fran-kb/nyheter-samverkan-och-utveckling/2024-01-15-national-guidelines-for-promoting-open-science-in-sweden.html). Hinsichtlich der Problematik von Machtmissbrauch wird das Problem beispielsweise in einem Eckpunkte-Papier des Landes NRW anerkannt, doch als Einzelfall- statt System-Problem verstanden (REF[LR7] , für einen Kommentar siehe z.B. REF[LR8] ).\n\n\n14.0.3 Universitäten[LR9] \nDas Thema Open Science hat bei vielen Universitäten bereits Anklang gefunden. Während die meisten deutschen Universitäten Mittel für Open Access Publikationen haben, existieren an einigen darüber hinaus Open Science Policys (z.B. FAU Erlangen, https://oa-info.sh/2022/01/open-science-policy-der-uni-erlangen-nuernberg/), Open Science Centers (z.B. LMU Open Science Center, https://osf.io/smqpn; Köln Open Science Center; Münster Center for Open Science; Mannheim Open Science Office). Darüber hinaus unterstützen das Leibniz-Informationszentrum Wirtschaft in Kiel und das Leibniz-Institut für Psychologie Replikationsforschung beispielsweise mit einer Replikationszeitschrift (https://www.jcr-econ.org) oder im Rahmen einer Juniorprofessur für Psychologische Metawissenschaft. Eines der Metawissenschaftlichen Zentren innerhalb Europas hat sich in den Niederlanden in Tilburg gebildet. Die Berliner Universitäten haben eine gemeinsame Open Access Erklärung entwickelt (https://openaccess.mpg.de/Berliner-Erklaerung[LR10] ) und in Frankreich ist die Universität Sorbonne ein Pionier: Gemeinsam mit der Universität Amsterdam und dem Universitätscollege London wurde eine Erklärung über die die Veröffentlichung von Forschungsdaten unterzeichnet. XXX[LR11] . Seit 2024 hat die Universität Sorbonne darüber hinaus den Vertrag mit Clarivate für die Nutzung der Forschungsdatenbank „Web of Science” gekündigt und arbeitet seitdem mit der Open Source Software „OpenAlex” (REF[LR12] ).\nFür die langfristige Entwicklung der Wissenschaften haben Universitäten dadurch eine besondere Verantwortung, dass sie Wissenschaftler*innen beschäftigen und an ihnen die Auswahl für die wenigen unbefristeten Arbeitsplätze in der Wissenschaft fallen. Wenn jahrzehntelang Professuren auf Basis subjektiver, nicht-reproduzierbarer, und für gute Wissenschaft nachrangigen Kriterien gewählt werden (z.B. Anzahl an Publikationen in Fachzeitschriften), kann sich das negativ auf die Entwicklung von Wissenschaften auswirken. Ein Forschungspreis des Berlin Institute of Health (BIH), der jährlich für Projekte zur Förderung von wissenschaftlicher Integrität verliehen wird, ging 2023 an ein Projekt, das objektive und sinnvolle Auswahlkriterien für Professor*innen entwickelt (REF, REF[LR13] ). Innerhalb von Universitäten spielen außerdem die Bibliotheken eine aufklärerische Rolle hinsichtlich Forschungsdatenmanagement und Publikationskultur (REF, https://liberquarterly.eu/article/view/14947). Über sie kann der Forschungsprozess mit entsprechender Infrastruktur (z.B. zum Lagern und Veröffentlichen von Forschungsmaterialien und -ergebnissen) unterstützt werden (z.B. REF[LR14] ) und verhindert werden, dass sich eine „Abhängigkeit von wenigen kommerziellen Anbietern” ergibt, die „begrenzen, was [be] der Forschung an Arbeitsmöglichkeiten und Fragestellungen erreichbar ist” (REF[LR15] ). Eine Pflicht, Mitglieder eine Universität zur Einhaltung von Open Science Strategien zu bewegen, haben Universitäten jedoch nur eingeschränkt durch den hohen Stellenwert der „Freiheit der Forschung” sowie der Gefahr, für Forschende weniger attraktiv zu werden, wenn beispielsweise auf namhafte Zeitschriften nicht mehr über die Universität zugegriffen werden kann, weil Verträge mit closed-access Zeitschriften gekündigt wurden. Beispielsweise klagte die juristische Fakultät der Universität Konstanz gegen eine Zweitveröffentlichungspflicht (REF[LR16] ), nach welcher Forschende von ihrem Zweitveröffentlichungsrecht Gebrauch machen müssen oder es werden Zeitschriften, deren wissenschaftliche Qualität angezweifelt wird, dennoch mit Mitteln zur Veröffentlichung bezuschusst (https://www.suub.uni-bremen.de/ueber-uns/neues-aus-der-suub/unter-kritischer-beobachtung-open-access-publikationen-im-mdpi-verlag).\nEine oft vernachlässigte Rolle kommt außerdem der universitären Lehre hinzu. Durch die Freiheit von Forschung und Lehre und bereits durchgeplanten Studiengängen gestaltet sich die Integration von Open Science in die Lehre schwierig. Forschende, in deren Lehre die Thematik eine Rolle spielt, teilen proaktiv ihre Materialien, erstellen gemeinsam Curricula, und sind beispielsweise in großen internationalen Initiativen wie dem Framework for Open and Reproducible Research Training (FORRT.org) vernetzt (konkrete Vorschläge zur Integration von Open Science in die Lehre siehe z.B. REF[LR17] ).\n\n\n14.0.4 Institute und Vereinigungen\nWissenschaftliche Gebiete leben vor allem durch Communities, also alle in dem Bereich forschenden Personen. Sie organisieren sich üblicherweise in Vereinen (z.B. Deutsche Gesellschaft für Psychologie), Interessensverbunden, oder ähnlichen Gemeinschaften. Eine besondere Stellung hat in Deutschland die Deutsche Forschungsgemeinschaft, welche staatlich und über die Bundesländer mit mehreren Milliarden Euro ausgestattet Forschungsgelder vergibt. Als eine der wichtigsten nationalen Institution hat ihre Open Science Positionierung einen hohen Stellenwert (REF[LR18] ), erfahrungsgemäß gehen Veränderungen jedoch nicht von der DFG aus, sondern die DFG wartet auf Anstöße aus den Fächern. DGPs[LR19] . In der Psychologie fördert darüber hinaus das ZPID die Infrastruktur durch Zeitschriften, Pre-Print Server, und weitere Methoden (REF[LR20] ). Auch interdisziplinäre Vereinigungen wie das CERN (REF[LR21] ) oder internationale Akteure wie die American Psychological Association (REF[LR22] ) verpflichten sich Offenheit und Transparenz. Für Bürger*innen der Europäischen Union existieren die European Open Science Cloud und die Open Access Publishing Plattform Open Research Europe.\nIm Rahmen der Open Science Reform entstanden außerdem viele neue Vereinigungen. Das interdisziplinäre und besonders von Wissenschaftler*innen in der frühen Karrierephase geleitete FORRT (REF[LR23] ) setzt sich für eine Verankerung von Open Science in der Lehre ein. Der Verbesserung psychologischer Forschung hat sich die Society for the Improvement of Psychological Science (SIPS) verschrieben. Sogenannte „grassroot”-Initiativen (also von jungen Wissenschaftler*innen ausgehende Bewegungen) haben sich an zahlreichen Universitäten herausgebildet und zu Netzwerken wie dem Netzwerk der Open Science Initiativen (NOSI) und „Reproducibility Networks” wie dem GRN (https://reproducibilitynetwork.de), dem UKRN (https://www.ukrn.org) und weiteren zusammengeschlossen. Aber auch Zusammenschlüsse von Professor*innen zur Änderung von Kurzzeitverträgen existieren (z.B. Netzwerk Nachhaltige Wissenschaft, https://netzwerk-nachhaltige-wissenschaft.de).\n\n\n14.0.5 Zeitschriften\nWissenschaftliche Zeitschriften gelten als Bühne des wissenschaftlichen Diskurses und bestimmen maßgeblich, welche Elemente des Forschungsprozesses zum „scientific record” gehören und damit relevant sind. Sie sind darüber hinaus als Organisator des Begutachtungsprozesses für die Qualitätssicherung in der Wissenschaft verantwortlich. Dem Mangel an Qualität entgegnend entstehen im Rahmen von Open Science Empfehlungen zur Gestaltung von Zeitschriften, es bilden sich neue Zeitschriften, und vollständige neue Begutachtungs- und Publikationsmodelle werden vorgeschlagen und vielseitig implementiert. Das Journal of Open Source Software (REF[LR24] ) basiert beispielsweise auf Github (XXX[LR25] ) und seine Infrastruktur lässt sich für weitere Zeitschriften kopieren und anpassen.\n\n14.0.5.1 Empfehlungen\nHerausgeber*innen, die sich in Bezug auf die von ihnen verwaltete Zeitschrift mit Open Science Praktiken auseinandersetzen möchten, können inzwischen auf einen umfangreichen Leitfaden zurückgreifen (REF[LR26] ). Über eine Diskussionsplattform (Journal Editors Discussion Interface, JEDI) wurden Vorschläge gesammelt und es wir erklärt, worum es sich bei Dingen wie Registered Reports, Open Peer Review, Diversifizierung, und Open Access handelt und wie diese in eine Zeitschrift implementiert werden können. Eventuelle Sorgen und Ängste werden angesprochen und beantwortet. Das Committee on Publication Ethics (COPE) setzt sich ebenfalls für Aufklärung und Lehre ein, die Herausgeber, Universitäten, und Forschungsinstitute im Umgang mit Problemen im Publikationssystem helfen soll. Es bietet beispielsweise Richtlinien unter welchen Umständen Publikationen zurückgezogen oder korrigiert werden sollten (https://publicationethics.org/retraction-guidelines), oder welche ethischen Standards ein Begutachtungsprozess erfüllen sollte (https://publicationethics.org/resources/guidelines/cope-ethical-guidelines-peer-reviewers). Herausgeber*innen, die Zeitschriften für kommerzielle Verlage verwalten und auf Systeme umsteigen möchten, die vollständig in der Hand der Forschenden liegen, können über Universitätsbibliotheken Hilfe bei der Migration von den kommerziellen zu offenen und kostenfreien Systemen erhalten und Zeitschriften beispielsweise mit dem Open Journal System verwalten (siehe z.B. OJS Netzwerk, https://ojs-de.net/start). Eine Datenbank mit bereits über 20,000 offen zugänglichen Zeitschriften verwaltet das Directory of Open Access Journals (DOAJ, https://doaj.org). Gutachter*innen von Forschungsartikeln können über die Reviewer Zero Initiative (https://www.reviewerzero.net) auf Lehrmaterialien und Leitfäden zugreifen (https://osf.io/e7z5k/wiki/Resources/).  \n\n\n14.0.5.2 Open Science Praktiken hervorheben\n-          Badges\n-          TOPfactor.org\n-          Badges können ge-“game”t werden: https://journals.sagepub.com/doi/10.1177/10731911241253430\n\n\n14.0.5.3 Review Systeme\n-          „criteria related to consensus-building do not yet espouse sufficient reliability” https://www.researchgate.net/publication/380433173_Inter-Rater_Reliability_in_Assessing_the_Methodological_Quality_of_Research_Papers_in_Psychology aber es gibt Kriterienkataloge, die gut funktionieren\n-          PCI\n-          F100research.com: Mischung aus Zeitschrift und Pre-Print Server; Artikel ist sofort öffentlich verfügbar ab Einreichung und es steht dann dort, dass er under review ist\n-          Review-Ampel\n-          Open Peer Review\n-          Crowd peer review\no   Wenig Beteiligung an offenem Review, Zeitschrift Synlett und ASAPbio: Koordinierung von Gruppe im Sinne eines Journal Clubs (=preprint review club), https://asapbio.org/crowd-preprint-review\n-          Review von negativen Zeitschriften: Infos via SIPS 2024 (Zoltan Kekecs SIPS Unconference, sehr spannende Ideen auch von Madhwa Galgali, Ekaterina Pronizius, Willemijn Plomp, Belay Weldemicheal)\no   Aktuell „Open peer review” betrifft nur akzeptierte Papers; Meta-Wissenschaft über Peer Review wäre spannend\no   Doppelarbeit und enorme Kosten https://link.springer.com/article/10.1186/s41073-021-00118-2\no   Daten im Review mit anschauen und Zombie Papers: https://associationofanaesthetists-publications.onlinelibrary.wiley.com/doi/full/10.1111/anae.15263\n§  Falsifizierte Daten wurden in Artikeln identifiziert, abgelehnt, aber dann woanders veröffentlict [LR27] hat;\n§  „losing epistemic arms race”, „we are arming the opposite site” à feedback für abgelehnte Paper hilft Datenfälscher*innen\n§  institutionelles Gedächtnis nötig; editorial notes / decisions sollten „für immer” an dem Artikel dranheften; dafür Versionierungssystem nötig, Namen von Beteiligten, Zeitschriften, usw.; Antwort auf Rejection sollte möglich sein\no   Implementierung: automatisiertes System über editorial management Systeme; Möglichkeit für Embargo\no   Mögliche Nachteile\n§  Paper mit 5 Rejections sieht nicht so hübsch aus; evtl. akzeptieren manche Journals nichts, was bei „schlechten” Journals rejected wurde „death spiral”, Ankereffekte zwischen Reviews; evtl. nur Editors erlauben, alte Reviews zu sehen?\n§  Reviewers orientieren sich evtl. aneinander\n§  Nachteile für Reviewer (ich wurde negativ bewertet und nun stellt mich die Person nicht ein)\no   Implementierung\n§  Soziales Netzwerk für Reviews mit Ratings dafür wie hilfreich usw. Reviews sind\n§  Pubpeer mit Anforderung, Papers als Pre-Print zu posten (keine Versionierung und nur mit Pre-Prints möglich)\n§  Personality Science: Einreichung mit bisherigen Reviews\n§  Über PCI (dort werden Reviews aktuell nicht veröffentlicht, wenn es negativ ausfällt)\n-          Post Publication Peer Review\no   Pubpeer.com\no   Hypothes.is (?)\no   Disqus,\no   aphaxiv.org\no   https://scirev.org\n\n\n14.0.5.4 Aufmerksamkeit zum Thema in bestehenden Zeitschriften\nAnforderungen an wissenschaftliche Artikel seitens der Zeitschriften sind 2010 maßgeblichen Änderungen untergangen. In den Sozialwissenschaften orientieren sich zahlreiche Zeitschriften an den Richtlinien zur „Transparency and Openness Promotion” (TOP) und erhalten entsprechende TOP-Faktoren (https://topfactor.org/summary). Dabei wird festgehalten, welcher Grad an Offenheit und Transparenz von Forschungsdaten und -materialien gefordert wird und ob Replikationen bei der jeweiligen Zeitschrift veröffentlicht werden. Neue Herausgeber*innen bei existierenden Zeitschriften haben große Änderungen vorgenommen (z.B. Editorial psych bulletin, REF[LR28] ). Beispielsweise haben Hardwicke und Vazire (REF[LR29] ) für die Zeitschrift Psychological Science ein standardmäßiges Nachrechnen aller berichteten Ergebnisse ab 2024 angekündigt (REF), indem sie mit dem Institute for Replication zusammenarbeiten (https://i4replication.org). Vereinzelt haben Zeitschriften Spezialausgaben herausgegeben, bei denen der Fokus auf Replikationsstudien oder der Reproduzierbarkeit von Ergebnissen lag (https://imstat.org/publications/sts/sts_38_4/sts_38_4.pdf).\n\n\n14.0.5.5 Zeitschriften für “nicht Innovatives”\nDurch die Selektion spannender Ergebnisse gibt es für Forschung, die nicht bahnbrechend und dennoch höchst relevant ist, keine Plattform. Schätzungen zufolge werden bis zu 40% aller durchgeführten Studien innerhalb von 4 Jahren nach Durchführung nicht veröffentlicht (REF[LR30] ). Andere Forschende können nicht davon lernen und Ressourcen, die in die Sammlung und Auswertung der Daten geflossen sind, Zeit von Versuchspersonen, und lange Vorbereitungen der Forschung werden schließlich verschwendet. Zur Lösung dieses Problems haben sich neue Zeitschriften und Formate gebildet. In der Ökonomie hat sich auf Forderungen (REF[LR31] ) hin beispielsweise eine Zeitschrift für Replikationen und Kommentare gebildet (JCRE[LR32] ), die Zeitschrift Meta-Psychology bietet das Format des „Schubladenbericht” an. Dieses Format ist für Studien vorgesehen, die wegen wenig überraschenden Ergebnissen oder Fehlern in der Durchführung anderweitig in der Schublade landen würden. Ebenfalls zur Abbildung des für den Forschungsprozess typischen Fehlschlagens wurde das Journal of Trial and Error (LINK) gegründet, und bei ReScience C werden Berichte über Reproduzierbarkeit veröffentlicht (LINK).\n\n\n14.0.5.6 Publikationsmodelle\nNoch radikalere Vorschläge als die Anpassung bisheriger Zeitschriften ist der Vorschlag, das bisherige System durch ein neues zu ersetzen. Dabei handelt es sich um ein soziales Dilemma, bei dem Millionen von Forschenden sich plötzlich anders verhalten müssen und dabei entgegen der Spielregeln des wissenschaftlichen Systems handeln müssen (REF[LR33] ). Das Dilemma wurde von kommerziellen Verlagen gestaltet, welche daraus Geld verdienen. Brembs et al. (REF[LR34] ) haben einen präzisen Vorschlag erarbeitet, bei welchem ein dezentrales System aufgebaut wird, das soziale Netzwerke wie Mastodon als Vorbild hat, von Wissenschaftler*innen organisiert wird, und Forschungsprodukte wie Daten oder Programme ebenso wie die traditionellen Forschungsberichte wertschätzt. Plattformen, die ein solches Mikro-Publishing-System bereits implementieren sind Research-Equals (https://www.researchequals.com/) oder Octopus.ac (Hsing et al.). Dem System, bei dem alle Produkte begutachtet werden, der Begutachtungsprozess aber nicht die Qualität sicherstellt, stehen hier Ampel-Systeme und öffentliche Kommentierung entgegen, die signalisieren, was und ob begutachtet wurde, welche Kritikpunkte vorlagen, und wie damit umgegangen wurde.\n\n\n\n14.0.6 Forschende\nUnabhängig von Nationalität, Wissenschaftsgebiet, und Universität haben viele Forschende ihre Arbeitsweisen im Zuge von Open Science überdacht und angepasst. Hunderte haben öffentliche Erklärungen zur Forschungstransparenz (http://www.researchtransparency.org) und der Forderung von Open Science Praktiken in der Rolle von Gutachter*innen (https://www.opennessinitiative.org) unterzeichnet. Einzelne Forschende führen im Rahmen von Lehre Replikationsstudien durch (REF[LR35] , REF[LR36] ), schließen sich weltweit zusammen um gemeinsam Projekte durchzuführen, die einzelne nicht stämmen könnten (https://psysciacc.org), und entwickeln Sammlungen (https://docs.google.com/spreadsheets/d/1KUMSeq_Pzp4KveZ7pb5rddcssk1XBTiLHniD0d3nDqo/edit#gid=0, https://oercommons.org/hubs/OSKB), Leitfäden (REF https://osf.io/jfh3t/) und Glossare (REF[LR37] ) um den Zugang zu Open Science zu erleichtern. Eine noch unerfüllte Forderung ist die, Forschende das System über Zusammenschlüsse im Rahmen von Gewerkschaften zu reformieren (REF[LR38] ).\n\n\n14.0.7 Bewertungskriterien\nhttps://www.researchgate.net/publication/351638932_The_Natural_Selection_of_Good_Science\n-          Systematische Erarbeitung objektiver Kriterien\no   https://psycharchives.org/en/item/dca8878a-1f6a-4599-abe0-16134b4b7f64\no   https://osf.io/preprints/psyarxiv/5yexm\n-          Peer review guidelines https://www.researchgate.net/publication/375607348_Quantitative_manuscript_peer_review_template\nhttps://psyarxiv.com/rgh5b/\nhttps://psyarxiv.com/5yexm/\n\n\nAlternative Auswahlkriterien für Professor*innen: https://osf.io/preprints/metaarxiv/juwck\n\nMultilevel-Selektion: https://journals.sagepub.com/doi/pdf/10.1177/17456916231182568\n\nhttps://tu-dresden.de/mn/psychologie/ifap/differentielle-psychologie/die-professur/Mitarbeiter/dipl-psych-anne-gaertner Einstein Award Thema (Auswahlkriterien)\n\nProblem von Kriterien ist, dass sie oft Dinge jenseits des Mainstreams benachteiligen (REF[LR39] )\n\n\n\n14.0.7.1 Alternativen zum Impact Factor\nAltmetric für Einfluss jenseits von Publikationen (z.B. in sozialen Medien, Wikipedia, news outlets),\nDORA: https://sfdora.org/read/read-the-declaration-deutsch/\nCOARA https://coara.eu Alternative Bewertungskriterien (qualitativ, nicht quantitativ; kann man signieren)\nTOP guidelines\n-          https://www.researchgate.net/publication/369670371_Evaluating_Implementation_of_the_Transparency_and_Openness_Promotion_Guidelines_Reliability_of_Instruments_to_Assess_Journal_Policies_Procedures_and_Practices\nCiTO https://sparontologies.github.io/cito/current/cito.html\nFree Lunch Index https://link.springer.com/article/10.1007/s11192-023-04862-8\nIm Rahmen von XXX wurden Richtlinien zur Förderung von Transparenz und Offenheit in der Wissenschaft erstellt (Transparency and Opennness Promotion guidelines; kurz: TOP guidelines). Eine Zeit lang gaben manche Zeitschriften freiwillig an, welche Richtlinien wie eingehalten werden. Darin befindet sich zum Beispiel die Frage, ob mindestens eine Studie eines publizierten Artikels präregistriert sein muss, oder ob Datensätze veröffentlicht werden müssen. Je Aspekt (z.B. Öffentlichkeit der Daten) gibt es dann XX Levels: Das niedrigste Level 0 gibt an, dass es zu dem jeweiligen Aspekt keine Empfehlungen oder Vorschriften gibt. Das höchste Level 3 hingegen fordert, dass alle verwendeten Datensätze online verfügbar sind. [XXX genau anpassen an Vorgaben].\nSeit 2022 lassen sich Zeitschriften, die TOP guidelines auf verschiedenen Levels umsetzen, in der topfactor.org Datenbank durchsuchen. Somit haben Forschende eine Alternative zum Impact Factor, die tatsächlich mit der Qualität der Zeitschrift zusammenhängt.\n\n\n\n14.0.8 Open Access Publikationen\nTypen von Open Access (siehe zenodo NRW AG Open Science Auflistung)\nAn manchen Orten gibt es ungeschriebene Gesetze wie “wenn du während deiner Promotion in einem hochrangigen Journal publizierst, wirst du die Bestnote kriegen”. Damit wird den sogenannten prestigereichen Zeitschriften immer mehr Macht zugeschoben. Leuten wird außerordentlich dafür gratuliert, dass sie etwas in Psychological Bulletin oder sogar Nature Human Behavior veröffentlicht haben. Dass niemand, der nicht mit einer Universität affiliiert ist (also an einer arbeitet oder studiert), den Artikel im Psychological Bulletin lesen kann, oder dass für die Veröffentlichung in Nature XXX Euro bezahlt wurden, spielt dabei keine Rolle. Leider sind genaue Regeln, wann welche Artikel von diesen Zeitschriften veröffentlicht werden intransparent. Auf den Seiten der Zeitschriften sind zwar üblicherweise Hinweise für Autor*innen und kurze Texte über die Zeitschrift, ob eine spezifische Studie genommen wird oder nicht, entscheiden zuerst alleine die Herausgeber*innen oder auf Englisch Editors. Zwischen zahlreichen Einreichungen müssen sie diejenigen auswählen, die an die Gutachtenden weitergeleitet werden.\nDiesem intransparenten System stehen Open Access Zeitschriften entgegen. Obgleich ihr Kern die Öffentlichkeit aller Artikel ist, geht das Prinzip oft damit einher, dass die Ergebnisse des Gutachtenprozesses ebenfalls veröffentlicht werden.\n-          Guide\no   https://psycharchives.org/en/item/4d0f12d8-a542-4cfd-b718-9ee7c9d11f5b\n-          Paywall umgehen\no   Sci-hub\no   12ft.io\no   #canihazpaper\no   Autor*innen persönlich anschreiben (z.B. Mail, Researchgate.org, Academia.edu)\n-          Offene zeitschriften\no   Directory of Open Access Journals: https://doaj.org\no   Free Journal Network: https://freejournals.org/current-member-journals/\no   Journals finden: https://service.tib.eu/bison/ \no   TOP Factor\n-          Predatory Publishers\no   Bealls Liste, https://beallslist.net\no   https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5493177/\no   https://thinkchecksubmit.org (oder bei Konferenzen: https://thinkcheckattend.org)\no   White list: https://doaj.org\no   Bei Bibliotheken nachfragen!\n-          Monitoring\no   OpenAPC\no   https://open-access-monitor.de\no   https://hal.science/hal-04121339v2\no   Offener Datensatz mit von AI kodierten Indikatoren von Open Science des Verlages PLOS: https://theplosblog.plos.org/2023/10/open-science-indicators-q2-2023/?utm_id=354&utm_source=internal&utm_medium=email&utm_campaign=354lib1023&utm_content=link\n-          Open Acces Typen\no   (moving wall\no   Green\no   Gold\no   diamond), siehe NRW OS Strategie  https://zenodo.org/record/8322048  s. 11-12\no   “guerilla open access” (sci-hub)\no  \n-          Open Journals:\no   http://www.theoj.org\no   OJS\no   Directory of open access journals\n-          Offene Bücherei Badge: https://badge.openbiblio.eu\n-          DEAL Verträge mit Publishern: https://deal-konsortium.de/publizierende\n-          Bücher (Open Textbooks): https://content.iospress.com/articles/education-for-information/efi190260\n-          Editorial Mass Resignations\no   Sprachwissenschaften: Linguista à Glossa (Johan Rooryck; Vortrag am 10.01.2024)\n§  Herausgeber (Wissenschaftler) hat sich um Zeitschrift gekümmert, Elsevier Vertrag um Typesetting\n§  Wissenschaftler*innen wollten Open Access Zeitschrift mit APCs, die transparent gemacht werden\n§  2015: 4 weitere Zeitschriften, die durch Community geleitet wurden, haben ebenfalls Diamond OA durchgesetzt\n§  Unterstützung durch Bücherei und Open Library of Humanities\n§  Eine vollständige Community wurde erfolgreich zu Diamond OA verschoben\n§  Zeitschrift als Vehikel für eine wissenschaftliche Community – wie ein Auto, das von einer Familie gekauft wird: man lässt das alte zurück und nimmt das neue, Editors, Reviewers, Readers, sind alle dieselben Personen\n§  Übergang zu OA ist schwierig\n·         Finanzielle Ungewissheit\n·         Impact Factors benachteiligen neue Journals, sind unabhängig von Content, Community, Mission Statement\n·         Alle Herausgeber*innen müssen zustimmen\n·         Montgomery & Neylon 2019: „The value of a journal is the community it cfreates, not the papers it publishes” REF[LR40] \n·         Was eine Zeitschrift nicht sein sollte X[LR41] \n·         Shared Ownership à Zeitschrift kann nicht an Verlag verkauft werden\n·         Content vs. Service: content controlled by academic community, technical services (publishing, typesetting) can be paid for\no   Neuroimage (Chris Chambers)\n§  Übergang zu OA 2020 mit APC (mehrere tausend Dollar)\n§  Pandemie war zeitgleich, Community fand es nicht gut, hat es aber mitgemacht\n§  Wegen großem Erfolg wurden APCs erhöht (3150$?)\n§  Community hat viele Jahre hart gearbeitet und wird dann mit höheren Kosten bestraft, hat aber nichts von den Kosten\n§  Chief Editor (Steve Smith) wollten Reduktion auf 2000$ verhandeln, Elsevier wollte das nicht: „Market Forces support the current APC” und fand die Kosten absolut berechtigt. Solche Kosten sind eine große Hürde für globalen Süden.\n§  Editorial Board hatte genug und alle haben resigniert. 3 Journals: Neuroimage, Neuroimage reports, Neuroimage clinical à ~40 Leute, haben das koordiniert, um öffentliche Wirkung zu maximieren (mit Medien, sozialen Medien, haben mit non-profit Publisher gesprochen)\n§  Kommerzielle Verlage sind Parasiten, fügen der Wissenschaft keinen Wert hinzu, existieren nur, um Profit aus Wissenschaft zu ziehen und geben den Wissenschaftler*innen fast nichts zurück (höchstens Typesetting)\n§  Wichtige Message an alle: „from this point onwards, it is socially inacceptable to support these journals” “it’s important that we do this all together”\n§  Die Zeitschriften nun so gut wie Tod à nicht einfach ein anderes Journal erstellen, sondern klar machen, dass der Name nichts bedeutet, sondern die Community\n§  Und das reicht noch nicht: Mass Resignations sollten eher wöchentlich stattfinden, sind aber ziemlich selten; strong editor in chief ist nötig; wir brauchen auch gar keine Publisher (Peer Community In Initiative zeigt das)\n§  Verlag (Elsevier) hat wohl versucht, öffentliche Kommunikation über EMR zu beeinflussen und versucht, Forschende einzuschüchtern\no   Critical Public Health (Judith XXX)\n§  Editorin hatte Moratorium für 1 Jahr, laut Vertrag durfte sie also 1 Jahr nach Ende des Vertrages Verbot, für eine Zeitschrift zu arbeiten\n§  Verlag wollte Verträge ändern: statt „page budget” (5 issues pro Jahr mit vorgegebener Seitenzahl) „minimum number of papers” Modell; Editorial Board hatte dabei kein Mitspracherecht\n§  Graduelle Erosion von akademischer Kontrolle\n§  2023: Board unilateral resigned (erstes Mal, dass sich alle einig waren)\n§  https://cphn.net/breaking-news/\n§  Journal of Critical Public Health neu gegründet, über OJS gehostet\n§  Schwierigkeit:\n·         Netzwerk, dem das Journal gehört, musste formalisiert werden (bisher vor allem jiscmail Liste)\n·         Nachhaltigkeit ist schwierig langfristig mit diamond Open Access\n·         Keine Indizierung bisher (ISSN, IF, Abstracting)\n·         Legale Fragen zum Intellectual Property unklar und nicht geprüft: Die Community hat Submission Guidelines usw. geschrieben, Verlag (Taylor and Francis) behauptet nun, dass diese Texte dem Verlag gehören. Wissenschaftler*innen haben keine Kapazitäten für Gerichtsprozess.\no   Neuropathologie: XXX à Free Neuropathology\no   Verlage sind dagegen ausgestattet\n§  Editorial Board wird rotiert, damit sich Editors nicht so stark vereinen können\n§  Unwahrscheinliche Treffen in Person\n§  Non-compete Klauseln / Moratorien\no   Liste von EMR: https://retractionwatch.com/the-retraction-watch-mass-resignations-list/\no   https://sparceurope.org/the-seed-of-a-global-federation-for-diamond-open-access-has-been-planted/\nEine besondere Form des öffentlichen Zugangs verbreitet sich aktuell in der biotechnologischen Forschung aus. Damit Wissenschaftler*innen, Ingenieur*innen, aber auch die allgemeine Bevölkerung Zugang zu Konstruktionen hat, wird dort zu Klemmbausteinen wie beispielsweise LEGO® Steinen [LR42] gegriffen (REF[LR43] ). So hat David Aguilar (Tabelle 1) beispielsweise einen prosthetischen Arm mit Klemmbausteinen entworfen.\n\n\n14.0.9 Pre-Prints\n-          Stark angelehnt an Jonny Coates Vortrag (REF); recording sollte bald verfügbar sein über FZJülich\n-          Definition: noch nicht von einem journal ge peer reviewet\n-          In Corona-Pandemie viel Aufmerksamkeit erfahren von Forschenden, Politik, und Medien, weil Journals zu langsam sind (Veröffentlichungsprozess kann bis zu 10 Jahre dauern) https://www.biorxiv.org/content/10.1101/2020.05.22.111294v3\n-          Qualität genau so gut: https://pubmed.ncbi.nlm.nih.gov/36240832/\no   Journals machen\n-          Pre-Prints von fast allen Journals gedulded (sherpa romeo v2)\n-          Können kommentiert werden, Probleme werden manchmal innerhalb von 2 Tagen aufgedeckt werden, statt mehreren Jahren https://www.statnews.com/2020/02/03/retraction-faulty-coronavirus-paper-good-moment-for-science/\no   Retractions in Journals sehr sehr schwierig, bei Preprints einfach\no   Retraction guidelines vom Committee on Publication Ethics (Barbour et al., 2019)\n-          Schneller: https://www.pnas.org/doi/10.1073/pnas.1511912112\n-          Alternatives Publishing Modell https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.3000116\no   Unabhängig von Publishern\no   Open Access\no   Schneller (https://europepmc.org/preprints)\no   Open Peer Review, funktioniert gut https://asapbio.org/review-commons-9-months\no   Kein Gatekeeping https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0281659\no   mit dem Modell wird experimentiert und geschaut, wie es funktioniert (Service zum Reviewen von Preprints: https://www.reviewcommons.org)\no   Zitat J. Coates: „I don’t think that we need peer review” (REF, später nochmal prüfen oder ihn anschreiben, er müsste bald dazu was veröffentlichen)”\n-          Tracker: https://europepmc.org/preprints\n-          Preprints als Promotionsleistung statt veröffentlichte Paper\n-          Günstiger: APCs revenue bei Springer Nature fast 600 Gazillionen!!! APCs werden immer teurer https://direct.mit.edu/qss/article/doi/10.1162/qss_a_00272/118070/The-Oligopoly-s-Shift-to-Open-Access-How-the-Big\n-          in line mit EU Council calls und US government\n-          Sorge: dass jemand vor einem kommt und etwas irgendwo veröffentlicht\no   Das geht auch bei Konferenzen schon immer, sogar bei veröffentlichten Papers\no   Bei Pre-Print gibt es eine DOI + Zeitstempel und alle können sehen, wer zuerst da war\no   Einige Zeitschriften prüfen auch schon, ob in einem Pre-Print was schon war\n-          “Put scientists back in charge”\n-          Institutionen, Funders, Reviewers: manche verpflichten schon, Preprints zu posten\n-          Je nach Feld sind Pre-Prints unterschiedlich beliebt\no   Felder mit großer Sorge von Wissensdiebstahl möchten Forschung evtl. möglichst lange zurückhalten – nicht ganz sinnvoll, weil Reviewer auch Ideen klauen können und dann sogar schwieriger zu fassen sind; mit Pre-Print ist öffentlich klar, wer zuerst wozu geforscht hat\no   In manchen Bereichen ist es nicht sinnvoll, wenn mehrere Forschende an derselben Sache arbeiten (z.B. zwei Forschungsgruppen können nicht gleichzeitig und unabhängig voneinander an einem bestimmten Ort archäologische Ausgrabungen machen)\n\n\n14.0.10 Ansätze gegen die Selektion spannender Ergebnisse\nDie Ergebnisse einer Untersuchung sind das, was am wenigsten in der Hand der forschenden Person liegt (bzw. liegen sollte - immerhin interessiert uns ja die Wahrheit und nicht die Kompetenz Forschender, Daten möglichst stark zu schönen). Umso frustrierender ist es, dass Zeitschriften das Ergebnis als Kriterium zur Publikation verwenden. Unter der Vielzahl von Einreichungen werden vor allem diejenigen Artikel gewählt, die spannende Ergebnisse erzielt haben oder die ihre anfängliche Vermutung bestätigen konnten (Confirmation Bias). Die folgenden Ansätze lösen dieses Problem zum Beispiel dadurch, dass die Ergebnisse aus dem Begutachtungsprozess ausgeschlossen werden.\n\n14.0.10.1 Results-blind peer review\nDer einfachste Weg ist dabei, den Ergebnisteil einfach zu schwärzen oder wegzulassen. Verschiedene Zeitschriften bieten das als Option an. Da es sich hierbei zurzeit (2024) eher um eine Ausnahme handelt, ist den meisten Gutachtenden jedoch klar, dass vor allem diejenigen die Option zum results-blind peer review wählen, deren Ergebnisse nicht “hübsch genug” für den klassischen Weg sind.\n\n\n14.0.10.2 Registered Report\nEin radikalerer Ansatz als die Begutachtung ohne Ergebnisteil ist die Begutachtung des Artikels, ohne dass Ergebnisse überhaupt existieren. Dieses Format heißt Registered Report. Dabei wird das Manuskript mit der zu prüfenden Theorie, Methodik, und dem Analyseplan bei der Zeitschrift eingereicht, ohne dass überhaupt Daten erhoben wurden. Kommt es zur Akzeptanz dieses „halbfertigen” Artikels (in principle acceptance), werden die Daten gesammelt, wie geplant ausgewertet, und es folgt eine weitere Begutachtungsrunde. Hierbei ist vorgeschrieben, dass die Autor*innen nichts an den bereits verfassten Teilen verändern dürfen und die Gutachter*innen im Nachhinein keine Kritik am bereits geprüften Vorgehen üben dürfen. Es geht nur noch darum, ob der Plan eingehalten wurde und ob die Schlussfolgerungen auf den geplanten Analysen fußen. Damit soll verhindert werden, dass Artikel abgelehnt werden, weil die Ergebnisse nicht spannend genug, innovativ genug, oder den Erwartungen entsprechend sind. Erste Untersuchungen können bereits nachweisen, dass sich damit die Qualität der Forschung gegenüber dem traditionellen Vorgehen verbessert (REF[LR44] ). Eine Übersicht über Zeitschriften, die dieses Format anbieten ist online verfügbar (https://www.cos.io/initiatives/registered-reports à Participating Journals).[LR45]  Ebenfalls wird dadurch deutlich, dass psychologische Forschung einem massiven Publikationsbias unterliegt (d.h. es werden vor allem Studien veröffentlicht, die ihre Vermutungen bestätigen konnten und kaum Studien, in denen das nicht geschah): REF [LR46] zeigten, dass der Anteil erwartungskonformer Ergebnisse bei Registered Reports mit 44% deutlich unter den in der Psychologie üblichen 96% liegt.\nAbbildung 6\nKlassisches Vorgehen beim Forschen und Veröffentlichen im Vergleich mit Registered Reports: Bei letzterem wird bereits der Studienplan (für gewöhnlich in Form eines unvollständigen Berichtes) begutachtet und revidiert. Eine Ablehnung im zweiten Peer Review aufgrund der Ergebnisse ist nicht möglich.\n\n\nAnleitung (ten rules paper): https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1010571\n\nmetaROR so wie PCI RR: https://easst.net/easst-review/easst-review-volume-421-july-2023/metaror-a-new-form-of-scholarly-publishing-and-peer-review-for-sts/\n\n\n\n\n14.0.10.3 Pre-Print basierte Modelle\nDurch die immer häufigere Veröffentlichung von Pre-Prints, also noch nicht begutachteten Manuskripten, eröffnen sich für die Begutachtung neue Wege. Sogenannte Overlay Journals (elife) wählen unter Pre-Prints solche aus, die sie an Gutachtende schicken um deren Meinungen einzuholen. Sofern die Autor*innen des Preprints einverstanden sind, erhalten sie dann Gutachten und ihr Artikel wird schließlich in der Zeitschrift veröffentlicht.\nEin Modell, das unabhängig von Zeitschriften arbeitet wird von Peer Community In (PCI) verwendet: Dabei werden Pre-Prints nicht zu Zeitschriften, sondern ohne Kosten zu einer PCI Initiative geschickt. Diese existieren für zahlreiche Fächer (z.B. XXX, XXX, XXX). Dort wird das Begutachtungsverfahren ähnlich wie bei klassischen Zeitschriften organisiert, nur statt am Ende im Idealfall eine Akzeptanz und eine Veröffentlichung gibt es von der PCI Initiative eine Empfehlung (Recommendation) und eine Veröffentlichung der Gutachten[LR47] . Gutachtende können selbst entscheiden, ob sie dabei anonym bleiben möchten. PCIs arbeiten mit fachspezifischen Zeitschriften zusammen. Sogenannte „PCI-friendly Journals” veröffentlichen dann die empfohlenen Pre-Prints ohne weitere Gutachten einzuholen. Dadurch sind sowohl der Mechanismus zur wissenschaftlichen Qualitätssicherung als auch der Selektionsmechanismus (was wird veröffentlicht) in den Händen der Forschenden. Dadurch, dass die Veröffentlichung in klassischen Zeitschriften als letzter Schritt optional ist, können sich wissenschaftliche Communities langfristig von Zeitschriften unabhängig machen. Ein Erklärvideo zu PCIs ist online verfügbar (https://www.youtube.com/watch?v=4PZhpnc8wwo). Die übergreifende Organisation ist nicht-kommerziell und wird seit ihrer Gründung 2017 von Forschenden geleitet (https://peercommunityin.org/pci-structure-history/). Eine spezifische PCI Initiative bezieht sich ausschließlich auf Registered Reports (PCI-RR).\nAbbildung erstellen, die den Ablauf grob erklärt\nJe nach Fach haben unterschiedlich viele Zeitschriften mit PCI Initiativen Vereinbarungen, dass sie die akzeptierten Artikel ohne eigenes Peer Review veröffentlichen. Mehr und vor allem bekannte teilnehmende Zeitschriften machen PCIs für Forschende attraktiver. Zeitschriften, die Interesse an einem PCI haben, aber die Begutachtung nicht aus der Hand geben möchten, können sich als „PCI interested Journals” listen lassen (vs. „PCI friendly journals”). Teilnehmende Zeitschriften sparen dadurch Arbeit und bleiben relevant, indem sich Ihre Funktion dahin verschiebt, dass sie thematisch relevante Forschung sammeln und disseminieren. In einem Fall hat bereits eine Zeitschrift, die als „PCI friendly” eine Vereinbarung mit PCI-RR hatte, ein Manuskript mit einer Recommendation zum erneuten Peer Review versendet und wurde sofort von den PCI Partnern entfernt. Forschende, die Gutachtenprozesse für PCIs organisieren möchten – analog zu Herausgeber*innen von klassischen Zeitschriften – können Recommender werden und müssen dazu ein Mindestmaß an Wissen haben sowie eine Schulung absolvieren (https://rr.peercommunityin.org/about/recommenders).\nTabelle 2\nZusammenfassung der verschiedenen Begutachtungsmodelle und der jeweiligen Art des Umgangs mit den Forschungsergebnissen\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBegutachtungsprozedur\n\nAusblendung der Ergebnisse\n\n\n\n\n\n\n\n\n\n\n\nTraditionell\n\nErgebnisse sind sichtbar und fließen in die Beurteilung ein\n\n\n\n\n\n\n\n\n\n\n\nResults-Blind Peer Review\n\nErgebnisse liegen vor, werden den Begutachtenden jedoch vorenthalten\n\n\n\n\n\n\n\n\n\n\n\nRegistered Report\n\nErgebnisse liegen noch nicht vor\n\n\n\n\n\n\n\n\n\n\n\nPeer-Community-In Registered Report\n(PCI-RR)\n\nErgebnisse liegen noch nicht vor\n\n\n\n\n\n\n\n\n\n\n\n\n\n14.0.10.4 Open peer-review\nSo können Leser*innen von Meta-Psychology oder dem Journal of Open Psychology Data zu jedem Artikel einen Review Report (XXX richiges Wort? Link einfügen) lesen und nachvollziehen, was die Reviewer genau getan haben und wie sich der Artikel dadurch entwickelt hat. Vereinzelten Studien zufolge (REF, REF) führt das offene Review zu faireren und positiveren Einschätzungen.\nXXX: Arten von Open Access einfügen - Gold – Diamond\nWeitere Peer-Review Modelle\n-          https://reuse-dept.org Replikation in Computer Science\nReview Ampel\n\n\n14.0.10.5 Meine Erfahrungen mit Peer Review\nPeer Review ist brutal. Das ist meine persönliche Erfahrung mit dem Prozess. Schnell habe ich in meiner wissenschaftlichen Arbeit gelernt, dass es in vielen Fällen ein Glücksspiel ist: Dabei geht es in erster Linie nicht einmal darum, ob der Artikel akzeptiert wird oder nicht, sondern ob die Gründe für die Ablehnung konstruktiv sind oder nicht. Hierzu ein paar Beispiele:\nEinreichung bei Collabra: Es handelte sich um einen Artikel, der zwischen den Disziplinen steht. Es geht nicht nur um Erwartungen, nicht nur um Produktbewertungen, nicht nur um die Methode, Daten direkt aus dem Internet herunterzuladen. Der “bunte-Vogel-Artikel” war bereits bei drei Zeitschriften abgelehnt worden. Bei keiner wurde er an die Reviewer weitergegeben, weil er nie zur Zeitschrift passte. Die zeischrift Collabra, bei der wir ihn schließlich einreichten, war zu dem Zeitpunkt noch wenige Jahre alt und war breit aufgestellt. Über ein halbes Jahr haben wir auf das Gutachten gewartet. Länger zu warten ist erstmal ein gutes Zeichen: Der Artikel wurde wohl an Reviewer rausgeschickt. In dem Fall wurden wir jedoch bitter enttäuscht: Es wurde nur XX Gutachter gefunden und die Kommentare waren nicht hilfreich. XXX NOCH GENAUER AUSFÜHREN\nEinreichung bei Journal of Experimental Social Psychology: Wir hatten eine Replikation einer dort erschienen Studie durchgeführt - der Befund ließ sich nicht replizieren. Meine Überlegung war, der Zeitschrift, die den nicht robusten Befund ursprünglich publizierte, selbst die Chance zur Selbstkorrektur zu geben. Die Reviews waren fair und positiv, es gab ein paar Punkte zu diskutieren, aber uns war klar, dass es sich hier um Verständnisprobleme und keine inhaltlichen Aspekte handelte. Nicht so der Editor: Er erklärte, dass der Befund nicht neu genug wäre und klar wäre, dass es nicht replizierbar ist. Ich erklärte ihm, dass noch niemand den Befund zu replizieren versucht hatte und wir selbst sogar vor der Analyse der Ergebnisse eine Abstimmung darüber gemacht hatten, welches Ergebnis wir erwarteten: Es war sehr ausgewogen 50-50. Wir stellten klar, dass wir alle aufgezeigten Probleme einfach lösen können und gerne die Chance zur Revidierung hätten. Der Editor hätte dabei kaum Arbeit außer den Artikel anschließend nochmal an Gutachtende zu schicken. Auf unsere Mail erhielten wir nur eine kurze Antwort: XXX (nachschauen und einfügen). Hier befand ich mich an einem Scheideweg: Warum reagiert jemand patzig auf eine ehrliche Nachfrage zu seinen Gründen? Wir entschieden uns, einen anderen Editor direkt zu kontaktieren. Nach kurzer Zeit erhielten wir die Einladung zu einer Revision. Der Artikel wurde in der revidierten Fassung veröffentlicht.\nEinreichung bei XXX: Die zentrale Aussage dieses Artikel war, dass verschiedene Messwerte einer angeblichen Eigenschaft nicht miteinander zusammenhängen. Der Befund stellte die Annahme infrage, dass es sich dabei überhaupt um eine Eigenschaft handelte. Die Ablehnungsgründe zweier Gutachtenden und der Herausgeberin machten deutlich: Niemand hatte den Artikel überhaupt gelesen. Ein Gutachter merkte an, dass etwas mit den Werten nicht stimme, weil sie laut einer der Tabellen nicht miteinander zusammenhängen. Genau das war ja unsere Aussage. Wir zeigten, dass es nich an unseren Daten lag, sondern sich in anderen Datensätzen so verhielt. Hätte er die Überschrift der Tabelle gelesen, den Absatz davor, oder den danach, wäre das klar geworden. Hat er aber nicht…\nXXX noch Einreichung bei Meta-Psych als Kontrast?\nDas sind nur kurze Auszüge aus dutzenden Einreichungen und Ablehnungen. Darüber hinaus kann fast jede*r Forschende*r von substanzlosen persönliche Beleidigungen berichten. Meiner Erfahrung nach ist anonymes versus öffentliches Peer Review wie ein Vergleich von anonymen Kommentarspalten mit nicht anonymen: Bei Anonymität beherrschen persönliche Beleidigungen und Unwahrheiten den Dialog.\n\n\n14.0.10.6 Die Abgründe des anonymen peer-reviews: Eine Sammlung von Zitaten\n·         “I tried to like it but I couldn’t”\n·         XXX hier auf Twitter nachfragen\n\n\n\n14.0.11 Open Science Incentives FORRT NOSI Badges Open Science in Berufungskommission\n-          N-best evaluation (REF[LR48] ): nur die z.B. N = 5 für die ausgeschriebene Stelle wichtigsten Publikationen werden berücksichtigt, statt auszuzählen werden diese dann ganz genau angeschaut\n-          StudySwap “However, a potential barrier to independent, prepublication replication attempts is that many researchers have a difficult time finding other labs to conduct such attempts. StudySwap can be used to find an independent research team to conduct a replication attempt of a not-yet-published study” (Chartier et al., 2018, p. 575). Zitat von: https://osf.io/preprints/psyarxiv/dtvs7/ p. 11\n-          Reviewer Initiativen und Boycotts\nRichard D. Morey et al. (2016) The Peer Reviewers’ Openness Initiative: incentivizing open research practices through peer review, Royal Society Open Science; DOI: 10.1098/rsos.150547. Published 13 January 2016\n\n\n14.0.12 Replikationsforschung[LR49] : Ansehen und Kompetenz (Normalität) im Umgang mit Replikationsstudien steigern\n\n\nUnterscheidung Vertrauenskrise und Replikationskrise: https://philarchive.org/archive/FEEWRI\n\n\nReplikationsstudien können keine Forschung ersetzen, die auf ein tieferes Verständnis von Theorien abzielt\n\nReplizierbarkeit betrifft Robustheit, Mindestanforderungen an Verallgemeinerbarkeit, und Erhöhung des Vertrauens dadurch, dass QRPs und Datenfälschung weniger wahrscheinlich werden [was, wenn original- und replikations-studie gefälscht sind?]\n\nKein Schwarz-weiß-denken: Unsicherheiten sind mit Replikations- und Original-Studien verbunden https://journals.sagepub.com/doi/abs/10.1177/10755470241239947?journalCode=scxb\n\n\n\nEs wird klar, was replizierbar sein sollte und was nicht. REF[LR50]  listen notwendige [LR51] Bedingungen dafür auf, dass „wahre” Befunde replizierbar sein können\n\n\nWahre, statistische Phänomene können nicht replizierbar sein\n\n\nMethode könnte sehr ungenau sein und trotz „wahrem” Befund nur eine sehr geringe Replikationsrate haben (mathematischer Beweis dafür bei https://royalsocietypublishing.org/doi/10.1098/rsos.200805#d1e454)\n\nMere-Measurement Effect\n\nEthnografie, qualitative Methoden, Archäologie („Befund wird bei Ausgrabung aus Kontext gerissen und dadurch zerstört”)\n\n\n\nSachen, die keine allgemeinen Gesetzmäßigkeiten darstellen können trotzdem replizierbar sein\n\n\nArtefakte\n\nUnpassende Modelle, deren Annahmen inkorrekt sind, lassen sich manchmal reproduzieren und mit neuen Daten auch replizieren\n\nHeisenbug (?)\n\n\n\nReplizierbarkeit ist nicht hinreichend für Wahrheit\n\nBelief updating nach Replikationen https://www.nature.com/articles/s41562-021-01220-7\n\n\n\nZiele von Replikationen[LR52] \n\n\n \n\n\n\nNiemand repliziert\n\n\nhttps://www.econstor.eu/bitstream/10419/267931/1/I4R-DP013.pdf\n\nhttps://osf.io/preprints/psyarxiv/fzngs\n\nhttps://journals.sagepub.com/doi/10.1177/1745691620979806\n\nhttps://peerj.com/articles/7654/\n\nhttps://doi.org/10.1016/j.respol.2018.07.019\n\nhttps://doi.org/10.1111/lang.12286\n\nhttps://journals.sagepub.com/doi/10.1177/0741932516646083\n\nhttps://journals.sagepub.com/doi/10.1177/1477370815578197\n\nhttps://journals.sagepub.com/doi/10.3102/0013189X14545513\n\nhttps://journals.sagepub.com/doi/10.1177/1745691612460688\n\nhttps://www.journals.uchicago.edu/doi/10.1086/506236\n\nhttps://osf.io/preprints/psyarxiv/sa6rc\n\nhttps://www.pnas.org/doi/abs/10.1073/pnas.2208863120\n\ntweet von gilad: https://twitter.com/giladfeldman/status/1735950123291779119\n\n\n\nGeringes Ansehen verändert sich\n\n\nVor 10 Jahren: https://osf.io/preprints/psyarxiv/dtvs7/ p. 9 replications of Bem’s pre-cognition study were desk-rejected by the editor of JPSP, Eliot Smith, who stated “This journal does not publish replication studies, whether successful or unsuccessful” and “We don’t want to be the Journal of Bem Replication” (Aldhous, 2011).\n\nJetzt immer mehr Replikation als innovativ: “Defining replication as a confrontation of current theoretical expectations clarifies its important, exciting, and generative role in scientific progress.” https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.3000691\n\nMehr Replikationen, aber zB in Soc Psych noch nicht Mainstream https://psycharchives.org/en/item/74463c10-7347-4bf2-8156-5618d42c4e93\n\nReplikationen Initiative: https://i4replication.org/reports.html\n\nhttps://openresearch.amsterdam/image/2018/1/15/20180115_replication_studies_web.pdf\n\nRuf nach Zeitschrift (https://s3.amazonaws.com/real.stlouisfed.org/wp/2015/2015-016.pdf) erhört (JCRe)\n\nDiskussionen sind zum Teil noch hart und Replikationsstudien werden falsch dargestellt https://osf.io/96pnj\n\n\n\nArten von Replikationsprojekten\n\n\nRRR: https://www.researchgate.net/publication/366862422_A_Roadmap_to_Large-Scale_Multi-Country_Replications_in_Psychology\n\nRegistered Repots /PCI-RR (steht dazu s chon woanders etwas? Ist ja nicht explizit für replikationen)\n\nAbschlussarbeiten für Replikationen nutzen:\n\n\nhttps://www.nature.com/articles/s41562-021-01192-8\n\nStudierende als Replizierende: https://link.springer.com/article/10.1007/s12286-023-00578-4\n\nHagen Cumulative Science Project:\nhttps://journals.sagepub.com/doi/10.1177/1475725719868149\n\nGilad Feldmans Projekte CORE https://osf.io/5z4a8/\n\n\n\n\n\n\n§  Leipziger Replikationsstudien: https://home.uni-leipzig.de/lerep/\n§  CREP https://www.frontiersin.org/articles/10.3389/fpsyg.2019.00247/full\n§  Laufende Großprojekte\n§  I4R und Replication Games: https://www.nature.com/articles/s41562-023-01807-2\n§  ZWB Lab²\n§  Beteiligung von Original-Autor*innen\n§  https://osf.io/preprints/psyarxiv/dwg9v\n\n\nVeröffentlichung von Replikationen\n\n\nVorschlag: jede Zeitschrift ist zuständig für Replizierbarkeit eigener Studien und MUSS Replikationsversuche veröffentlichen; pottery barn rule https://thehardestscience.com/2012/09/27/a-pottery-barn-rule-for-scientific-journals/\n\nNeue Journals\n\n\nReplikationen Journal: https://www.jcr-econ.org\n\nReproducibility in Neuroscience: https://jrn.epistemehealth.com/about/editorialteam/\n\nJournal of Trial and Error https://trialanderror.org\n\nhttp://rescience.org/x Replication Journals\n\n\n\nDatenbanken für Replikationen\n\n\nGöttingen Replication Wiki https://replication.uni-goettingen.de/wiki/index.php/Special:FormEdit/New_Replication or https://blog.repec.org/2020/08/04/a-replication-database-for-economics-and-social-sciences-the-replicationwiki/ (Jan H. Höffler) https://doi.org/10.1045/march2017-hoeffler\n\nCurateScience.org\n\nFORRT Replications and Reversals\n\nReplication Database (ReD)\n\n\n\nAuffinden von Replikationen: https://arxiv.org/pdf/2311.15055.pdf\n\n\n\nRessourcen zum Lernen über Replikationen\n\n\nReview über Replikationen in quantitativer Soziologie: https://www.annualreviews.org/doi/10.1146/annurev-soc-060116-053450\n\nWiki für Studierende / Webinar in Göttingen: https://replication.uni-goettingen.de/wiki/index.php/Webinar_series:_Replicating_empirical_studies_in_economics_-_an_opportunity_for_students Replication Wiki\n\n\n\nReplikationsmethodologie weiterentwickelt\n\n\nTerminologie wie am Anfang vorgestellt (Hüffmeier, Paper zu Generalizability von dem akademischen Rat aus Bamberg [evolutionsbiologie])\n\nSmall telescopes approach\n\nBayesian sample size planning https://www.researchgate.net/publication/373047996_Bayesian_approaches_to_designing_replication_studies\nhttps://link.springer.com/article/10.1007/s11749-023-00916-4\n\nSelection of replication studies https://www.sciencedirect.com/science/article/pii/S0010945223002691\n\nhttps://www.researchgate.net/publication/365181293_Bayesian_approaches_to_designing_replication_studies\n\nEmpfehlungen, wie Autor*innen von Replikationsstudien mit den Originalautor*innen kommunizieren sollten: https://doi.org/10.1017/S1049096520000943\n\nKriterien für Bewertung: https://docs.google.com/document/d/1p7GeOpwzQyTuzAWsD1w3zql2dS6mFgEhdf38Lc3uXC4/edit#heading=h.oti7bgwflneo\n\n\nProblem beim Updaten von Materialien: wenn original, dann veraltet und unpassend – wenn neu, dann anders und unpassend; siehe auch Kommentar von John Protzko https://rr.peercommunityin.org/PCIRegisteredReports/articles/rec?id=750\n\n\n\nIdentifikation probleamtischer Forschungsliteratur:  Original Replication of Meta-Analyses or ORMA https://www.researchgate.net/publication/365057816_ORMA_A_strategy_to_reduce_Psychology’s_replication_problems\n\n\n\n\n§  Qualitative Forschung profitiert evtl. nicht von Replikationen https://open.lnu.se/index.php/metapsychology/article/view/3764\nKASTEN / Leitfaden: Eine Replikationsstudie durchführen\n1.      Wahl der Studie\n·         Wichtigkeit der Originalstudie (Grundlegend, evtl. erkennbar durch viele Zitationen)\n·         Ungewissheit der Originalstudie (Replikationswahrscheinlichkeit möglichst nicht 0 oder 1)\n·         Bisher noch keine Replikationen\n·         Diskussionen dazu beachten (z.B. via hypothes.is oder pubpeer.com à auf pubpeer.com Paper suchen und schauen, ob es dazu Kommentare gibt)\n·         Machbarkeit (online möglich? Querschnitt? Instrumente Verfügbar bzw. bezahlbar?)\n·         Etablierte Replikationsstandards für den Bereich? (bei FMRI Daten oder Längsschnittanalysen schwieriger als bei einfachen F- oder t-Tests)\n·         Zusatz: Meta-analytische Indikatoren: auffällige P-Curve/Z-Curve, klare Hinweise auf Publikationsbias oder QRP\n2.      Durchführung\n·         Bestandsaufnahme der veröffentlichten Materialien und Daten\n·         Daten berechnen\n·   Bei verfügbaren Daten: komputationale Reproduzierbarkeit prüfen\n·   Bei fehlenden Teststatistiken diese ausrechnen\n·   Bei fehlenden Werten zu allem möglichen, evtl. andere Studien hinzuziehen\n·         Kontaktaufnahme zu Originalautor*innen, wenn möglich\n·         Stichprobenplanung: nicht den Originalstichprobenumfang nehmen, sondern Poweranalyse-Ansätze nutzen\n·   Small-Telescopes-Approach\n·   Untere KI-Grenze\n·   Äquivalenztests\n·         Anpassung der Methode\n·   Ggf. Update veralteter Materialien\n·   Übersetzung in andere Sprache\n·   Notwendigkeit einer besonderen Stichprobe?\n·   Gibt es etablierte Standards bezüglich des statistischen Tests (v.a. bei standardisierten Effektstärken [r, d, f], nicht bei komplexen Verfahren wie RSA, Clustering, Decision Trees, FMRI Daten)\n·   Gibt es ähnliche Replikationsstudien in dem Bereich, die für die Methode herangezogen werden können (z.B. via Replication Database suchen)\n·         Anpassungen\n·   Qualitätssicherung: Welche Veränderungen sind nötig oder sinnvoll? Was wissen wir neues, was wir berücksichtigen sollten?\n·   Extensions: Lassen sich zusätzliche Items o.ä. „hinten” an die Studie anhängen, womit sich die externe Validität erhöhen könnte? Gibt es zusätzliche einfache Ergänzungen, bei denen eine Replikation nicht zu erwarten ist?\n·         Präregistrierung mittels Template (Empfehlung: Replication Recipe, Brandt et al., 2014 http://dx.doi.org/10.1016/j.jesp.2013.10.005\n·         Ggf. als PCI-RR submitten\n3.      Analyse\n·         Berechnung der Ergebnisse\n·         Vergleich Original vs. Replikation\n·         Vergleich naher vs. „ferner” Ergebnisse (z.B. bei denselben Items vs. Bei Extension [falls vorhanden]\n4.      Diskussion\n·         Bewertung des Ergebnis\n·   LeBel Kriterien (signal/no signal + size)\n·   Umgangssprache (success, failure, practical failure, inconclusive)\n·         Abweichungen von Präregistrierung (jeweils Beschreibung, Begründung, und Auswirkung auf Ergebnisse)\n·         Abweichung von Originalstudie (jeweils Beschreibung, Begründung, und ob es die Ähnlichkeit der Ergebnisse hätte beeinflussen können)\n·         Bei anderen Ergebnissen Originalautor*innen um Kommentar bitten\n5.      Bericht\n·         Manuskript: Orientierung an state-of-the Art von Gilad Feldman (LINK/REF) und an I4R Replication Report Template (https://osf.io/j2qrx)\n·         Eintragung in Replication Database (via Fragebogen oder StaRT)\n·         Registrierung der Ergebnisse über Replication Recipe („post completion”)\n·         Ggf. Verweis auf Replikationsstudie unter der Originalstudie bei pubpeer.com\n·         Preprint: auf passendem Server hochladen\n·         Zeitschrift: Originalzeitschrift (meistens kein Interesse), Collabra: Psychology, PCI (wenn es eins gibt), Meta-Psychology (designierte Section für Replikationen); Hinweise zu Zeitschriften hier: https://i4replication.org/publishing.html\n\n\n14.0.13 Modellierung von Replizierbarkeit\nAlle Studien in einem Wissenschaftsbereich zu replizieren ist aktuell unrealistisch und würde enorme Ressourcen benötigen. Daher befinden sich verschiedene Methoden in Entwicklung, um auf Basis von Eigenschaften einer Studie vorherzusagen, welche Replikationsergebnis zu erwarten ist. Im repliCATS Projekt (REF[LR53] ), das Teil des SCORE Projektes zur Messung wissenschaftlicher Qualität ist, werden Einschätzungen von Forschenden, Reproduktionsversuche, und Replikationsversuche kombiniert. Ähnliche Projekte verwenden komplexe statistische Modelle (z.B. Large-Language-Models) oder meta-analytische Methoden wie p-curve oder Z-curve zur Vorhersage von Replikationsraten. Solche Modelle benötigen üblicherweise sehr viele Beobachtungen und Vorhersagen sind nur auf der Ebene von hunderten Studien sinnvoll. Beispielsweise rechneten Boyce et al. 2023 und Hagen Cumulative Science project Moderationsanalysen, prüften also welche Eigenschaften von Studien sich auf das Replikationsergebnis auswirken (z.B. Wechsel von Studie im Labor zu Online-Studie). Mittels umfassender Datenbanken wie der FORRT Replication Database können in Zukunft präzisere Modelle erstellt werden. Aktuelle Ergebnisse sind als vorläufig und mit Vorsicht zu interpretieren, da sie auf nicht-zufälligen Stichproben basieren, die Studieneigenschaften nicht systematisch und zufällig variiert wurden, und Replikationen solcher meta-analytischen Befunde schwierig sind.\n\n\n14.0.14 Veröffentlichung aller Ergebnisse\n-          https://opentrials.net/about/\n-          Unveröffentlichte Medizinische Studien: Quest Dashboard: https://quest-cttd.bihealth.org/#tabStart\n-          File drawer reports\no   Meta psychology\no   psychfiledrawer.org meta-psychology\n-          Datenbanken, die auch unveröffentlichte Ergebnisse aufnehmen (auch Lösung für cumulative science)\no   Bodypositions, OpAQ Community Augmented: metalab.stanford, metabus, https://camarades.shinyapps.io/AD-SOLES/ , …\no   Replication database\no   Metapsy https://www.metapsy.org\no   Metalab stanford https://langcog.github.io/metalab/\n\n\n\n\n\n\n\n\n\n\n\n\n\nName[LR54]\n\nLink\n\nTopics\n\n\n\n\n\n\n\n\n\n\n\n\n\nMetaLab\n\nlangcog.github.io/metalab\n\nEarly Language, Cognitive Development\n\n\n\n\n\n\n\n\n\n\n\n\n\nmetaBUS\n\nmetabus.org\n\nAll of social sciences\n\n\n\n\n\n\n\n\n\n\n\n\n\nSOLES\n\ncamarades.shinyapps.io/AD-SOLES/\n\nAnimal models of Alzheimer’s disease\n\n\n\n\n\n\n\n\n\n\n\n\n\nOpAQ\n\nt1p.de/openanchoring\n\nAnchoring Effects\n\n\n\n\n\n\n\n\n\n\n\n\n\nFReD\n\nt1p.de/ReD\n\nReplications (all fields)\n\n\n\n\n\n\n\n\n\n\n\n\n\nPower Posing\n\nmetaanalyses.shinyapps.io/bodypositions\n\nBody Position Effects\n\n\n\n\n\n\n\n\n\n\n\n\n\nMetadataset\n\nmetadataset.com\n\nAgriculture\n\n\n\n\n\n\n\n\n\n\n\n\n\nMETAPSY\n\nhttps://www.metapsy.org\n\nPsychotherapy\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnleitungen:\nmetaUI: https://github.com/lukaswallrich/metaui\nDynameta: https://github.com/gls21/Dynameta\nMaterialien: https://cama.psychopen.eu\n-          Journal of Articles in Support of the Null Hypothesis\n-          Journal of Negative Results https://www.jnr-eeb.org/index.php/jnr/about\n\n\n14.0.15 Umgang mit Fehlern\n-          Fehlerkultur\n-          Retraction\no   Standardisierte Retraction Notes: https://www.researchgate.net/publication/374082265_A_tale_of_three_retractions_a_call_for_standardized_categorization_and_criteria_in_retraction_statements\no   Wissenschaftliche Fehler können extreme Folgen haben; Lancet MMR Autism Fraud https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2831678/\no   Einige Wissenschaftler*innen prüfen große Mengen an Artikeln, Elisabeth Bik z.B. hat zu Retractions von fast 600 und Korrekturen von fast 500 Artikeln beigetragen (https://www.buzzfeednews.com/article/stephaniemlee/elisabeth-bik-didier-raoult-hydroxychloroquine-study)\no   Aktuell Studie zur Sichtbarkeit von Retractions mit Möglichkeit, sich über zurückgezogene Artikel, die man zitiert, informieren zu lassen (RetractoBot, https://www.retracted.net) vom Bennett Institute for Applied Data Science at the University of Oxford.\n-          Pubpeer.com\no   Beispiel einer Diskussion eines Papers vom Nobelpreisträger Thomas Südhof https://pubpeer.com/publications/DAF32F6DB6C166337E5381F769AE52#1\no   Hohe Aufmerksamkeit führt zu Korrektur vieler Fehler, zum Beispiel von 12 Fehlern (Stand: Dezember 2023) im Südhof Lab (https://med.stanford.edu/sudhoflab/science-resources/integrity---pubpeer.html)\n-          Error project (malte elson)\n\n\n14.0.16 Offene Lehre / Open Teaching / Open Educational Resources\n-          https://zpid.cloud.panopto.eu/Panopto/Pages/Sessions/List.aspx?utm_campaign=ptos_videos#folderID=“2936633e-384a-4cf1-a8ea-aee500adc653”&notificationBannerShown=true\n-          Repositorien für Bildungs- und Lehrmaterialien (Beispiele u.a. von https://leibniz-psychology.org/practices-and-tools-of-open-science/open-science-in-der-lehre)\no   https://www.oerbw.de\no   https://www.twillo.de/oer/web/\no   https://portal.hoou.de\no   https://zenodo.org\no   Osf.io\no   https://forrt.org/syllabus/ (spezifisch open science)\n-          Offene Webinare, zB zu Replikationsforschung https://acm-rep.github.io/2024/posters/ACM_REP24_paper_30.pdf\n-          Anleitungen und Glossare (Parsons et al., 2022) zum Beispiel wundervoll gesammelt bei FORRT.org\n-          Open Scholarship Knowledge Base (OSKB)\n-          ROSiE[LR55]  Knowledge Hub\n-          Open Economics Guide https://openeconomics.zbw.eu/en/\n-          ARIADNE\n [LR1]REF\n [LR2]https://opusproject.eu/openscience-news/open-science-horizon-unescos-recommendation/\n-           [LR3]https://www8.cao.go.jp/cstp/kokusaiteki/g7_2023/2023_bestpracticepaper.pdf\n [LR4]https://www.consilium.europa.eu/en/press/press-releases/2023/05/23/council-calls-for-transparent-equitable-and-open-access-to-scholarly-publications/\n [LR5]https://www.ema.europa.eu/en/documents/regulatory-procedural-guideline/triggers-audits-good-laboratory-practice-glp-studies_en.pdf\n [LR6]https://zenodo.org/record/8322048 \n [LR7]https://opal.landtag.nrw.de/portal/WWW/dokumentenarchiv/Dokument/MMV18-2422.pdf\n [LR8]https://www.jmwiarda.de/https-www.jmwiarda.de-2024-05-28-bitte-nachschaerfen/\n [LR9]Erhöhung der Offenheit an finnischen Universitäten https://www.zbw-mediatalk.eu/de/2024/02/open-science-monitoring-offenheit-als-wichtiger-teil-guter-forschungspraxis-in-finnland/\nNetzwerk Nachhaltige Wissenschaft\nhttps://zenodo.org/records/11204833\n [LR10]Das zu Universitäten?\n [LR11]Worum geht es dabei genau?\n [LR12]https://arxiv.org/abs/2205.01833\n [LR13]schönbrodt research practices I und II\n [LR14]https://www.cambridge.org/core/books/abs/data-science-in-the-library/toward-reproducibility-academic-libraries-and-open-science/6FB18D7BCD8B82E597ED0D4B390C72EE\n [LR15]https://www.degruyter.com/document/doi/10.1515/bfp-2024-0008/html\n [LR16]https://de.wikipedia.org/wiki/Zweitveröffentlichung\n [LR17]https://doi.org/10.31234/osf.io/prx8w\n [LR18]https://zenodo.org/records/7193838\n [LR19]ergänzen, Gesellschaft für Wirtschaftswissenschaften?\no    [LR20]Public Open Science Institut für die Psychologie (ZPID)\n [LR21]https://openscience.cern\no    [LR22]https://www.apa.org/pubs/journals/resources/publishing-tips/transparency-openness-promotion-guidelines?utm_campaign=apa_publishing&utm_medium=direct_email&utm_source=businessdevelopment&utm_content=openscience_promo_11302023&utm_term=text_middle_learnmore\n [LR23]https://osf.io/bnh7p\n [LR24]https://joss.theoj.org\n [LR25]erklären\n [LR26]https://www.researchgate.net/publication/372509504_A_Guide_for_Social_Science_Journal_Editors_on_Easing_into_Open_Science\n [LR27]Idee: Review als Kommentar bei Pubpeer posten bei Ablehnung\n [LR28]https://doi.org/10.1177/09567976231221573\n [LR29]https://doi.org/10.1177/09567976231221573\n [LR30]https://osf.io/preprints/psyarxiv/5hkjz\n [LR31]https://s3.amazonaws.com/real.stlouisfed.org/wp/2015/2015-016.pdf\n [LR32]link\n [LR33]https://royalsocietypublishing.org/doi/10.1098/rsos.230206\n [LR34]https://royalsocietypublishing.org/doi/10.1098/rsos.230206\n [LR35]boyce 2023 eleven years\n [LR36]hagen cumulative science project\n [LR37]forrt glossary\n [LR38]https://osf.io/preprints/psyarxiv/bkxfq\n [LR39]https://open.lnu.se/index.php/metapsychology/article/view/3764\n [LR40]https://blogs.lse.ac.uk/impactofsocialsciences/2019/03/29/the-value-of-a-journal-is-the-community-it-creates-not-the-papers-it-publishes/\n [LR41]\n [LR42]Hier Bild von Lego-Steinen einfügen (selbst schießen)\n [LR43]https://doi.org/10.1016/j.tibtech.2022.02.003\n [LR44]https://osf.io/preprints/metaarxiv/7x9vy\n [LR45]ergänzen:\n·         10.1038/s41562-021-01193-7\n [LR46]https://journals.sagepub.com/doi/10.1177/25152459211007467\n [LR47]Irgendwo diskutieren, warum negative gutachten nicht veröffentlicht werden\n [LR48]https://osf.io/preprints/psyarxiv/tzaqh\n [LR49]hier nochmal durchschauen: https://lakens.github.io/statistical_inferences/17-replication.html#analyzing-replication-studies \nund\nhttps://pubmed.ncbi.nlm.nih.gov/26214497/\n [LR50]https://royalsocietypublishing.org/doi/10.1098/rsos.200805#d1e454\n [LR51]Kasten: Begriffserklärung für Notwendige und hinreichende Bedingungen\n [LR52]Fletcher (2020) describes the function of replication is to confirm one of the following claims regarding the findings of the original research: (1) they are not due to mistakes in data analysis, (2) they are not due to sampling error, (3) they do not depend on contextual factors, (4) they do not arise from fraud or questionable research practices, (5) they generalize to a larger population than that sampled in the original, (6) their aspects pertaining to the theoretical hypothesis of interest hold even when that hypothesis is operationalized or tested in completely different ways\nhttps://cornerstone.lib.mnsu.edu/cgi/viewcontent.cgi?article=2424&context=etds\n [LR53]https://www.youtube.com/watch?v=qa2_9NrEmeA\n [LR54]aktualisieren (siehe ppt)\n [LR55]https://rosie-project.eu/knowledge-hub/#!training/health-and-life-sciences",
    "crumbs": [
      "Lösungen",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Das System</span>"
    ]
  },
  {
    "objectID": "lösungen_methoden.html",
    "href": "lösungen_methoden.html",
    "title": "15  Methoden",
    "section": "",
    "text": "Große Menge an Lösungsvorschlägen, für einzelne Forschende oft überwältigend\nWegweise:\n-          Ariadne\n-          Kumu Access2perspectives https://kumu.io/access2perspectives/open-science#disciplines/\n\n15.0.0.1 Meta-Analyses und Reviews\n-          use meta-analysis for replication studies https://www.researchgate.net/publication/360848749_Replication_Is_for_Meta-Analysis\n-          Ricthlinien im Hinblick auf neue Standards: https://osf.io/preprints/psyarxiv/59vsw\n\n15.0.0.1.1 Assessing publication bias\n-          https://www.researchgate.net/publication/363583595_Assessing_Publication_Bias_a_7-Step_User’s_Guide_with_Best-Practice_Recommendations\n-          Toolbox: https://www.sciencedirect.com/science/article/abs/pii/S0148296323005489\n-          z-curve und z-curve validation preprint\n-          Test for excess significance (Ioannidis & Trikalinos, 2007)\n-          Incredibility Index (Schimmack, 2012)\n-          Funnel plot (Light & Pillemer, 1984)\n-          P-curve (Simonsohn, Nelson, & Simmons, 2014)\n-          Z-Curve (Bartos & Schimmack, 2020)\n-          Sensitivitätsbatterie (bushman kepes röseler körner)\n-          RoBMA bayesian https://onlinelibrary.wiley.com/doi/full/10.1002/jrsm.1594\n-          Cumulative science\no   Machine readable manuscripts\no   Reporting templates (e.g., Brandt et al., 2014, replication recipe post completion registration on OSF, REF successes and failures)\no   Dynamic meta analyses\n§  Bodypositions,\n§  OpAQ Community Augmented:\n§  metalab.stanford,\n§  metabus,\n§  soles, …\n§  Replication database\n§  ReproSci https://reprosci.epfl.ch\n§  https://portlandpress.com/clinsci/article/137/10/773/233083/Systematic-online-living-evidence-summaries\n§  metaUI r Paket\no   Multiverse analyses\no    \n\n\n\n15.0.1 Reproducibility Checks / Reproduzierbarkeit\n-          Meta-Psychology\no   Societies: Besteht auf Reproducibility: https://doi.org/10.15626/MP.2023.4020\n-          Statcheck\no   https://www.mdpi.com/2306-5729/1/3/14\no   M nuijten paper\n-          https://osf.io/mydzv/\n-          Guidelines: https://docs.google.com/document/d/1p7GeOpwzQyTuzAWsD1w3zql2dS6mFgEhdf38Lc3uXC4/edit#heading=h.gjdgxs\n-          Container / Capsules\n-          Journal für Computational Reproducibility: http://rescience.github.io\n-          Übergreifendes Gebiet: Research Software Engineering (RSE)\no   https://arxiv.org/pdf/1609.00037.pdf\n-          Großangelegte Reproduktionsprojekte\no   Vor allem zwei Projekte, beteiligte Personen überschneiden sich, sind aber unabhängig\no   Verwenden ordinalen Reproducibility Score\n§  Erklärung https://bitss.github.io/ACRE/assessment.html#levels-of-computational-reproducibility-for-a-specific-display-item\no   Social Science Reproduction Platform, https://www.socialsciencereproduction.org/metrics\n§  Leitfaden für Durchführung von Reproduktionen: https://www.socialsciencereproduction.org/metrics\no   Institute for Replication (I4R)\n§  Eigene Datenbank\n§  Internationale Events „Replication Games”\n§  Vor allem Ökonomie und Politikwissenschaften (REF[LR1] )\n§  Ergebnisse von 110 Reproduktionen „Mass reproducibility and replicability: a new hope” https://econpapers.repec.org/paper/zbwi4rdps/\no   Open Code wird seitens Zeitschriften und zukünftig eventuell auch für den Abschluss von Datennutzungsverträgen mit Panels nötig (SOEP hat zB nur 20% open code, https://www.wifa.uni-leipzig.de/fileadmin/Fakultät_Wifa/Institut_für_Theoretische_Volkswirtschaftslehre/Professur_Makroökonomik/Economics_Research_Seminar/ERS-Paper_Marcus.pdf)\n\n\n15.0.2 Robustness checks\n-          Multiverse analysis https://www.researchgate.net/publication/367204012_EEGManyPipelines_A_large-scale_grass-root_multi-analyst_study_of_EEG_analysis_practices_in_the_wild\n\n15.0.2.1 Markdown Documents\n\n\nComputational Reproducibility Pilot Project: https://psyarxiv.com/k8d4u/ | HIER AUCH DISKUTIERT ALS SERVICE (könnte ULB machen)\n\n\ninteractive multiverse / multiverse analysis\nStatistik\n-          Bayesianische Statistik\n-          Verbannung von NHST bei Basic and Applied Psychology, bisher mit negativen Auswirkungen, die die Probleme eventuell noch verschärfen (https://www.tandfonline.com/doi/full/10.1080/00031305.2018.1537892#:~:text=Part%20of%20the%20BASP%20editors,their%20expository%20text%20with%20a)\n#### Transparency Checklist\n\n\n\n15.0.3 Open Data\n-          Abbruch einer Promotion, Wechsel der Institution, usw. können dazu führen, dass wertvolle Zeit & Arbeit verloren gehen; im Extremfall wurden Tiere aufgezogen, operiert, und Forschung kann nicht weitergeführt werden wegen schlechter Dokumentierung\n-          Vor Durchführung der Studie sollte klar sein, wie die Daten aussehen werden (Variablen, Bedeutung, Umfang, usw.)\n-          https://www.datalad.org  \n-          Myths: https://www.sortee.org/blog/2024/04/12/2024_open_data_myths/\n-          https://leibniz-psychology.org/das-institut/drittmittelprojekte/datawiz-ii\n-          originalstudie hierzu,\n-          Vanpaemel et al., 205,\n-          und das: https://www.researchgate.net/publication/370001752_Data_sharing_upon_request_and_statistical_consistency_errors_in_psychology_A_replication_of_Wicherts_Bakker_and_Molenaar_2011\n-          Repositorien für daten\no   Osf.io\no   Psych archives zpid\no   Re3\no   Universitätsspezifische (z.B. datastore der Uni Münster)\no   Fachspezifische (info auch hier: https://forschungsdaten.info/themen/veroeffentlichen-und-archivieren/repositorien/)\n§  Economics & social sciences: data.gesis.org\n§  Earth science and life science: Pangaea.de\n§  Humanities, cultural studies: de.dariah.eu\n§  Linguistics: www.clarin.eu\n§  Biology: gfbio.org\n§  Materialwissenschaften: nomad-lab-eu\n§  Psychologie: Researchbox.org\no   Coscine https://about.coscine.de\no   Zenodo (interdisziplinär)\no   Git\no   Open Source Lösungen statt proprietärer Software, https://www.mkw.nrw/hochschule-und-forschung/digitalisierung-hochschule-und-wissenschaft/forschungsdatenmanagement-fdm https://www.nfdi.de\no   https://www.researchobject.org/ro-crate/\no   https://www.protocols.io\no   https://www.icpsr.umich.edu/web/pages/ICPSR/index.html\no   Für Qualitative Daten: https://qdr.syr.edu/deposit/data\n-          Kriterien für Daten teilen: FAIR\no   Pyramid of requirements https://content.iospress.com/articles/information-services-and-use/isu805\no   Halbautomatische Extraktion von Meta-Daten https://www.nfdi4chem.de/de/lister-halbautomatische-metadatenextraktion-aus-kommentierter-experimentdokumentation-in-elabftw/\no   Datenstruktur: https://psych-ds.github.io\no   Fair Aware Tool zum Testen des Wissens zu FAIR https://fairaware.dans.knaw.nl\no   Ohne Fair kostet es Europäische Wirtschaft jedes Jahr 10.2 Milliarden Euro https://publications.europa.eu/resource/cellar/d375368c-1a0a-11e9-8d04-01aa75ed71a1.0001.01/DOC_1\no   Feld-spezifische Templates für FAIRe daten: https://www.cos.io/blog/cedar-embeddable-editor?utm_source=linkedin&utm_medium=social\n-          CARE\no   Collective benefit: Ergebnisse sind auch von Gesellschaft verwendbar, Bürgerbeteiligung\no   Authority to control:\no   Responsibility: verschiedene Sprachenu und Weltanschauungen werden berücksichtigt\no   Ethics: Wertschätzung\n-          Zeitschriften\no   JOPD (psychologie)\no   Ing.grid (ingenieurswissenschaften) https://www.inggrid.org\no   Weitere wichtige?\n-          Beispiele für Datenbanken\no   https://db.mocoda2.de/c/home Chatverläufe\no   OpAQ Ankereffekte\no   Cooperation database\no   Stanford meta analysis aggregator\no   http://metabus.org#\no   Listen\n§  Aaron Charlton Mktgo.org\n§  https://replications.clearerthinking.org/replications/\n-          Teilen von qualitativen Daten, dort ist Anonymisierung relative schwierig: https://doi.org/10.1177/25152459231205832\n-          Data parasites / data police / data terrorists\n-          Nachfragen: https://www.whatdotheyknow.com/request/trial_protocols_behavioural_insi_2\n-          Mehr Zitate? https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0230416\n-          Workshop-Unterlagen: https://j-5chneider.github.io/PTOS-open-data/\n-          Infosammlung zu Forschungsdaten: https://github.com/UB-Mannheim/awesome-RDM\n-          Daten auch wertvoll als Gedächtnis der Gesellschaft, soetwas sollte archiviert werden, z.B. https://liebesbriefarchiv.de\no   Daten von Personen aus allen Feldern der Gesellschaft (crowdsourced) und öffentlich verfügbar\no   Andere Beispiele für Citizen Science: https://www.buergerschaffenwissen.de und https://wissenschaft-im-dialog.de/mitmachen/\n-          Langzeitarchivierung vs. Archivierung\no   1000 Jahre https://archiveprogram.github.com\n\n\n15.0.4 Replicability\n#### Assuring Replicability in Primary Research\n\nReplikationsstudien in Sozialpsychologie schwierig wegen Anreizstruktur und Ressourcen https://doi.org/10.1027/1864-9335/a000548\nAndere Felder, Soziologie, Präregistrierung: https://www.researchgate.net/publication/368454140_Preregistration_and_Registered_Reports_in_Sociology_Strengths_Weaknesses_and_Other_Considerations\n\nIrise project (EU-gefördertes Projekt): https://irise-project.eu/research-outputs\n\nMessungenauigkeit: https://osf.io/2me7t\n\n\n\n15.0.4.1 Preregistration / Präregistrierung\n-          Erklärung\n-          Gütekriterien: Struktur + Vollständigkeit (simmons nelson vs. Pham consumer psych)\n-          Analyseplan wichtig, um wirklich P-Hacking vorzubeugen: https://www.journals.uchicago.edu/doi/abs/10.1086/730455\n-          https://www.researchgate.net/publication/375575020_Preregistration_in_practice_A_comparison_of_preregistered_and_non-preregistered_studies_in_psychology\n-          High replicability neues project https://www.nature.com/articles/s41562-023-01749-9\n-          Prävalenz: https://datacolada.org/115\n-          Es sollte nicht nur über Bereitschaft von Forschenden gehen, sondern auch im Peer Review verankert werden, sauber zu präregistrieren: https://journal.trialanderror.org/pub/reflections-on-preregistration/release/2\n-          Deviations:\no   Alle weichen ab: https://osf.io/preprints/psyarxiv/nj4es , https://osf.io/f2z7y slide 12/21\no   niemand prüft auf Abweichungen: https://osf.io/preprints/psyarxiv/nh7qw\no   Transparent changes: https://osf.io/6fk87\no   How to deviate: https://osf.io/preprints/psyarxiv/ha29k\n-          Repositorien\no   https://meta-meta-resources.org/running-studies/preparation/pre-reg-repos/#!\no   https://www.alltrials.net\no   https://clinicaltrials.gov\no   http://www.crd.york.ac.uk/PROSPERO/\n-          Auswirkung auf Ergebnisse\no   Wahrscheinlichkeit positiven Befunden statt &gt;90% dadurch nur 40-50%, Scheel, Schijen, & Lakens(2021) [siehe auch https://osf.io/f2z7y, slide 17/21]\no   https://drive.google.com/file/d/1gcyBE78tb9zerl4M35uS3npVGvMp-MPZ/view The effectiveness of preregistration in psychology: Assessing preregistration strictness and preregistration-study consistency olmo van den akker[LR2] \n##### Pre-Registration templates what is a good prereg? (see Simmons et al) What to preregister? Analysis script\nOSF arbeitet neue ein https://www.cos.io/blog/call-for-submission-for-preregistration-templates\n\n\nDiskrepanz zwischen Präregistrierung/Prereg und Publikation: https://bmjopen.bmj.com/content/13/10/e076264.long\n\nKritik: https://journals.sagepub.com/doi/10.1037/gpr0000135\n\nPrereg als Mittel zur Transparenz in der Forschungsplanung auch für Exploration sinnvoll, bei Hypothesentesten v.a. Eliminierung von Freiheitsgraden und Erhöhung von Vertrauen\n\n\n\n\n15.0.4.2 Internal Replications\nhttps://perception.yale.edu/papers/17-Scholl-APSObserver.pdf\n\n\n15.0.4.3 Power Analysis\n-          Power\n-          Small-telescopes approach\n-          Bayesianischer Ansatz\no   So lange erheben, bis Bayes-Faktor konvergiert; kann Stichprobeunumfang reduzieren, wurde so zB für Verhaltensforschung bei Tieren empfohlen (https://www.nature.com/articles/s41684-023-01308-9)\n\n\n15.0.4.4 New statistics\n-          Frequentistische Statistik wird benutzt, ist oft missverstanden und Inhalte sind mit anderen Perpsektiven vermischt (https://journals.sagepub.com/doi/10.1177/0959354314546157?icid=int.sj-full-text.similar-articles.3)\no   Gigerenzer P-Value unwissen\n-          Bayesian, Kritik: https://journals.sagepub.com/doi/10.1177/25152459231213371\n-          Effektstärken statt p-Werte\no   Equivalence testing\no   Rölle von kleinen Effekten: https://journals.sagepub.com/doi/full/10.1177/17456916221100420\no   practical relevance of small effect sizes https://www.researchgate.net/publication/352412241_Evaluating_the_practical_relevance_of_observed_effect_sizes_in_psychological_research\no   effect sizes guide\no    \n-          Alternative lakens paper “alternative to p-value is correctly used p-value”\no   Philosophische Diskussion über Nutzen von P-Werten: https://journals.sagepub.com/doi/full/10.1177/09593543231160112 und hier https://journals.sagepub.com/doi/10.1177/0959354312465483?icid=int.sj-full-text.similar-articles.1\n\n\n15.0.4.5 21 Word Solution\n\n\n15.0.4.6 Open Materials\n\n\n15.0.4.7 Transparency\n-          Bewertungskriterien: https://osf.io/preprints/psyarxiv/djmcq\n-          Transparency Checklist\n\n\n\n15.0.5 Qualitative Forschung und Open Science\n-          Pseudonymisierung von Daten ist hier sehr besonders, denn…: https://doi.org/10.1177/160940692110346\n-          Präregistrierungs-Template: https://docs.google.com/document/d/16vLoRAs6RmXKy0v1RCx6osSaYk4r64Hgl6S4_KtWbeM/edit#heading=h.fwbi14d4b65g\n-          Sharing sensitive qualitative data: https://www.youtube.com/watch?v=-Pha3EINF3s\no   Berichte von Opfern von Vergewaltigungen à müssen anonymisiert werden, extrem aufwändig, kann stellvertretene Traumata auslösen\n\n\n15.0.6 Open Source Software\n-          Research Software Engineering (RSE) [siehe auch oben bei reproducible code, aber hier sollte das meiste dazu stehen]\n-          Non proprietär à Gefahr vor Lock-in\n-          GNU Lizenz\n-          Open Source Bedeutung\n-          Bibliotheken, die Wissen verwalten, positionieren sich neu, um nun Infrastruktur zu stellen (z.B. https://www.cambridge.org/core/books/abs/data-science-in-the-library/toward-reproducibility-academic-libraries-and-open-science/6FB18D7BCD8B82E597ED0D4B390C72EE#)\n-          Von Chan-Zuckerberg-Initiative Unternehmen: https://chanzuckerberg.com/rfa/essential-open-source-software-for-science/\n-          Alternative zu Clarivate Analytics Web of Science Literaturdatenbank: https://openalex.org REF[LR3] \n\n\n\n\n\n\n\n\n\n\n\n\n\nInfrastruktur\n\nKommerzielle Software\n\nOpen Source Software\n\n\n\n\n\n\n\n\n\n\n\n\n\nLiteraturdatenbank\n\n\n\nhttps://openalex.org\n\n\n\n\n\n\n\n\n\n\n\n\n\nLiteraturverwaltung\n\nCitavi, Mendeley\n\nZotero\n\n\n\n\n\n\n\n\n\n\n\n\n\nDatenerhebung\n\nUnipark, Millisecond Inquisit\n\nPsychoPy\n\n\n\n\n\n\n\n\n\n\n\n\n\nDatenanalyse\n\nIBM SPSS, Stata\n\nGNU R, Python, Jupyter Notebook\n\n\n\n\n\n\n\n\n\n\n\n\n\nBegutachtung und Veröffentlichung\n\nEditorial Manager\n\nOpen Journal System\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n15.0.7 Slow Science\nViele der Entwicklungen lassen sich als „langsame Wissenschaft” (oder engl. Slow Science) zusammenfassen. Der traditionellen Produktion von Forschungsartikeln steht die achtsame Auseinandersetzung entgegen. Dass viele der vorgeschlagenen Lösungen im Wettbewerb um veröffentlichte Forschungsartikel nachteilig sind, kritisiert beispielsweise Hyman (2024[LR4] ). Er schlägt stattdessen vor, die Probleme mittels künstlicher Intelligenz zu lösen. Verlage wie Elsevier benutzen diese beispielsweise bereits für Peer Review (https://www.elsevier.com/about/policies-and-standards/the-use-of-generative-ai-and-ai-assisted-technologies-in-the-review-process), wenngleich Modelle wie ChatGPT 4.0 nicht dafür geeignet sind (REF[LR5] ).\n\n\n15.0.8 Big Team Science\n-          CRediT und Übersetzungen https://github.com/contributorshipcollaboration/credit-translation\n [LR1]https://journals.sagepub.com/doi/10.1177/20531680241233439\n [LR2]viele paper von denen dazu\n [LR3]https://arxiv.org/abs/2205.01833\n [LR4]https://www.researchgate.net/publication/377965519_Freeing_social_and_medical_scientists_from_the_replication_crisis\n [LR5]https://arxiv.org/abs/2402.05519#:~:text=Practical%20implications%3A%20Overall%2C%20ChatGPT%20does,steps%20to%20control%20its%20use.",
    "crumbs": [
      "Lösungen",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Methoden</span>"
    ]
  },
  {
    "objectID": "lösungen_theorien.html",
    "href": "lösungen_theorien.html",
    "title": "16  Theorien",
    "section": "",
    "text": "Theorien übernehmen in der Wissenschaft oft die Funktion eines Wegweisers. Durch sie lässt sich einschätzen, wie die Ergebnisse einer Untersuchung aussehen, bevor sie durchgeführt werden. Falls die Ergebnisse also nicht wie erwartet sind, zum Beispiel wenn eine Replikation fehlschlägt, besteht der Verdacht, die Theorie könnte falsch oder unvollständig sein. In der Psychologie, in welcher Theorien häufig in Alltagssprache und nicht formalisiert (also als mathematische Formeln) notiert werden, ist der Sinn von formalen Varianten schon lange diskutiert (REF[LR1] , REF[LR2] , REF[LR3] ) und Replikationsfehlschläge werden darauf zurückgeführt, dass die Theorien zu unpräzise, missverständlich, und nicht gut ausgearbeitet sind. Es herrscht das Motto, Theorien seien wie Zahnbürsten und man solle niemals die von jemand anderem nehmen (REF[LR4] ). Daran, dass das Problem seit Jahrzehnten beschrieben wird, hat sich wenig geändert (REF[LR5] , REF[LR6] ). Inzwischen gibt es jedoch Bemühungen, das Problem auch zu lösen (REF[LR7] , REF[LR8] ), beispielsweise durch das herausstellen der Vorteile von bekannten formalen Theorien (REF[LR9] [LR10] ), Entwicklung von Werkzeugen zur Formalisierung von Theorien (https://www.theorymaps.org), oder durch einzelne Forschende. Der Wert von Replikationsstudien wird im Rahmen der „Theorie-Krise” darin gesehen, dass nur durch wiederholte Untersuchung festgestellt werden kann, unter welchen Bedingungen ein bestimmtes Phänomen nachweisbar ist (z.B. REF[LR11] ).\n\n16.0.1 Spezifizierung und Formalisierung\nDas Problem einer Theorie in Alltagssprache lässt sich wie folgt veranschaulichen: REF[LR12]  versuchten, eine Studie zur Facial Feedback Hypothese zu replizieren. Die Hypothese besagt, dass Personen, wenn sie einen Stift so im Mund halten, dass dabei diejenigen Muskeln beansprucht werden, die auch beim Grinsen genutzt werden, Dinge lustiger finden, als wenn sie den Stift anders im Mund halten. Damit geprüft werden konnte, ob Versuchspersonen ihre Stifte richtig in den Mund nahmen, wurden sie in der Replikationsstudie gefilmt. Das war in der Originalstudie nicht der Fall. Die Replikationsstudien schlugen fehl und der Unterschied der Kamera wurde von den Autoren der Originalstudie als Grund genannt, weshalb die Replikation fehlschlug (REF[LR13] ). Ob und wie die Präsenz einer Kamera in dem Versuchsaufbau relevant ist, war nicht Teil der Theorie beziehungsweise Hypothese. Durch die Formulierung der Theorie in gewöhnlicher Sprache können unendlich viele solcher „Post Hoc” Erklärungen in die Diskussion eingebracht werden, die jegliche Replikationsfehlschläge relativieren. Bei einer formalen Theorie sind solche Missverständnisse seltener.\nAus dem Problem, dass Theorien schwer verständlich und wenig informativ sind, folgt die Tatsache, dass die meisten Theorien nie falsifiziert werden, aber auch nicht weiter untersucht werden („Zombie-Theorien”, REF[LR14] ). Ähnlich verhält es sich mit psychologischen Skalen, bei denen die meisten nur einmal verwendet werden (REF[LR15] ). Flexible Theorien vereinfachen außerdem das p-Hacking: In der Theorie ist keine Vorgabe zum Aufbereiten der Daten, also kann diejenige Variante gewählt werden, die die Theorie bestätigt (REF[LR16] ).\n\n\n16.0.2 Gegnerische Kollaborationen\nEin Ansatz, bei dem verhindert werden soll, dass uninformative Studien durchgeführt werden und sich Diskussionen auf Verstehensweisen von Theorien fokussieren, sind gegnerische Kollaborationen (adversarial collaborations, REF[LR17] ). Dabei arbeiten Vertreter*innen von zwei inkompatiblen Standpunkten miteinander und erstellen beispielsweise eine Studie, die den Konflikt beilegen soll (z.B. REF[LR18] , REF[LR19] ).\n [LR1]meehl\n [LR2]Severe testing(?) platt 1984\n [LR3]https://doi.org/10.31219/osf.io/9amhe\n [LR4]Herkunft des zitates rausfinden\n [LR5]Two crises lakens\n [LR6]muthukrishna\n [LR7]https://www.researchgate.net/publication/378966795_Replication_and_Theory_Development_in_the_Social_and_Psychological_Sciences\n [LR8]https://www.researchgate.net/publication/375305901_Tension_Between_Theory_and_Practice_of_Replication\n [LR9]Formal theories paper mit angst, partnerschaft\n [LR10]Robinaugh, D. J., Haslbeck, J. M., Ryan, O., Fried, E. I., & Waldorp, L. J. (2021). Invisible hands and fine calipers: A call to use formal theory as a toolkit for theory construction. Perspectives on Psychological Science, 16(4), 725-743.\n [LR11]https://journal.trialanderror.org/pub/tension-between-theory/release/1\n [LR12]replication facial feedback\n [LR13]antwort auf RRR facial feedback\n [LR14]Ferguson, C. J., & Heene, M. (2012). A vast graveyard of undead theories: Publication bias and psychological science’s aversion to the null. Perspectives on Psychological Science, 7(6), 555-561.\n [LR15]Elson, M., Hussey, I., Alsalti, T., & Arslan, R. C. (2023). Psychological measures aren’t toothbrushes. Communications Psychology, 1(1), 25.\n [LR16]https://www.researchgate.net/publication/364419271_Selective_Hypothesis_Reporting_in_Psychology_Comparing_Preregistrations_and_Corresponding_Publications\n [LR17]Cleeremans, A. (2022). Theory as adversarial collaboration. Nature Human Behaviour, 6(4), 485-486.\n [LR18]Mellers, B., Hertwig, R., & Kahneman, D. (2001). Do frequency representations eliminate conjunction effects? An exercise in adversarial collaboration. Psychological Science, 12(4), 269-275.\n [LR19]Cowan, N., Belletier, C., Doherty, J. M., Jaroslawska, A. J., Rhodes, S., Forsberg, A., ... & Logie, R. H. (2020). How do scientific views change? Notes from an extended adversarial collaboration. Perspectives on Psychological Science, 15(4), 1011-1025.",
    "crumbs": [
      "Lösungen",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Theorien</span>"
    ]
  },
  {
    "objectID": "lösungen_welt.html",
    "href": "lösungen_welt.html",
    "title": "17  Die Welt",
    "section": "",
    "text": "Nachdem nun systemische, methodische, und theoretische Gründe für die Replikationskrise diskutiert wurden, endet die Diskussion mit einem allgemeinen und ontologischen Punkt: Die Erwartung, dass sich Befunde wiederholt feststellen lassen müssen beruht auf der Annahme oder Theorie, dass die Welt – oder zumindest die jeweils in der Wissenschaft untersuchte Gesetzmäßigkeit – stabil ist. Bei fluktuierenden Mustern in der Welt oder im menschlichen Verhalten sind Replikationsfehlschläge wenig überraschend. Für die Psychologie haben REF [LR1] und Gergen [LR2] (REF, siehe auch REF[LR3] )\n-          Historizität (why psychology cannot be a science jan smedslund)\n-          Indeed, the argument put forward by Gergen (1973) that most (if not all) effects in psychology are not stable across time and context was one of the starting points for the first crisis in social psychology. Gergen writes: “It is the purpose of this paper to argue that social psychology is primarily an historical inquiry. Unlike the natural sciences, it deals with facts that are largely nonrepeatable and which fluctuate markedly over time” (1973, p. 310). Nach Lakens two crises https://osf.io/preprints/psyarxiv/dtvs7/ p. 12\no   Das ist aber unklar und auch nicht ausreichend erforscht\n-          more research\n-          evtl Ontologie[LR4] \n [LR1]Dänemark, why psychology cannot be a science\n [LR2]REF siehe bei lakens\n [LR3]Lakens two crises\n [LR4]https://psycnet.apa.org/record/2024-93999-001",
    "crumbs": [
      "Lösungen",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Die Welt</span>"
    ]
  },
  {
    "objectID": "einleitung.html",
    "href": "einleitung.html",
    "title": "Einleitung",
    "section": "",
    "text": "Einleitung",
    "crumbs": [
      "Einleitung"
    ]
  },
  {
    "objectID": "geschichte.html",
    "href": "geschichte.html",
    "title": "Die Geschichte der Open Science Bewegung",
    "section": "",
    "text": "Geschichte\nIm Rahmen dieses Buches wird die Open Science Bewegung also als eine Reaktion auf identifizierte Probleme und damit als Selbstkorrektur-Prozess der Wissenschaft verstanden. In im Zentrum steht ein mangelndes Vertrauen in Befunde, die in wissenschaftlichen Fachzeitschriften veröffentlicht wurden. Einige Probleme sind schon seit Jahrzehnten in ähnlicher Form bekannt. Bis sie allerdings öffentlich diskutiert wurden und Lösungsvorschläge erarbeiteten, benötigte es einschneidende Ereignisse.",
    "crumbs": [
      "Die Geschichte der Open Science Bewegung"
    ]
  },
  {
    "objectID": "probleme.html",
    "href": "probleme.html",
    "title": "Probleme",
    "section": "",
    "text": "Probleme, die im Rahmen der Revolution identifiziert wurden\nIm folgenden werden zahlreiche Probleme des Wissenschaftssystems erklärt und aufgelistet. Manche der Probleme sind seit vielen Jahrzehnten bekannt und andere erst seit wenigen Jahren. Es sei bei der Lektüre zu beachten, dass es zu allen diesen Problemen bereits ausgearbeitete und teilweise auch ausgeführte Lösungsansätze gibt. Wie die Lösungsansätze aussehen und welche Lösungen auf welche Probleme zugeschnitten sind, wird im darauffolgenden Kapitel erläutert.",
    "crumbs": [
      "Probleme"
    ]
  },
  {
    "objectID": "lösungen.html",
    "href": "lösungen.html",
    "title": "Lösungen",
    "section": "",
    "text": "Lösungen und Ansätze zur Verbesserung der Lage der Psychologie\nWerfen wir ein Blick darauf, was sich seit 2012 in der Psychologie verändert hat. Eingeteilt sind die Veränderungen dahingehend, welchen Teil des Problems sie vor allem betreffen: Ist der Zweck einer neuen Vorgabe, das System, die Methodik, oder die Theorien der Forschung zu verbessern? Einige Lösungen sind mit vielen Problemen gleichzeitig verknüpft und manche sind auf bestimmte Angelegenheiten maßgeschneidert. Abbildung 3 enthält eine grobe Unterteilung.",
    "crumbs": [
      "Lösungen"
    ]
  },
  {
    "objectID": "fazit.html",
    "href": "fazit.html",
    "title": "Fazit und Ausblick",
    "section": "",
    "text": "Was hat sich verbessert?\n-          Review der Veränderungen https://www.researchgate.net/publication/355421853_Replicability_Robustness_and_Reproducibility_in_Psychological_Science\n-          journal guidelines und reporting: https://www.researchgate.net/publication/364374923_The_influence_of_journal_submission_guidelines_on_authors’_reporting_of_statistics_and_use_of_open_research_practices_Five_years_later\n-          https://www.nature.com/articles/s41562-023-01749-9 extreme Erhöhung von Replikationsrate durch neue Methoden in kleienr Stichprobe von Phänomenen\n-          https://www.nature.com/articles/s44271-023-00003-2 positive structural, procedural, and community changes\n-          Neue Meinung zu OS Praktiken: https://www.researchgate.net/publication/373685253_Survey_of_open_science_practices_and_attitudes_in_the_social_sciences\n-          Meta-Wissenschaftliche Evaluation von neuen Methoden\no   Bisherige Vorschläge sind zum Teil intuitiv und nicht ausführlich geprüft https://journals.sagepub.com/doi/10.1177/10892680241235120; manche der Intuitionen sind auch falsch (z.B. wichtig ist nur, was replizierbar ist)\no   Initial evidence registerd reports\no   Nosek 2023 paper (protzko?)\no   Hilft Preregistration gegen p-hacking? https://ideas.repec.org/p/zbw/i4rdps/101.html\n-          Strikte Normen könnten Flexibilität verringern und Offenheit und Innovationspotenziale einschränken\no   Anfänglich unplausible Hypothesen sollten nicht direkt verworfen werden, manchmal führt auch eine Beharrungstendenz von Wissenschaftler*innen dazu, dass sich neue, anfänglich unplausible Theorien, durchsetzen (https://journals.sagepub.com/doi/10.1177/10892680241235120)",
    "crumbs": [
      "Fazit und Ausblick"
    ]
  },
  {
    "objectID": "fazit.html#literaturempfehlungen",
    "href": "fazit.html#literaturempfehlungen",
    "title": "Fazit und Ausblick",
    "section": "Literaturempfehlungen",
    "text": "Literaturempfehlungen\n·         Chris Chambers Buch https://doi.org/10.2307/j.ctvc779w5\n·         Charlotte Pennington Buch, hier ein Kapitel: https://osf.io/preprints/psyarxiv/2tqep\n·         Matters of Significance https://discovery.ucl.ac.uk/id/eprint/10186829/1/Matters-of-Significance.pdf\n [LR1]https://ideas.repec.org/p/stg/wpaper/2020_01.html\n [LR2]",
    "crumbs": [
      "Fazit und Ausblick"
    ]
  }
]